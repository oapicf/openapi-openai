/*
 * OpenAI API
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.0.0
 * Contact: blah+oapicf@cliffano.com
 *
 * NOTE: This class is auto generated by the OAS code generator program.
 * https://github.com/OpenAPITools/openapi-generator
 * Do not edit the class manually.
 */

/**
 * OASCreateChatCompletionRequest
 */
public class OASCreateChatCompletionRequest implements OAS.MappedProperties {
    /**
     * A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
     * @return messages
     */
    public List<OASChatCompletionRequestMessage> messages { get; set; }

    /**
     * Get model
     * @return model
     */
    public OASCreateChatCompletionRequestModel model { get; set; }

    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\'s likelihood to repeat the same line verbatim.\n\n[See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)\n
     * minimum: -2
     * maximum: 2
     * @return frequencyPenalty
     */
    public Double frequencyPenalty { get; set; }

    /**
     * Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n
     * @return logitBias
     */
    public Map<String, Integer> logitBias { get; set; }

    /**
     * Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.
     * @return logprobs
     */
    public Boolean logprobs { get; set; }

    /**
     * An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
     * minimum: 0
     * maximum: 20
     * @return topLogprobs
     */
    public Integer topLogprobs { get; set; }

    /**
     * The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.\n\nThe total length of input tokens and generated tokens is limited by the model\'s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.\n
     * @return maxTokens
     */
    public Integer maxTokens { get; set; }

    /**
     * How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
     * minimum: 1
     * maximum: 128
     * @return n
     */
    public Integer n { get; set; }

    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model\'s likelihood to talk about new topics.\n\n[See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)\n
     * minimum: -2
     * maximum: 2
     * @return presencePenalty
     */
    public Double presencePenalty { get; set; }

    /**
     * Get responseFormat
     * @return responseFormat
     */
    public OASCreateChatCompletionRequestRespon responseFormat { get; set; }

    /**
     * This feature is in Beta.\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.\nDeterminism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n
     * minimum: -9223372036854775808
     * maximum: 9223372036854775807
     * @return seed
     */
    public Integer seed { get; set; }

    /**
     * Get stop
     * @return stop
     */
    public OASCreateChatCompletionRequestStop stop { get; set; }

    /**
     * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n
     * @return stream
     */
    public Boolean stream { get; set; }

    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or `top_p` but not both.\n
     * minimum: 0
     * maximum: 2
     * @return temperature
     */
    public Double temperature { get; set; }

    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both.\n
     * minimum: 0
     * maximum: 1
     * @return topP
     */
    public Double topP { get; set; }

    /**
     * A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\n
     * @return tools
     */
    public List<OASChatCompletionTool> tools { get; set; }

    /**
     * Get toolChoice
     * @return toolChoice
     */
    public OASChatCompletionToolChoiceOption toolChoice { get; set; }

    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n
     * @return user
     */
    public String user { get; set; }

    /**
     * Get functionCall
     * @return functionCall
     */
    public OASCreateChatCompletionRequestFuncti functionCall { get; set; }

    /**
     * Deprecated in favor of `tools`.\n\nA list of functions the model may generate JSON inputs for.\n
     * @return functions
     */
    public List<OASChatCompletionFunctions> functions { get; set; }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    public OASCreateChatCompletionRequest() {
        messages = new List<OASChatCompletionRequestMessage>();
        frequencyPenalty = 0;
        logitBias = new Map<String, Integer>();
        logprobs = false;
        n = 1;
        presencePenalty = 0;
        stream = false;
        temperature = 1;
        topP = 1;
        tools = new List<OASChatCompletionTool>();
        functions = new List<OASChatCompletionFunctions>();
    }

    public static OASCreateChatCompletionRequest getExample() {
        OASCreateChatCompletionRequest createChatCompletionRequest = new OASCreateChatCompletionRequest();
          createChatCompletionRequest.messages = new List<OASChatCompletionRequestMessage>{OASChatCompletionRequestMessage.getExample()};
          createChatCompletionRequest.model = OASCreateChatCompletionRequestModel.getExample();
          createChatCompletionRequest.frequencyPenalty = 1.3579;
          createChatCompletionRequest.logitBias = new Map<String, Integer>{'key'=>0};
          createChatCompletionRequest.logprobs = true;
          createChatCompletionRequest.topLogprobs = 0;
          createChatCompletionRequest.maxTokens = 0;
          createChatCompletionRequest.n = 1;
          createChatCompletionRequest.presencePenalty = 1.3579;
          createChatCompletionRequest.responseFormat = OASCreateChatCompletionRequestRespon.getExample();
          createChatCompletionRequest.seed = 0;
          createChatCompletionRequest.stop = OASCreateChatCompletionRequestStop.getExample();
          createChatCompletionRequest.stream = true;
          createChatCompletionRequest.temperature = 1;
          createChatCompletionRequest.topP = 1;
          createChatCompletionRequest.tools = new List<OASChatCompletionTool>{OASChatCompletionTool.getExample()};
          createChatCompletionRequest.toolChoice = OASChatCompletionToolChoiceOption.getExample();
          createChatCompletionRequest.user = 'user-1234';
          createChatCompletionRequest.functionCall = OASCreateChatCompletionRequestFuncti.getExample();
          createChatCompletionRequest.functions = new List<OASChatCompletionFunctions>{OASChatCompletionFunctions.getExample()};
        return createChatCompletionRequest;
    }

    public Boolean equals(Object obj) {
        if (obj instanceof OASCreateChatCompletionRequest) {           
            OASCreateChatCompletionRequest createChatCompletionRequest = (OASCreateChatCompletionRequest) obj;
            return this.messages == createChatCompletionRequest.messages
                && this.model == createChatCompletionRequest.model
                && this.frequencyPenalty == createChatCompletionRequest.frequencyPenalty
                && this.logitBias == createChatCompletionRequest.logitBias
                && this.logprobs == createChatCompletionRequest.logprobs
                && this.topLogprobs == createChatCompletionRequest.topLogprobs
                && this.maxTokens == createChatCompletionRequest.maxTokens
                && this.n == createChatCompletionRequest.n
                && this.presencePenalty == createChatCompletionRequest.presencePenalty
                && this.responseFormat == createChatCompletionRequest.responseFormat
                && this.seed == createChatCompletionRequest.seed
                && this.stop == createChatCompletionRequest.stop
                && this.stream == createChatCompletionRequest.stream
                && this.temperature == createChatCompletionRequest.temperature
                && this.topP == createChatCompletionRequest.topP
                && this.tools == createChatCompletionRequest.tools
                && this.toolChoice == createChatCompletionRequest.toolChoice
                && this.user == createChatCompletionRequest.user
                && this.functionCall == createChatCompletionRequest.functionCall
                && this.functions == createChatCompletionRequest.functions;
        }
        return false;
    }

    public Integer hashCode() {
        Integer hashCode = 43;
        hashCode = (17 * hashCode) + (messages == null ? 0 : System.hashCode(messages));
        hashCode = (17 * hashCode) + (model == null ? 0 : System.hashCode(model));
        hashCode = (17 * hashCode) + (frequencyPenalty == null ? 0 : System.hashCode(frequencyPenalty));
        hashCode = (17 * hashCode) + (logitBias == null ? 0 : System.hashCode(logitBias));
        hashCode = (17 * hashCode) + (logprobs == null ? 0 : System.hashCode(logprobs));
        hashCode = (17 * hashCode) + (topLogprobs == null ? 0 : System.hashCode(topLogprobs));
        hashCode = (17 * hashCode) + (maxTokens == null ? 0 : System.hashCode(maxTokens));
        hashCode = (17 * hashCode) + (n == null ? 0 : System.hashCode(n));
        hashCode = (17 * hashCode) + (presencePenalty == null ? 0 : System.hashCode(presencePenalty));
        hashCode = (17 * hashCode) + (responseFormat == null ? 0 : System.hashCode(responseFormat));
        hashCode = (17 * hashCode) + (seed == null ? 0 : System.hashCode(seed));
        hashCode = (17 * hashCode) + (stop == null ? 0 : System.hashCode(stop));
        hashCode = (17 * hashCode) + (stream == null ? 0 : System.hashCode(stream));
        hashCode = (17 * hashCode) + (temperature == null ? 0 : System.hashCode(temperature));
        hashCode = (17 * hashCode) + (topP == null ? 0 : System.hashCode(topP));
        hashCode = (17 * hashCode) + (tools == null ? 0 : System.hashCode(tools));
        hashCode = (17 * hashCode) + (toolChoice == null ? 0 : System.hashCode(toolChoice));
        hashCode = (17 * hashCode) + (user == null ? 0 : System.hashCode(user));
        hashCode = (17 * hashCode) + (functionCall == null ? 0 : System.hashCode(functionCall));
        hashCode = (17 * hashCode) + (functions == null ? 0 : System.hashCode(functions));
        return hashCode;
    }
}

