/*
 * OpenAI API
 * APIs for sampling from and fine-tuning language models
 *
 * The version of the OpenAPI document: 2.0.0
 * Contact: blah+oapicf@cliffano.com
 *
 * NOTE: This class is auto generated by the OAS code generator program.
 * https://github.com/OpenAPITools/openapi-generator
 * Do not edit the class manually.
 */

/**
 * OASCreateChatCompletionRequest
 */
public class OASCreateChatCompletionRequest implements OAS.MappedProperties {
    /**
     * Get model
     * @return model
     */
    public OASCreateChatCompletionRequestModel model { get; set; }

    /**
     * A list of messages comprising the conversation so far. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
     * @return messages
     */
    public List<OASChatCompletionRequestMessage> messages { get; set; }

    /**
     * A list of functions the model may generate JSON inputs for.
     * @return functions
     */
    public List<OASChatCompletionFunctions> functions { get; set; }

    /**
     * Get functionCall
     * @return functionCall
     */
    public OASCreateChatCompletionRequestFuncti functionCall { get; set; }

    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or `top_p` but not both.\n
     * minimum: 0
     * maximum: 2
     * @return temperature
     */
    public Double temperature { get; set; }

    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both.\n
     * minimum: 0
     * maximum: 1
     * @return topP
     */
    public Double topP { get; set; }

    /**
     * How many chat completion choices to generate for each input message.
     * minimum: 1
     * maximum: 128
     * @return n
     */
    public Integer n { get; set; }

    /**
     * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).\n
     * @return stream
     */
    public Boolean stream { get; set; }

    /**
     * Get stop
     * @return stop
     */
    public OASCreateChatCompletionRequestStop stop { get; set; }

    /**
     * The maximum number of [tokens](/tokenizer) to generate in the chat completion.\n\nThe total length of input tokens and generated tokens is limited by the model\'s context length. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) for counting tokens.\n
     * @return maxTokens
     */
    public Integer maxTokens { get; set; }

    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model\'s likelihood to talk about new topics.\n\n[See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n
     * minimum: -2
     * maximum: 2
     * @return presencePenalty
     */
    public Double presencePenalty { get; set; }

    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\'s likelihood to repeat the same line verbatim.\n\n[See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n
     * minimum: -2
     * maximum: 2
     * @return frequencyPenalty
     */
    public Double frequencyPenalty { get; set; }

    /**
     * Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n
     * @return logitBias
     */
    public Object logitBias { get; set; }

    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n
     * @return user
     */
    public String user { get; set; }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'function_call' => 'functionCall',
        'top_p' => 'topP',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'function_call' => 'functionCall',
        'top_p' => 'topP',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'function_call' => 'functionCall',
        'top_p' => 'topP',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'function_call' => 'functionCall',
        'top_p' => 'topP',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'function_call' => 'functionCall',
        'top_p' => 'topP',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'function_call' => 'functionCall',
        'top_p' => 'topP',
        'max_tokens' => 'maxTokens',
        'presence_penalty' => 'presencePenalty',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    public OASCreateChatCompletionRequest() {
        messages = new List<OASChatCompletionRequestMessage>();
        functions = new List<OASChatCompletionFunctions>();
        temperature = 1;
        topP = 1;
        n = 1;
        stream = false;
        presencePenalty = 0;
        frequencyPenalty = 0;
    }

    public static OASCreateChatCompletionRequest getExample() {
        OASCreateChatCompletionRequest createChatCompletionRequest = new OASCreateChatCompletionRequest();
          createChatCompletionRequest.model = OASCreateChatCompletionRequestModel.getExample();
          createChatCompletionRequest.messages = new List<OASChatCompletionRequestMessage>{OASChatCompletionRequestMessage.getExample()};
          createChatCompletionRequest.functions = new List<OASChatCompletionFunctions>{OASChatCompletionFunctions.getExample()};
          createChatCompletionRequest.functionCall = OASCreateChatCompletionRequestFuncti.getExample();
          createChatCompletionRequest.temperature = 1;
          createChatCompletionRequest.topP = 1;
          createChatCompletionRequest.n = 1;
          createChatCompletionRequest.stream = true;
          createChatCompletionRequest.stop = OASCreateChatCompletionRequestStop.getExample();
          createChatCompletionRequest.maxTokens = 0;
          createChatCompletionRequest.presencePenalty = 1.3579;
          createChatCompletionRequest.frequencyPenalty = 1.3579;
          createChatCompletionRequest.logitBias = null;
          createChatCompletionRequest.user = 'user-1234';
        return createChatCompletionRequest;
    }

    public Boolean equals(Object obj) {
        if (obj instanceof OASCreateChatCompletionRequest) {           
            OASCreateChatCompletionRequest createChatCompletionRequest = (OASCreateChatCompletionRequest) obj;
            return this.model == createChatCompletionRequest.model
                && this.messages == createChatCompletionRequest.messages
                && this.functions == createChatCompletionRequest.functions
                && this.functionCall == createChatCompletionRequest.functionCall
                && this.temperature == createChatCompletionRequest.temperature
                && this.topP == createChatCompletionRequest.topP
                && this.n == createChatCompletionRequest.n
                && this.stream == createChatCompletionRequest.stream
                && this.stop == createChatCompletionRequest.stop
                && this.maxTokens == createChatCompletionRequest.maxTokens
                && this.presencePenalty == createChatCompletionRequest.presencePenalty
                && this.frequencyPenalty == createChatCompletionRequest.frequencyPenalty
                && this.logitBias == createChatCompletionRequest.logitBias
                && this.user == createChatCompletionRequest.user;
        }
        return false;
    }

    public Integer hashCode() {
        Integer hashCode = 43;
        hashCode = (17 * hashCode) + (model == null ? 0 : System.hashCode(model));
        hashCode = (17 * hashCode) + (messages == null ? 0 : System.hashCode(messages));
        hashCode = (17 * hashCode) + (functions == null ? 0 : System.hashCode(functions));
        hashCode = (17 * hashCode) + (functionCall == null ? 0 : System.hashCode(functionCall));
        hashCode = (17 * hashCode) + (temperature == null ? 0 : System.hashCode(temperature));
        hashCode = (17 * hashCode) + (topP == null ? 0 : System.hashCode(topP));
        hashCode = (17 * hashCode) + (n == null ? 0 : System.hashCode(n));
        hashCode = (17 * hashCode) + (stream == null ? 0 : System.hashCode(stream));
        hashCode = (17 * hashCode) + (stop == null ? 0 : System.hashCode(stop));
        hashCode = (17 * hashCode) + (maxTokens == null ? 0 : System.hashCode(maxTokens));
        hashCode = (17 * hashCode) + (presencePenalty == null ? 0 : System.hashCode(presencePenalty));
        hashCode = (17 * hashCode) + (frequencyPenalty == null ? 0 : System.hashCode(frequencyPenalty));
        hashCode = (17 * hashCode) + (logitBias == null ? 0 : System.hashCode(logitBias));
        hashCode = (17 * hashCode) + (user == null ? 0 : System.hashCode(user));
        return hashCode;
    }
}

