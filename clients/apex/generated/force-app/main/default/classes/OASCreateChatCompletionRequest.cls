/*
 * OpenAI API
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.3.0
 * Contact: blah+oapicf@cliffano.com
 *
 * NOTE: This class is auto generated by the OAS code generator program.
 * https://github.com/OpenAPITools/openapi-generator
 * Do not edit the class manually.
 */

/**
 * OASCreateChatCompletionRequest
 */
public class OASCreateChatCompletionRequest implements OAS.MappedProperties {
    /**
     * A list of messages comprising the conversation so far. Depending on the\n[model](/docs/models) you use, different message types (modalities) are\nsupported, like [text](/docs/guides/text-generation),\n[images](/docs/guides/vision), and [audio](/docs/guides/audio).\n
     * @return messages
     */
    public List<OASChatCompletionRequestMessage> messages { get; set; }

    /**
     * Get model
     * @return model
     */
    public OASCreateChatCompletionRequestModel model { get; set; }

    /**
     * Whether or not to store the output of this chat completion request for \nuse in our [model distillation](/docs/guides/distillation) or\n[evals](/docs/guides/evals) products.\n
     * @return store
     */
    public Boolean store { get; set; }

    /**
     * **o1 models only** \n\nConstrains effort on reasoning for \n[reasoning models](https://platform.openai.com/docs/guides/reasoning).\nCurrently supported values are `low`, `medium`, and `high`. Reducing\nreasoning effort can result in faster responses and fewer tokens used\non reasoning in a response.\n
     */
    public enum ReasoningEffortEnum {
        LOW,
        MEDIUM,
        HIGH
    }

    /**
     * **o1 models only** \n\nConstrains effort on reasoning for \n[reasoning models](https://platform.openai.com/docs/guides/reasoning).\nCurrently supported values are `low`, `medium`, and `high`. Reducing\nreasoning effort can result in faster responses and fewer tokens used\non reasoning in a response.\n
     * @return reasoningEffort
     */
    public ReasoningEffortEnum reasoningEffort { get; set; }

    /**
     * Developer-defined tags and values used for filtering completions\nin the [dashboard](https://platform.openai.com/chat-completions).\n
     * @return metadata
     */
    public Map<String, String> metadata { get; set; }

    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on\ntheir existing frequency in the text so far, decreasing the model\'s\nlikelihood to repeat the same line verbatim.\n
     * minimum: -2
     * maximum: 2
     * @return frequencyPenalty
     */
    public Double frequencyPenalty { get; set; }

    /**
     * Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the\ntokenizer) to an associated bias value from -100 to 100. Mathematically,\nthe bias is added to the logits generated by the model prior to sampling.\nThe exact effect will vary per model, but values between -1 and 1 should\ndecrease or increase likelihood of selection; values like -100 or 100\nshould result in a ban or exclusive selection of the relevant token.\n
     * @return logitBias
     */
    public Map<String, Integer> logitBias { get; set; }

    /**
     * Whether to return log probabilities of the output tokens or not. If true,\nreturns the log probabilities of each output token returned in the\n`content` of `message`.\n
     * @return logprobs
     */
    public Boolean logprobs { get; set; }

    /**
     * An integer between 0 and 20 specifying the number of most likely tokens to\nreturn at each token position, each with an associated log probability.\n`logprobs` must be set to `true` if this parameter is used.\n
     * minimum: 0
     * maximum: 20
     * @return topLogprobs
     */
    public Integer topLogprobs { get; set; }

    /**
     * The maximum number of [tokens](/tokenizer) that can be generated in the\nchat completion. This value can be used to control\n[costs](https://openai.com/api/pricing/) for text generated via API.\n\nThis value is now deprecated in favor of `max_completion_tokens`, and is\nnot compatible with [o1 series models](/docs/guides/reasoning).\n
     * @return maxTokens
     */
    public Integer maxTokens { get; set; }

    /**
     * An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).\n
     * @return maxCompletionTokens
     */
    public Integer maxCompletionTokens { get; set; }

    /**
     * How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
     * minimum: 1
     * maximum: 128
     * @return n
     */
    public Integer n { get; set; }

    /**
     * Gets or Sets modalities
     */
    public enum ModalitiesEnum {
        TEXT,
        AUDIO
    }

    /**
     * Output types that you would like the model to generate for this request.\nMost models are capable of generating text, which is the default:\n\n`["text"]`\n\nThe `gpt-4o-audio-preview` model can also be used to [generate audio](/docs/guides/audio). To\nrequest that this model generate both text and audio responses, you can\nuse:\n\n`["text", "audio"]`\n
     * @return modalities
     */
    public List<ModalitiesEnum> modalities { get; set; }

    /**
     * Get prediction
     * @return prediction
     */
    public OASPredictionContent prediction { get; set; }

    /**
     * Get audio
     * @return audio
     */
    public OASCreateChatCompletionRequestAudio audio { get; set; }

    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on\nwhether they appear in the text so far, increasing the model\'s likelihood\nto talk about new topics.\n
     * minimum: -2
     * maximum: 2
     * @return presencePenalty
     */
    public Double presencePenalty { get; set; }

    /**
     * Get responseFormat
     * @return responseFormat
     */
    public OASCreateChatCompletionRequestRespon responseFormat { get; set; }

    /**
     * This feature is in Beta.\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.\nDeterminism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n
     * minimum: -9223372036854776000
     * maximum: 9223372036854776000
     * @return seed
     */
    public Integer seed { get; set; }

    /**
     * Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:\n\n  - If set to \'auto\', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.\n  - If set to \'auto\', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.\n  - If set to \'default\', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.\n  - When not set, the default behavior is \'auto\'.\n\n  When this parameter is set, the response body will include the `service_tier` utilized.\n
     */
    public enum ServiceTierEnum {
        AUTO,
        DEFAULT
    }

    /**
     * Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:\n\n  - If set to \'auto\', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.\n  - If set to \'auto\', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.\n  - If set to \'default\', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.\n  - When not set, the default behavior is \'auto\'.\n\n  When this parameter is set, the response body will include the `service_tier` utilized.\n
     * @return serviceTier
     */
    public ServiceTierEnum serviceTier { get; set; }

    /**
     * Get stop
     * @return stop
     */
    public OASCreateChatCompletionRequestStop stop { get; set; }

    /**
     * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n
     * @return stream
     */
    public Boolean stream { get; set; }

    /**
     * Get streamOptions
     * @return streamOptions
     */
    public OASChatCompletionStreamOptions streamOptions { get; set; }

    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or `top_p` but not both.\n
     * minimum: 0
     * maximum: 2
     * @return temperature
     */
    public Double temperature { get; set; }

    /**
     * An alternative to sampling with temperature, called nucleus sampling,\nwhere the model considers the results of the tokens with top_p probability\nmass. So 0.1 means only the tokens comprising the top 10% probability mass\nare considered.\n\nWe generally recommend altering this or `temperature` but not both.\n
     * minimum: 0
     * maximum: 1
     * @return topP
     */
    public Double topP { get; set; }

    /**
     * A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\n
     * @return tools
     */
    public List<OASChatCompletionTool> tools { get; set; }

    /**
     * Get toolChoice
     * @return toolChoice
     */
    public OASChatCompletionToolChoiceOption toolChoice { get; set; }

    /**
     * Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.
     * @return parallelToolCalls
     */
    public Boolean parallelToolCalls { get; set; }

    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).\n
     * @return user
     */
    public String user { get; set; }

    /**
     * Get functionCall
     * @return functionCall
     */
    public OASCreateChatCompletionRequestFuncti functionCall { get; set; }

    /**
     * Deprecated in favor of `tools`.\n\nA list of functions the model may generate JSON inputs for.\n
     * @return functions
     */
    public List<OASChatCompletionFunctions> functions { get; set; }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'reasoning_effort' => 'reasoningEffort',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'max_completion_tokens' => 'maxCompletionTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'service_tier' => 'serviceTier',
        'stream_options' => 'streamOptions',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'parallel_tool_calls' => 'parallelToolCalls',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'reasoning_effort' => 'reasoningEffort',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'max_completion_tokens' => 'maxCompletionTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'service_tier' => 'serviceTier',
        'stream_options' => 'streamOptions',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'parallel_tool_calls' => 'parallelToolCalls',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'reasoning_effort' => 'reasoningEffort',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'max_completion_tokens' => 'maxCompletionTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'service_tier' => 'serviceTier',
        'stream_options' => 'streamOptions',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'parallel_tool_calls' => 'parallelToolCalls',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'reasoning_effort' => 'reasoningEffort',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'max_completion_tokens' => 'maxCompletionTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'service_tier' => 'serviceTier',
        'stream_options' => 'streamOptions',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'parallel_tool_calls' => 'parallelToolCalls',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'reasoning_effort' => 'reasoningEffort',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'max_completion_tokens' => 'maxCompletionTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'service_tier' => 'serviceTier',
        'stream_options' => 'streamOptions',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'parallel_tool_calls' => 'parallelToolCalls',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'reasoning_effort' => 'reasoningEffort',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'max_completion_tokens' => 'maxCompletionTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'service_tier' => 'serviceTier',
        'stream_options' => 'streamOptions',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'parallel_tool_calls' => 'parallelToolCalls',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'reasoning_effort' => 'reasoningEffort',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'max_completion_tokens' => 'maxCompletionTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'service_tier' => 'serviceTier',
        'stream_options' => 'streamOptions',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'parallel_tool_calls' => 'parallelToolCalls',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'reasoning_effort' => 'reasoningEffort',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'max_completion_tokens' => 'maxCompletionTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'service_tier' => 'serviceTier',
        'stream_options' => 'streamOptions',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'parallel_tool_calls' => 'parallelToolCalls',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'reasoning_effort' => 'reasoningEffort',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'max_completion_tokens' => 'maxCompletionTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'service_tier' => 'serviceTier',
        'stream_options' => 'streamOptions',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'parallel_tool_calls' => 'parallelToolCalls',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'reasoning_effort' => 'reasoningEffort',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'max_completion_tokens' => 'maxCompletionTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'service_tier' => 'serviceTier',
        'stream_options' => 'streamOptions',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'parallel_tool_calls' => 'parallelToolCalls',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'reasoning_effort' => 'reasoningEffort',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'max_completion_tokens' => 'maxCompletionTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'service_tier' => 'serviceTier',
        'stream_options' => 'streamOptions',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'parallel_tool_calls' => 'parallelToolCalls',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'reasoning_effort' => 'reasoningEffort',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'max_completion_tokens' => 'maxCompletionTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'service_tier' => 'serviceTier',
        'stream_options' => 'streamOptions',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'parallel_tool_calls' => 'parallelToolCalls',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'reasoning_effort' => 'reasoningEffort',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'max_completion_tokens' => 'maxCompletionTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'service_tier' => 'serviceTier',
        'stream_options' => 'streamOptions',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'parallel_tool_calls' => 'parallelToolCalls',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    private static final Map<String, String> propertyMappings = new Map<String, String>{
        'reasoning_effort' => 'reasoningEffort',
        'frequency_penalty' => 'frequencyPenalty',
        'logit_bias' => 'logitBias',
        'top_logprobs' => 'topLogprobs',
        'max_tokens' => 'maxTokens',
        'max_completion_tokens' => 'maxCompletionTokens',
        'presence_penalty' => 'presencePenalty',
        'response_format' => 'responseFormat',
        'service_tier' => 'serviceTier',
        'stream_options' => 'streamOptions',
        'top_p' => 'topP',
        'tool_choice' => 'toolChoice',
        'parallel_tool_calls' => 'parallelToolCalls',
        'function_call' => 'functionCall'
    };

    public Map<String, String> getPropertyMappings() {
        return propertyMappings;
    }

    public OASCreateChatCompletionRequest() {
        messages = new List<OASChatCompletionRequestMessage>();
        store = false;
        reasoningEffort = ReasoningEffortEnum.MEDIUM;
        metadata = new Map<String, String>();
        frequencyPenalty = 0;
        logitBias = new Map<String, Integer>();
        logprobs = false;
        n = 1;
        modalities = new List<String>();
        presencePenalty = 0;
        serviceTier = ServiceTierEnum.AUTO;
        stream = false;
        temperature = 1;
        topP = 1;
        tools = new List<OASChatCompletionTool>();
        parallelToolCalls = true;
        functions = new List<OASChatCompletionFunctions>();
    }

    public static OASCreateChatCompletionRequest getExample() {
        OASCreateChatCompletionRequest createChatCompletionRequest = new OASCreateChatCompletionRequest();
          createChatCompletionRequest.messages = new List<OASChatCompletionRequestMessage>{OASChatCompletionRequestMessage.getExample()};
          createChatCompletionRequest.model = OASCreateChatCompletionRequestModel.getExample();
          createChatCompletionRequest.store = true;
          createChatCompletionRequest.reasoningEffort = ReasoningEffortEnum.LOW;
          createChatCompletionRequest.metadata = new Map<String, String>{'key'=>''};
          createChatCompletionRequest.frequencyPenalty = 1.3579;
          createChatCompletionRequest.logitBias = new Map<String, Integer>{'key'=>0};
          createChatCompletionRequest.logprobs = true;
          createChatCompletionRequest.topLogprobs = 0;
          createChatCompletionRequest.maxTokens = 0;
          createChatCompletionRequest.maxCompletionTokens = 0;
          createChatCompletionRequest.n = 1;
          createChatCompletionRequest.modalities = List<ModalitiesEnum>.LIST_MODALITIESENUM_NEW_LIST_STRING_TEXT_;
          createChatCompletionRequest.prediction = OASPredictionContent.getExample();
          createChatCompletionRequest.audio = OASCreateChatCompletionRequestAudio.getExample();
          createChatCompletionRequest.presencePenalty = 1.3579;
          createChatCompletionRequest.responseFormat = OASCreateChatCompletionRequestRespon.getExample();
          createChatCompletionRequest.seed = 0;
          createChatCompletionRequest.serviceTier = ServiceTierEnum.AUTO;
          createChatCompletionRequest.stop = OASCreateChatCompletionRequestStop.getExample();
          createChatCompletionRequest.stream = true;
          createChatCompletionRequest.streamOptions = OASChatCompletionStreamOptions.getExample();
          createChatCompletionRequest.temperature = 1;
          createChatCompletionRequest.topP = 1;
          createChatCompletionRequest.tools = new List<OASChatCompletionTool>{OASChatCompletionTool.getExample()};
          createChatCompletionRequest.toolChoice = OASChatCompletionToolChoiceOption.getExample();
          createChatCompletionRequest.parallelToolCalls = true;
          createChatCompletionRequest.user = 'user-1234';
          createChatCompletionRequest.functionCall = OASCreateChatCompletionRequestFuncti.getExample();
          createChatCompletionRequest.functions = new List<OASChatCompletionFunctions>{OASChatCompletionFunctions.getExample()};
        return createChatCompletionRequest;
    }

    public Boolean equals(Object obj) {
        if (obj instanceof OASCreateChatCompletionRequest) {           
            OASCreateChatCompletionRequest createChatCompletionRequest = (OASCreateChatCompletionRequest) obj;
            return this.messages == createChatCompletionRequest.messages
                && this.model == createChatCompletionRequest.model
                && this.store == createChatCompletionRequest.store
                && this.reasoningEffort == createChatCompletionRequest.reasoningEffort
                && this.metadata == createChatCompletionRequest.metadata
                && this.frequencyPenalty == createChatCompletionRequest.frequencyPenalty
                && this.logitBias == createChatCompletionRequest.logitBias
                && this.logprobs == createChatCompletionRequest.logprobs
                && this.topLogprobs == createChatCompletionRequest.topLogprobs
                && this.maxTokens == createChatCompletionRequest.maxTokens
                && this.maxCompletionTokens == createChatCompletionRequest.maxCompletionTokens
                && this.n == createChatCompletionRequest.n
                && this.modalities == createChatCompletionRequest.modalities
                && this.prediction == createChatCompletionRequest.prediction
                && this.audio == createChatCompletionRequest.audio
                && this.presencePenalty == createChatCompletionRequest.presencePenalty
                && this.responseFormat == createChatCompletionRequest.responseFormat
                && this.seed == createChatCompletionRequest.seed
                && this.serviceTier == createChatCompletionRequest.serviceTier
                && this.stop == createChatCompletionRequest.stop
                && this.stream == createChatCompletionRequest.stream
                && this.streamOptions == createChatCompletionRequest.streamOptions
                && this.temperature == createChatCompletionRequest.temperature
                && this.topP == createChatCompletionRequest.topP
                && this.tools == createChatCompletionRequest.tools
                && this.toolChoice == createChatCompletionRequest.toolChoice
                && this.parallelToolCalls == createChatCompletionRequest.parallelToolCalls
                && this.user == createChatCompletionRequest.user
                && this.functionCall == createChatCompletionRequest.functionCall
                && this.functions == createChatCompletionRequest.functions;
        }
        return false;
    }

    public Integer hashCode() {
        Integer hashCode = 43;
        hashCode = (17 * hashCode) + (messages == null ? 0 : System.hashCode(messages));
        hashCode = (17 * hashCode) + (model == null ? 0 : System.hashCode(model));
        hashCode = (17 * hashCode) + (store == null ? 0 : System.hashCode(store));
        hashCode = (17 * hashCode) + (reasoningEffort == null ? 0 : System.hashCode(reasoningEffort));
        hashCode = (17 * hashCode) + (metadata == null ? 0 : System.hashCode(metadata));
        hashCode = (17 * hashCode) + (frequencyPenalty == null ? 0 : System.hashCode(frequencyPenalty));
        hashCode = (17 * hashCode) + (logitBias == null ? 0 : System.hashCode(logitBias));
        hashCode = (17 * hashCode) + (logprobs == null ? 0 : System.hashCode(logprobs));
        hashCode = (17 * hashCode) + (topLogprobs == null ? 0 : System.hashCode(topLogprobs));
        hashCode = (17 * hashCode) + (maxTokens == null ? 0 : System.hashCode(maxTokens));
        hashCode = (17 * hashCode) + (maxCompletionTokens == null ? 0 : System.hashCode(maxCompletionTokens));
        hashCode = (17 * hashCode) + (n == null ? 0 : System.hashCode(n));
        hashCode = (17 * hashCode) + (modalities == null ? 0 : System.hashCode(modalities));
        hashCode = (17 * hashCode) + (prediction == null ? 0 : System.hashCode(prediction));
        hashCode = (17 * hashCode) + (audio == null ? 0 : System.hashCode(audio));
        hashCode = (17 * hashCode) + (presencePenalty == null ? 0 : System.hashCode(presencePenalty));
        hashCode = (17 * hashCode) + (responseFormat == null ? 0 : System.hashCode(responseFormat));
        hashCode = (17 * hashCode) + (seed == null ? 0 : System.hashCode(seed));
        hashCode = (17 * hashCode) + (serviceTier == null ? 0 : System.hashCode(serviceTier));
        hashCode = (17 * hashCode) + (stop == null ? 0 : System.hashCode(stop));
        hashCode = (17 * hashCode) + (stream == null ? 0 : System.hashCode(stream));
        hashCode = (17 * hashCode) + (streamOptions == null ? 0 : System.hashCode(streamOptions));
        hashCode = (17 * hashCode) + (temperature == null ? 0 : System.hashCode(temperature));
        hashCode = (17 * hashCode) + (topP == null ? 0 : System.hashCode(topP));
        hashCode = (17 * hashCode) + (tools == null ? 0 : System.hashCode(tools));
        hashCode = (17 * hashCode) + (toolChoice == null ? 0 : System.hashCode(toolChoice));
        hashCode = (17 * hashCode) + (parallelToolCalls == null ? 0 : System.hashCode(parallelToolCalls));
        hashCode = (17 * hashCode) + (user == null ? 0 : System.hashCode(user));
        hashCode = (17 * hashCode) + (functionCall == null ? 0 : System.hashCode(functionCall));
        hashCode = (17 * hashCode) + (functions == null ? 0 : System.hashCode(functions));
        return hashCode;
    }
}

