/**
* OpenAI API
* The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
*
* The version of the OpenAPI document: 2.3.0
* Contact: blah+oapicf@cliffano.com
*
* NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
* https://openapi-generator.tech
* Do not edit the class manually.
*/
/*
 * CreateBatch_request.h
 *
 * 
 */

#ifndef CreateBatch_request_H_
#define CreateBatch_request_H_


#include <string>
#include <map>
#include <nlohmann/json.hpp>

namespace org::openapitools::server::model
{

/// <summary>
/// 
/// </summary>
class  CreateBatch_request
{
public:
    CreateBatch_request();
    virtual ~CreateBatch_request() = default;


    /// <summary>
    /// Validate the current data in the model. Throws a ValidationException on failure.
    /// </summary>
    void validate() const;

    /// <summary>
    /// Validate the current data in the model. Returns false on error and writes an error
    /// message into the given stringstream.
    /// </summary>
    bool validate(std::stringstream& msg) const;

    /// <summary>
    /// Helper overload for validate. Used when one model stores another model and calls it's validate.
    /// Not meant to be called outside that case.
    /// </summary>
    bool validate(std::stringstream& msg, const std::string& pathPrefix) const;

    bool operator==(const CreateBatch_request& rhs) const;
    bool operator!=(const CreateBatch_request& rhs) const;

    /////////////////////////////////////////////
    /// CreateBatch_request members

    /// <summary>
    /// The ID of an uploaded file that contains requests for the new batch.  See [upload file](/docs/api-reference/files/create) for how to upload a file.  Your input file must be formatted as a [JSONL file](/docs/api-reference/batch/request-input), and must be uploaded with the purpose &#x60;batch&#x60;. The file can contain up to 50,000 requests, and can be up to 200 MB in size. 
    /// </summary>
    std::string getInputFileId() const;
    void setInputFileId(std::string const& value);
    /// <summary>
    /// The endpoint to be used for all requests in the batch. Currently &#x60;/v1/chat/completions&#x60;, &#x60;/v1/embeddings&#x60;, and &#x60;/v1/completions&#x60; are supported. Note that &#x60;/v1/embeddings&#x60; batches are also restricted to a maximum of 50,000 embedding inputs across all requests in the batch.
    /// </summary>
    std::string getEndpoint() const;
    void setEndpoint(std::string const& value);
    /// <summary>
    /// The time frame within which the batch should be processed. Currently only &#x60;24h&#x60; is supported.
    /// </summary>
    std::string getCompletionWindow() const;
    void setCompletionWindow(std::string const& value);
    /// <summary>
    /// Optional custom metadata for the batch.
    /// </summary>
    std::map<std::string, std::string> getMetadata() const;
    void setMetadata(std::map<std::string, std::string> const& value);
    bool metadataIsSet() const;
    void unsetMetadata();

    friend  void to_json(nlohmann::json& j, const CreateBatch_request& o);
    friend  void from_json(const nlohmann::json& j, CreateBatch_request& o);
protected:
    std::string m_Input_file_id;

    std::string m_Endpoint;

    std::string m_Completion_window;

    std::map<std::string, std::string> m_Metadata;
    bool m_MetadataIsSet;
    
};

} // namespace org::openapitools::server::model

#endif /* CreateBatch_request_H_ */
