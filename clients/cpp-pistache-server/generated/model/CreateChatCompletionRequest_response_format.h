/**
* OpenAI API
* The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
*
* The version of the OpenAPI document: 2.3.0
* Contact: blah+oapicf@cliffano.com
*
* NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
* https://openapi-generator.tech
* Do not edit the class manually.
*/
/*
 * CreateChatCompletionRequest_response_format.h
 *
 * An object specifying the format that the model must output.  Setting to &#x60;{ \&quot;type\&quot;: \&quot;json_schema\&quot;, \&quot;json_schema\&quot;: {...} }&#x60; enables Structured Outputs which ensures the model will match your supplied JSON schema. Learn more in the [Structured Outputs guide](/docs/guides/structured-outputs).  Setting to &#x60;{ \&quot;type\&quot;: \&quot;json_object\&quot; }&#x60; enables JSON mode, which ensures the message the model generates is valid JSON.  **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \&quot;stuck\&quot; request. Also note that the message content may be partially cut off if &#x60;finish_reason&#x3D;\&quot;length\&quot;&#x60;, which indicates the generation exceeded &#x60;max_tokens&#x60; or the conversation exceeded the max context length. 
 */

#ifndef CreateChatCompletionRequest_response_format_H_
#define CreateChatCompletionRequest_response_format_H_


#include <string>
#include "ResponseFormatJsonSchema.h"
#include "ResponseFormatText.h"
#include "ResponseFormatJsonObject.h"
#include "ResponseFormatJsonSchema_json_schema.h"
#include <nlohmann/json.hpp>

namespace org::openapitools::server::model
{

/// <summary>
/// An object specifying the format that the model must output.  Setting to &#x60;{ \&quot;type\&quot;: \&quot;json_schema\&quot;, \&quot;json_schema\&quot;: {...} }&#x60; enables Structured Outputs which ensures the model will match your supplied JSON schema. Learn more in the [Structured Outputs guide](/docs/guides/structured-outputs).  Setting to &#x60;{ \&quot;type\&quot;: \&quot;json_object\&quot; }&#x60; enables JSON mode, which ensures the message the model generates is valid JSON.  **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \&quot;stuck\&quot; request. Also note that the message content may be partially cut off if &#x60;finish_reason&#x3D;\&quot;length\&quot;&#x60;, which indicates the generation exceeded &#x60;max_tokens&#x60; or the conversation exceeded the max context length. 
/// </summary>
class  CreateChatCompletionRequest_response_format
{
public:
    CreateChatCompletionRequest_response_format();
    virtual ~CreateChatCompletionRequest_response_format() = default;


    /// <summary>
    /// Validate the current data in the model. Throws a ValidationException on failure.
    /// </summary>
    void validate() const;

    /// <summary>
    /// Validate the current data in the model. Returns false on error and writes an error
    /// message into the given stringstream.
    /// </summary>
    bool validate(std::stringstream& msg) const;

    /// <summary>
    /// Helper overload for validate. Used when one model stores another model and calls it's validate.
    /// Not meant to be called outside that case.
    /// </summary>
    bool validate(std::stringstream& msg, const std::string& pathPrefix) const;

    bool operator==(const CreateChatCompletionRequest_response_format& rhs) const;
    bool operator!=(const CreateChatCompletionRequest_response_format& rhs) const;

    /////////////////////////////////////////////
    /// CreateChatCompletionRequest_response_format members

    /// <summary>
    /// The type of response format being defined: &#x60;text&#x60;
    /// </summary>
    std::string getType() const;
    void setType(std::string const& value);
    /// <summary>
    /// 
    /// </summary>
    org::openapitools::server::model::ResponseFormatJsonSchema_json_schema getJsonSchema() const;
    void setJsonSchema(org::openapitools::server::model::ResponseFormatJsonSchema_json_schema const& value);

    friend  void to_json(nlohmann::json& j, const CreateChatCompletionRequest_response_format& o);
    friend  void from_json(const nlohmann::json& j, CreateChatCompletionRequest_response_format& o);
protected:
    std::string m_Type;

    org::openapitools::server::model::ResponseFormatJsonSchema_json_schema m_Json_schema;

    
};

} // namespace org::openapitools::server::model

#endif /* CreateChatCompletionRequest_response_format_H_ */
