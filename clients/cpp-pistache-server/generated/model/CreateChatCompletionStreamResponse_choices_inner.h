/**
* OpenAI API
* The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
*
* The version of the OpenAPI document: 2.0.0
* Contact: blah+oapicf@cliffano.com
*
* NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
* https://openapi-generator.tech
* Do not edit the class manually.
*/
/*
 * CreateChatCompletionStreamResponse_choices_inner.h
 *
 * 
 */

#ifndef CreateChatCompletionStreamResponse_choices_inner_H_
#define CreateChatCompletionStreamResponse_choices_inner_H_


#include <string>
#include "CreateChatCompletionResponse_choices_inner_logprobs.h"
#include "ChatCompletionStreamResponseDelta.h"
#include <nlohmann/json.hpp>

namespace org::openapitools::server::model
{

/// <summary>
/// 
/// </summary>
class  CreateChatCompletionStreamResponse_choices_inner
{
public:
    CreateChatCompletionStreamResponse_choices_inner();
    virtual ~CreateChatCompletionStreamResponse_choices_inner() = default;


    /// <summary>
    /// Validate the current data in the model. Throws a ValidationException on failure.
    /// </summary>
    void validate() const;

    /// <summary>
    /// Validate the current data in the model. Returns false on error and writes an error
    /// message into the given stringstream.
    /// </summary>
    bool validate(std::stringstream& msg) const;

    /// <summary>
    /// Helper overload for validate. Used when one model stores another model and calls it's validate.
    /// Not meant to be called outside that case.
    /// </summary>
    bool validate(std::stringstream& msg, const std::string& pathPrefix) const;

    bool operator==(const CreateChatCompletionStreamResponse_choices_inner& rhs) const;
    bool operator!=(const CreateChatCompletionStreamResponse_choices_inner& rhs) const;

    /////////////////////////////////////////////
    /// CreateChatCompletionStreamResponse_choices_inner members

    /// <summary>
    /// 
    /// </summary>
    org::openapitools::server::model::ChatCompletionStreamResponseDelta getDelta() const;
    void setDelta(org::openapitools::server::model::ChatCompletionStreamResponseDelta const& value);
    /// <summary>
    /// 
    /// </summary>
    org::openapitools::server::model::CreateChatCompletionResponse_choices_inner_logprobs getLogprobs() const;
    void setLogprobs(org::openapitools::server::model::CreateChatCompletionResponse_choices_inner_logprobs const& value);
    bool logprobsIsSet() const;
    void unsetLogprobs();
    /// <summary>
    /// The reason the model stopped generating tokens. This will be &#x60;stop&#x60; if the model hit a natural stop point or a provided stop sequence, &#x60;length&#x60; if the maximum number of tokens specified in the request was reached, &#x60;content_filter&#x60; if content was omitted due to a flag from our content filters, &#x60;tool_calls&#x60; if the model called a tool, or &#x60;function_call&#x60; (deprecated) if the model called a function. 
    /// </summary>
    std::string getFinishReason() const;
    void setFinishReason(std::string const& value);
    /// <summary>
    /// The index of the choice in the list of choices.
    /// </summary>
    int32_t getIndex() const;
    void setIndex(int32_t const value);

    friend  void to_json(nlohmann::json& j, const CreateChatCompletionStreamResponse_choices_inner& o);
    friend  void from_json(const nlohmann::json& j, CreateChatCompletionStreamResponse_choices_inner& o);
protected:
    org::openapitools::server::model::ChatCompletionStreamResponseDelta m_Delta;

    org::openapitools::server::model::CreateChatCompletionResponse_choices_inner_logprobs m_Logprobs;
    bool m_LogprobsIsSet;
    std::string m_Finish_reason;

    int32_t m_Index;

    
};

} // namespace org::openapitools::server::model

#endif /* CreateChatCompletionStreamResponse_choices_inner_H_ */
