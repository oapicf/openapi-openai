/**
* OpenAI API
* The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
*
* The version of the OpenAPI document: 2.3.0
* Contact: blah+oapicf@cliffano.com
*
* NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
* https://openapi-generator.tech
* Do not edit the class manually.
*/
/*
 * CreateRunRequest.h
 *
 * 
 */

#ifndef CreateRunRequest_H_
#define CreateRunRequest_H_


#include <nlohmann/json.hpp>
#include "CreateRunRequest_model.h"
#include "TruncationObject.h"
#include "AssistantObject_tools_inner.h"
#include <string>
#include "AssistantsApiResponseFormatOption.h"
#include "CreateMessageRequest.h"
#include "AssistantsApiToolChoiceOption.h"
#include <vector>
#include <nlohmann/json.hpp>

namespace org::openapitools::server::model
{

/// <summary>
/// 
/// </summary>
class  CreateRunRequest
{
public:
    CreateRunRequest();
    virtual ~CreateRunRequest() = default;


    /// <summary>
    /// Validate the current data in the model. Throws a ValidationException on failure.
    /// </summary>
    void validate() const;

    /// <summary>
    /// Validate the current data in the model. Returns false on error and writes an error
    /// message into the given stringstream.
    /// </summary>
    bool validate(std::stringstream& msg) const;

    /// <summary>
    /// Helper overload for validate. Used when one model stores another model and calls it's validate.
    /// Not meant to be called outside that case.
    /// </summary>
    bool validate(std::stringstream& msg, const std::string& pathPrefix) const;

    bool operator==(const CreateRunRequest& rhs) const;
    bool operator!=(const CreateRunRequest& rhs) const;

    /////////////////////////////////////////////
    /// CreateRunRequest members

    /// <summary>
    /// The ID of the [assistant](/docs/api-reference/assistants) to use to execute this run.
    /// </summary>
    std::string getAssistantId() const;
    void setAssistantId(std::string const& value);
    /// <summary>
    /// 
    /// </summary>
    org::openapitools::server::model::CreateRunRequest_model getModel() const;
    void setModel(org::openapitools::server::model::CreateRunRequest_model const& value);
    bool modelIsSet() const;
    void unsetModel();
    /// <summary>
    /// Overrides the [instructions](/docs/api-reference/assistants/createAssistant) of the assistant. This is useful for modifying the behavior on a per-run basis.
    /// </summary>
    std::string getInstructions() const;
    void setInstructions(std::string const& value);
    bool instructionsIsSet() const;
    void unsetInstructions();
    /// <summary>
    /// Appends additional instructions at the end of the instructions for the run. This is useful for modifying the behavior on a per-run basis without overriding other instructions.
    /// </summary>
    std::string getAdditionalInstructions() const;
    void setAdditionalInstructions(std::string const& value);
    bool additionalInstructionsIsSet() const;
    void unsetAdditional_instructions();
    /// <summary>
    /// Adds additional messages to the thread before creating the run.
    /// </summary>
    std::vector<org::openapitools::server::model::CreateMessageRequest> getAdditionalMessages() const;
    void setAdditionalMessages(std::vector<org::openapitools::server::model::CreateMessageRequest> const& value);
    bool additionalMessagesIsSet() const;
    void unsetAdditional_messages();
    /// <summary>
    /// Override the tools the assistant can use for this run. This is useful for modifying the behavior on a per-run basis.
    /// </summary>
    std::vector<org::openapitools::server::model::AssistantObject_tools_inner> getTools() const;
    void setTools(std::vector<org::openapitools::server::model::AssistantObject_tools_inner> const& value);
    bool toolsIsSet() const;
    void unsetTools();
    /// <summary>
    /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
    /// </summary>
    nlohmann::json getMetadata() const;
    void setMetadata(nlohmann::json const& value);
    bool metadataIsSet() const;
    void unsetMetadata();
    /// <summary>
    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. 
    /// </summary>
    double getTemperature() const;
    void setTemperature(double const value);
    bool temperatureIsSet() const;
    void unsetTemperature();
    /// <summary>
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or temperature but not both. 
    /// </summary>
    double getTopP() const;
    void setTopP(double const value);
    bool topPIsSet() const;
    void unsetTop_p();
    /// <summary>
    /// If &#x60;true&#x60;, returns a stream of events that happen during the Run as server-sent events, terminating when the Run enters a terminal state with a &#x60;data: [DONE]&#x60; message. 
    /// </summary>
    bool isStream() const;
    void setStream(bool const value);
    bool streamIsSet() const;
    void unsetStream();
    /// <summary>
    /// The maximum number of prompt tokens that may be used over the course of the run. The run will make a best effort to use only the number of prompt tokens specified, across multiple turns of the run. If the run exceeds the number of prompt tokens specified, the run will end with status &#x60;incomplete&#x60;. See &#x60;incomplete_details&#x60; for more info. 
    /// </summary>
    int32_t getMaxPromptTokens() const;
    void setMaxPromptTokens(int32_t const value);
    bool maxPromptTokensIsSet() const;
    void unsetMax_prompt_tokens();
    /// <summary>
    /// The maximum number of completion tokens that may be used over the course of the run. The run will make a best effort to use only the number of completion tokens specified, across multiple turns of the run. If the run exceeds the number of completion tokens specified, the run will end with status &#x60;incomplete&#x60;. See &#x60;incomplete_details&#x60; for more info. 
    /// </summary>
    int32_t getMaxCompletionTokens() const;
    void setMaxCompletionTokens(int32_t const value);
    bool maxCompletionTokensIsSet() const;
    void unsetMax_completion_tokens();
    /// <summary>
    /// 
    /// </summary>
    org::openapitools::server::model::TruncationObject getTruncationStrategy() const;
    void setTruncationStrategy(org::openapitools::server::model::TruncationObject const& value);
    bool truncationStrategyIsSet() const;
    void unsetTruncation_strategy();
    /// <summary>
    /// 
    /// </summary>
    org::openapitools::server::model::AssistantsApiToolChoiceOption getToolChoice() const;
    void setToolChoice(org::openapitools::server::model::AssistantsApiToolChoiceOption const& value);
    bool toolChoiceIsSet() const;
    void unsetTool_choice();
    /// <summary>
    /// Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.
    /// </summary>
    bool isParallelToolCalls() const;
    void setParallelToolCalls(bool const value);
    bool parallelToolCallsIsSet() const;
    void unsetParallel_tool_calls();
    /// <summary>
    /// 
    /// </summary>
    org::openapitools::server::model::AssistantsApiResponseFormatOption getResponseFormat() const;
    void setResponseFormat(org::openapitools::server::model::AssistantsApiResponseFormatOption const& value);
    bool responseFormatIsSet() const;
    void unsetResponse_format();

    friend  void to_json(nlohmann::json& j, const CreateRunRequest& o);
    friend  void from_json(const nlohmann::json& j, CreateRunRequest& o);
protected:
    std::string m_Assistant_id;

    org::openapitools::server::model::CreateRunRequest_model m_Model;
    bool m_ModelIsSet;
    std::string m_Instructions;
    bool m_InstructionsIsSet;
    std::string m_Additional_instructions;
    bool m_Additional_instructionsIsSet;
    std::vector<org::openapitools::server::model::CreateMessageRequest> m_Additional_messages;
    bool m_Additional_messagesIsSet;
    std::vector<org::openapitools::server::model::AssistantObject_tools_inner> m_Tools;
    bool m_ToolsIsSet;
    nlohmann::json m_Metadata;
    bool m_MetadataIsSet;
    double m_Temperature;
    bool m_TemperatureIsSet;
    double m_Top_p;
    bool m_Top_pIsSet;
    bool m_Stream;
    bool m_StreamIsSet;
    int32_t m_Max_prompt_tokens;
    bool m_Max_prompt_tokensIsSet;
    int32_t m_Max_completion_tokens;
    bool m_Max_completion_tokensIsSet;
    org::openapitools::server::model::TruncationObject m_Truncation_strategy;
    bool m_Truncation_strategyIsSet;
    org::openapitools::server::model::AssistantsApiToolChoiceOption m_Tool_choice;
    bool m_Tool_choiceIsSet;
    bool m_Parallel_tool_calls;
    bool m_Parallel_tool_callsIsSet;
    org::openapitools::server::model::AssistantsApiResponseFormatOption m_Response_format;
    bool m_Response_formatIsSet;
    
};

} // namespace org::openapitools::server::model

#endif /* CreateRunRequest_H_ */
