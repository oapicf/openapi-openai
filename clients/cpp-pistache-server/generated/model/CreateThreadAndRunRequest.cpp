/**
* OpenAI API
* The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
*
* The version of the OpenAPI document: 2.0.0
* Contact: blah+oapicf@cliffano.com
*
* NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
* https://openapi-generator.tech
* Do not edit the class manually.
*/


#include "CreateThreadAndRunRequest.h"
#include "Helpers.h"

#include <sstream>

namespace org::openapitools::server::model
{

CreateThreadAndRunRequest::CreateThreadAndRunRequest()
{
    m_Assistant_id = "";
    m_ThreadIsSet = false;
    m_ModelIsSet = false;
    m_Instructions = "";
    m_InstructionsIsSet = false;
    m_ToolsIsSet = false;
    m_MetadataIsSet = false;
    m_Temperature = 1;
    m_TemperatureIsSet = false;
    m_Stream = false;
    m_StreamIsSet = false;
    m_Max_prompt_tokens = 0;
    m_Max_prompt_tokensIsSet = false;
    m_Max_completion_tokens = 0;
    m_Max_completion_tokensIsSet = false;
    m_Truncation_strategyIsSet = false;
    m_Tool_choiceIsSet = false;
    m_Response_formatIsSet = false;
    
}

void CreateThreadAndRunRequest::validate() const
{
    std::stringstream msg;
    if (!validate(msg))
    {
        throw org::openapitools::server::helpers::ValidationException(msg.str());
    }
}

bool CreateThreadAndRunRequest::validate(std::stringstream& msg) const
{
    return validate(msg, "");
}

bool CreateThreadAndRunRequest::validate(std::stringstream& msg, const std::string& pathPrefix) const
{
    bool success = true;
    const std::string _pathPrefix = pathPrefix.empty() ? "CreateThreadAndRunRequest" : pathPrefix;

                         
    if (toolsIsSet())
    {
        const std::vector<org::openapitools::server::model::CreateThreadAndRunRequest_tools_inner>& value = m_Tools;
        const std::string currentValuePath = _pathPrefix + ".tools";
                
        
        if (value.size() > 20)
        {
            success = false;
            msg << currentValuePath << ": must have at most 20 elements;";
        }
        { // Recursive validation of array elements
            const std::string oldValuePath = currentValuePath;
            int i = 0;
            for (const org::openapitools::server::model::CreateThreadAndRunRequest_tools_inner& value : value)
            { 
                const std::string currentValuePath = oldValuePath + "[" + std::to_string(i) + "]";
                        
        success = value.validate(msg, currentValuePath + ".tools") && success;
 
                i++;
            }
        }

    }
             
    if (temperatureIsSet())
    {
        const double& value = m_Temperature;
        const std::string currentValuePath = _pathPrefix + ".temperature";
                
        
        if (value < 0)
        {
            success = false;
            msg << currentValuePath << ": must be greater than or equal to 0;";
        }
        if (value > 2)
        {
            success = false;
            msg << currentValuePath << ": must be less than or equal to 2;";
        }

    }
             
    if (maxPromptTokensIsSet())
    {
        const int32_t& value = m_Max_prompt_tokens;
        const std::string currentValuePath = _pathPrefix + ".maxPromptTokens";
                
        
        if (value < 256)
        {
            success = false;
            msg << currentValuePath << ": must be greater than or equal to 256;";
        }

    }
         
    if (maxCompletionTokensIsSet())
    {
        const int32_t& value = m_Max_completion_tokens;
        const std::string currentValuePath = _pathPrefix + ".maxCompletionTokens";
                
        
        if (value < 256)
        {
            success = false;
            msg << currentValuePath << ": must be greater than or equal to 256;";
        }

    }
                
    return success;
}

bool CreateThreadAndRunRequest::operator==(const CreateThreadAndRunRequest& rhs) const
{
    return
    
    
    (getAssistantId() == rhs.getAssistantId())
     &&
    
    
    ((!threadIsSet() && !rhs.threadIsSet()) || (threadIsSet() && rhs.threadIsSet() && getThread() == rhs.getThread())) &&
    
    
    ((!modelIsSet() && !rhs.modelIsSet()) || (modelIsSet() && rhs.modelIsSet() && getModel() == rhs.getModel())) &&
    
    
    ((!instructionsIsSet() && !rhs.instructionsIsSet()) || (instructionsIsSet() && rhs.instructionsIsSet() && getInstructions() == rhs.getInstructions())) &&
    
    
    ((!toolsIsSet() && !rhs.toolsIsSet()) || (toolsIsSet() && rhs.toolsIsSet() && getTools() == rhs.getTools())) &&
    
    
    ((!metadataIsSet() && !rhs.metadataIsSet()) || (metadataIsSet() && rhs.metadataIsSet() && getMetadata() == rhs.getMetadata())) &&
    
    
    ((!temperatureIsSet() && !rhs.temperatureIsSet()) || (temperatureIsSet() && rhs.temperatureIsSet() && getTemperature() == rhs.getTemperature())) &&
    
    
    ((!streamIsSet() && !rhs.streamIsSet()) || (streamIsSet() && rhs.streamIsSet() && isStream() == rhs.isStream())) &&
    
    
    ((!maxPromptTokensIsSet() && !rhs.maxPromptTokensIsSet()) || (maxPromptTokensIsSet() && rhs.maxPromptTokensIsSet() && getMaxPromptTokens() == rhs.getMaxPromptTokens())) &&
    
    
    ((!maxCompletionTokensIsSet() && !rhs.maxCompletionTokensIsSet()) || (maxCompletionTokensIsSet() && rhs.maxCompletionTokensIsSet() && getMaxCompletionTokens() == rhs.getMaxCompletionTokens())) &&
    
    
    ((!truncationStrategyIsSet() && !rhs.truncationStrategyIsSet()) || (truncationStrategyIsSet() && rhs.truncationStrategyIsSet() && getTruncationStrategy() == rhs.getTruncationStrategy())) &&
    
    
    ((!toolChoiceIsSet() && !rhs.toolChoiceIsSet()) || (toolChoiceIsSet() && rhs.toolChoiceIsSet() && getToolChoice() == rhs.getToolChoice())) &&
    
    
    ((!responseFormatIsSet() && !rhs.responseFormatIsSet()) || (responseFormatIsSet() && rhs.responseFormatIsSet() && getResponseFormat() == rhs.getResponseFormat()))
    
    ;
}

bool CreateThreadAndRunRequest::operator!=(const CreateThreadAndRunRequest& rhs) const
{
    return !(*this == rhs);
}

void to_json(nlohmann::json& j, const CreateThreadAndRunRequest& o)
{
    j = nlohmann::json::object();
    j["assistant_id"] = o.m_Assistant_id;
    if(o.threadIsSet())
        j["thread"] = o.m_Thread;
    if(o.modelIsSet())
        j["model"] = o.m_Model;
    if(o.instructionsIsSet())
        j["instructions"] = o.m_Instructions;
    if(o.toolsIsSet() || !o.m_Tools.empty())
        j["tools"] = o.m_Tools;
    if(o.metadataIsSet())
        j["metadata"] = o.m_Metadata;
    if(o.temperatureIsSet())
        j["temperature"] = o.m_Temperature;
    if(o.streamIsSet())
        j["stream"] = o.m_Stream;
    if(o.maxPromptTokensIsSet())
        j["max_prompt_tokens"] = o.m_Max_prompt_tokens;
    if(o.maxCompletionTokensIsSet())
        j["max_completion_tokens"] = o.m_Max_completion_tokens;
    if(o.truncationStrategyIsSet())
        j["truncation_strategy"] = o.m_Truncation_strategy;
    if(o.toolChoiceIsSet())
        j["tool_choice"] = o.m_Tool_choice;
    if(o.responseFormatIsSet())
        j["response_format"] = o.m_Response_format;
    
}

void from_json(const nlohmann::json& j, CreateThreadAndRunRequest& o)
{
    j.at("assistant_id").get_to(o.m_Assistant_id);
    if(j.find("thread") != j.end())
    {
        j.at("thread").get_to(o.m_Thread);
        o.m_ThreadIsSet = true;
    } 
    if(j.find("model") != j.end())
    {
        j.at("model").get_to(o.m_Model);
        o.m_ModelIsSet = true;
    } 
    if(j.find("instructions") != j.end())
    {
        j.at("instructions").get_to(o.m_Instructions);
        o.m_InstructionsIsSet = true;
    } 
    if(j.find("tools") != j.end())
    {
        j.at("tools").get_to(o.m_Tools);
        o.m_ToolsIsSet = true;
    } 
    if(j.find("metadata") != j.end())
    {
        j.at("metadata").get_to(o.m_Metadata);
        o.m_MetadataIsSet = true;
    } 
    if(j.find("temperature") != j.end())
    {
        j.at("temperature").get_to(o.m_Temperature);
        o.m_TemperatureIsSet = true;
    } 
    if(j.find("stream") != j.end())
    {
        j.at("stream").get_to(o.m_Stream);
        o.m_StreamIsSet = true;
    } 
    if(j.find("max_prompt_tokens") != j.end())
    {
        j.at("max_prompt_tokens").get_to(o.m_Max_prompt_tokens);
        o.m_Max_prompt_tokensIsSet = true;
    } 
    if(j.find("max_completion_tokens") != j.end())
    {
        j.at("max_completion_tokens").get_to(o.m_Max_completion_tokens);
        o.m_Max_completion_tokensIsSet = true;
    } 
    if(j.find("truncation_strategy") != j.end())
    {
        j.at("truncation_strategy").get_to(o.m_Truncation_strategy);
        o.m_Truncation_strategyIsSet = true;
    } 
    if(j.find("tool_choice") != j.end())
    {
        j.at("tool_choice").get_to(o.m_Tool_choice);
        o.m_Tool_choiceIsSet = true;
    } 
    if(j.find("response_format") != j.end())
    {
        j.at("response_format").get_to(o.m_Response_format);
        o.m_Response_formatIsSet = true;
    } 
    
}

std::string CreateThreadAndRunRequest::getAssistantId() const
{
    return m_Assistant_id;
}
void CreateThreadAndRunRequest::setAssistantId(std::string const& value)
{
    m_Assistant_id = value;
}
org::openapitools::server::model::CreateThreadRequest CreateThreadAndRunRequest::getThread() const
{
    return m_Thread;
}
void CreateThreadAndRunRequest::setThread(org::openapitools::server::model::CreateThreadRequest const& value)
{
    m_Thread = value;
    m_ThreadIsSet = true;
}
bool CreateThreadAndRunRequest::threadIsSet() const
{
    return m_ThreadIsSet;
}
void CreateThreadAndRunRequest::unsetThread()
{
    m_ThreadIsSet = false;
}
org::openapitools::server::model::CreateRunRequest_model CreateThreadAndRunRequest::getModel() const
{
    return m_Model;
}
void CreateThreadAndRunRequest::setModel(org::openapitools::server::model::CreateRunRequest_model const& value)
{
    m_Model = value;
    m_ModelIsSet = true;
}
bool CreateThreadAndRunRequest::modelIsSet() const
{
    return m_ModelIsSet;
}
void CreateThreadAndRunRequest::unsetModel()
{
    m_ModelIsSet = false;
}
std::string CreateThreadAndRunRequest::getInstructions() const
{
    return m_Instructions;
}
void CreateThreadAndRunRequest::setInstructions(std::string const& value)
{
    m_Instructions = value;
    m_InstructionsIsSet = true;
}
bool CreateThreadAndRunRequest::instructionsIsSet() const
{
    return m_InstructionsIsSet;
}
void CreateThreadAndRunRequest::unsetInstructions()
{
    m_InstructionsIsSet = false;
}
std::vector<org::openapitools::server::model::CreateThreadAndRunRequest_tools_inner> CreateThreadAndRunRequest::getTools() const
{
    return m_Tools;
}
void CreateThreadAndRunRequest::setTools(std::vector<org::openapitools::server::model::CreateThreadAndRunRequest_tools_inner> const& value)
{
    m_Tools = value;
    m_ToolsIsSet = true;
}
bool CreateThreadAndRunRequest::toolsIsSet() const
{
    return m_ToolsIsSet;
}
void CreateThreadAndRunRequest::unsetTools()
{
    m_ToolsIsSet = false;
}
nlohmann::json CreateThreadAndRunRequest::getMetadata() const
{
    return m_Metadata;
}
void CreateThreadAndRunRequest::setMetadata(nlohmann::json const& value)
{
    m_Metadata = value;
    m_MetadataIsSet = true;
}
bool CreateThreadAndRunRequest::metadataIsSet() const
{
    return m_MetadataIsSet;
}
void CreateThreadAndRunRequest::unsetMetadata()
{
    m_MetadataIsSet = false;
}
double CreateThreadAndRunRequest::getTemperature() const
{
    return m_Temperature;
}
void CreateThreadAndRunRequest::setTemperature(double const value)
{
    m_Temperature = value;
    m_TemperatureIsSet = true;
}
bool CreateThreadAndRunRequest::temperatureIsSet() const
{
    return m_TemperatureIsSet;
}
void CreateThreadAndRunRequest::unsetTemperature()
{
    m_TemperatureIsSet = false;
}
bool CreateThreadAndRunRequest::isStream() const
{
    return m_Stream;
}
void CreateThreadAndRunRequest::setStream(bool const value)
{
    m_Stream = value;
    m_StreamIsSet = true;
}
bool CreateThreadAndRunRequest::streamIsSet() const
{
    return m_StreamIsSet;
}
void CreateThreadAndRunRequest::unsetStream()
{
    m_StreamIsSet = false;
}
int32_t CreateThreadAndRunRequest::getMaxPromptTokens() const
{
    return m_Max_prompt_tokens;
}
void CreateThreadAndRunRequest::setMaxPromptTokens(int32_t const value)
{
    m_Max_prompt_tokens = value;
    m_Max_prompt_tokensIsSet = true;
}
bool CreateThreadAndRunRequest::maxPromptTokensIsSet() const
{
    return m_Max_prompt_tokensIsSet;
}
void CreateThreadAndRunRequest::unsetMax_prompt_tokens()
{
    m_Max_prompt_tokensIsSet = false;
}
int32_t CreateThreadAndRunRequest::getMaxCompletionTokens() const
{
    return m_Max_completion_tokens;
}
void CreateThreadAndRunRequest::setMaxCompletionTokens(int32_t const value)
{
    m_Max_completion_tokens = value;
    m_Max_completion_tokensIsSet = true;
}
bool CreateThreadAndRunRequest::maxCompletionTokensIsSet() const
{
    return m_Max_completion_tokensIsSet;
}
void CreateThreadAndRunRequest::unsetMax_completion_tokens()
{
    m_Max_completion_tokensIsSet = false;
}
org::openapitools::server::model::TruncationObject CreateThreadAndRunRequest::getTruncationStrategy() const
{
    return m_Truncation_strategy;
}
void CreateThreadAndRunRequest::setTruncationStrategy(org::openapitools::server::model::TruncationObject const& value)
{
    m_Truncation_strategy = value;
    m_Truncation_strategyIsSet = true;
}
bool CreateThreadAndRunRequest::truncationStrategyIsSet() const
{
    return m_Truncation_strategyIsSet;
}
void CreateThreadAndRunRequest::unsetTruncation_strategy()
{
    m_Truncation_strategyIsSet = false;
}
org::openapitools::server::model::AssistantsApiToolChoiceOption CreateThreadAndRunRequest::getToolChoice() const
{
    return m_Tool_choice;
}
void CreateThreadAndRunRequest::setToolChoice(org::openapitools::server::model::AssistantsApiToolChoiceOption const& value)
{
    m_Tool_choice = value;
    m_Tool_choiceIsSet = true;
}
bool CreateThreadAndRunRequest::toolChoiceIsSet() const
{
    return m_Tool_choiceIsSet;
}
void CreateThreadAndRunRequest::unsetTool_choice()
{
    m_Tool_choiceIsSet = false;
}
org::openapitools::server::model::AssistantsApiResponseFormatOption CreateThreadAndRunRequest::getResponseFormat() const
{
    return m_Response_format;
}
void CreateThreadAndRunRequest::setResponseFormat(org::openapitools::server::model::AssistantsApiResponseFormatOption const& value)
{
    m_Response_format = value;
    m_Response_formatIsSet = true;
}
bool CreateThreadAndRunRequest::responseFormatIsSet() const
{
    return m_Response_formatIsSet;
}
void CreateThreadAndRunRequest::unsetResponse_format()
{
    m_Response_formatIsSet = false;
}


} // namespace org::openapitools::server::model

