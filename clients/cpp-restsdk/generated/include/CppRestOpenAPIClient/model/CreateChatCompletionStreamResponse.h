/**
 * OpenAI API
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.3.0
 * Contact: blah+oapicf@cliffano.com
 *
 * NOTE: This class is auto generated by OpenAPI-Generator 7.18.0.
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */

/*
 * CreateChatCompletionStreamResponse.h
 *
 * Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
 */

#ifndef ORG_OPENAPITOOLS_CLIENT_MODEL_CreateChatCompletionStreamResponse_H_
#define ORG_OPENAPITOOLS_CLIENT_MODEL_CreateChatCompletionStreamResponse_H_

#include <stdexcept>
#include <boost/optional.hpp>

#include "CppRestOpenAPIClient/ModelBase.h"

#include "CppRestOpenAPIClient/model/CreateChatCompletionStreamResponse_usage.h"
#include <cpprest/details/basic_types.h>
#include "CppRestOpenAPIClient/model/CreateChatCompletionStreamResponse_choices_inner.h"
#include <vector>

namespace org {
namespace openapitools {
namespace client {
namespace model {

class CreateChatCompletionStreamResponse_choices_inner;
class CreateChatCompletionStreamResponse_usage;


/// <summary>
/// Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
/// </summary>
class  CreateChatCompletionStreamResponse
    : public ModelBase
{
public:
    CreateChatCompletionStreamResponse();
    virtual ~CreateChatCompletionStreamResponse();

    /////////////////////////////////////////////
    /// ModelBase overrides

    void validate() override;

    web::json::value toJson() const override;
    bool fromJson(const web::json::value& json) override;

    void toMultipart(std::shared_ptr<MultipartFormData> multipart, const utility::string_t& namePrefix) const override;
    bool fromMultiPart(std::shared_ptr<MultipartFormData> multipart, const utility::string_t& namePrefix) override;


    /////////////////////////////////////////////
    /// CreateChatCompletionStreamResponse members

    enum class Service_tierEnum
    {
        SCALE,
        DEFAULT,
    };
    /// <summary>
    /// The service tier used for processing the request. This field is only included if the &#x60;service_tier&#x60; parameter is specified in the request.
    /// </summary>
    enum class ObjectEnum
    {
        CHAT_COMPLETION_CHUNK,
    };
    /// <summary>
    /// The object type, which is always &#x60;chat.completion.chunk&#x60;.
    /// </summary>

    Service_tierEnum toService_tierEnum(const utility::string_t& value) const;
    const utility::string_t fromService_tierEnum(const Service_tierEnum value) const;


    ObjectEnum toObjectEnum(const utility::string_t& value) const;
    const utility::string_t fromObjectEnum(const ObjectEnum value) const;


    /// <summary>
    /// A unique identifier for the chat completion. Each chunk has the same ID.
    /// </summary>
    utility::string_t getId() const;
    bool idIsSet() const;
    void unsetId();
    void setId(const utility::string_t& value);

    /// <summary>
    /// A list of chat completion choices. Can contain more than one elements if &#x60;n&#x60; is greater than 1. Can also be empty for the last chunk if you set &#x60;stream_options: {\&quot;include_usage\&quot;: true}&#x60;. 
    /// </summary>
    std::vector<std::shared_ptr<CreateChatCompletionStreamResponse_choices_inner>> getChoices() const;
    bool choicesIsSet() const;
    void unsetChoices();
    void setChoices(const std::vector<std::shared_ptr<CreateChatCompletionStreamResponse_choices_inner>>& value);

    /// <summary>
    /// The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
    /// </summary>
    int32_t getCreated() const;
    bool createdIsSet() const;
    void unsetCreated();
    void setCreated(int32_t value);

    /// <summary>
    /// The model to generate the completion.
    /// </summary>
    utility::string_t getModel() const;
    bool modelIsSet() const;
    void unsetModel();
    void setModel(const utility::string_t& value);

    /// <summary>
    /// The service tier used for processing the request. This field is only included if the &#x60;service_tier&#x60; parameter is specified in the request.
    /// </summary>
    Service_tierEnum getServiceTier() const;
    bool serviceTierIsSet() const;
    void unsetService_tier();
    void setServiceTier(const Service_tierEnum value);

    /// <summary>
    /// This fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the &#x60;seed&#x60; request parameter to understand when backend changes have been made that might impact determinism. 
    /// </summary>
    utility::string_t getSystemFingerprint() const;
    bool systemFingerprintIsSet() const;
    void unsetSystem_fingerprint();
    void setSystemFingerprint(const utility::string_t& value);

    /// <summary>
    /// The object type, which is always &#x60;chat.completion.chunk&#x60;.
    /// </summary>
    ObjectEnum getObject() const;
    bool objectIsSet() const;
    void unsetobject();
    void setObject(const ObjectEnum value);

    std::shared_ptr<CreateChatCompletionStreamResponse_usage> getUsage() const;
    bool usageIsSet() const;
    void unsetUsage();
    void setUsage(const std::shared_ptr<CreateChatCompletionStreamResponse_usage>& value);


protected:
    utility::string_t m_Id;
    bool m_IdIsSet;

    std::vector<std::shared_ptr<CreateChatCompletionStreamResponse_choices_inner>> m_Choices;
    bool m_ChoicesIsSet;

    int32_t m_Created;
    bool m_CreatedIsSet;

    utility::string_t m_Model;
    bool m_ModelIsSet;

    boost::optional<Service_tierEnum> m_Service_tier;

    utility::string_t m_System_fingerprint;
    bool m_System_fingerprintIsSet;

    ObjectEnum m_object;
    bool m_objectIsSet;

    boost::optional<std::shared_ptr<CreateChatCompletionStreamResponse_usage>> m_Usage;

};


}
}
}
}

#endif /* ORG_OPENAPITOOLS_CLIENT_MODEL_CreateChatCompletionStreamResponse_H_ */
