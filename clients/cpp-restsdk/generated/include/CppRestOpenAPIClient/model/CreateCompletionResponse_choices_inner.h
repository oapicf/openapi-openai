/**
 * OpenAI API
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.0.0
 * Contact: blah+oapicf@cliffano.com
 *
 * NOTE: This class is auto generated by OpenAPI-Generator 7.18.0.
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */

/*
 * CreateCompletionResponse_choices_inner.h
 *
 * 
 */

#ifndef ORG_OPENAPITOOLS_CLIENT_MODEL_CreateCompletionResponse_choices_inner_H_
#define ORG_OPENAPITOOLS_CLIENT_MODEL_CreateCompletionResponse_choices_inner_H_

#include <stdexcept>
#include <boost/optional.hpp>

#include "CppRestOpenAPIClient/ModelBase.h"

#include <cpprest/details/basic_types.h>
#include "CppRestOpenAPIClient/model/CreateCompletionResponse_choices_inner_logprobs.h"

namespace org {
namespace openapitools {
namespace client {
namespace model {

class CreateCompletionResponse_choices_inner_logprobs;


class  CreateCompletionResponse_choices_inner
    : public ModelBase
{
public:
    CreateCompletionResponse_choices_inner();
    virtual ~CreateCompletionResponse_choices_inner();

    /////////////////////////////////////////////
    /// ModelBase overrides

    void validate() override;

    web::json::value toJson() const override;
    bool fromJson(const web::json::value& json) override;

    void toMultipart(std::shared_ptr<MultipartFormData> multipart, const utility::string_t& namePrefix) const override;
    bool fromMultiPart(std::shared_ptr<MultipartFormData> multipart, const utility::string_t& namePrefix) override;


    /////////////////////////////////////////////
    /// CreateCompletionResponse_choices_inner members

    enum class Finish_reasonEnum
    {
        STOP,
        LENGTH,
        CONTENT_FILTER,
    };
    /// <summary>
    /// The reason the model stopped generating tokens. This will be &#x60;stop&#x60; if the model hit a natural stop point or a provided stop sequence, &#x60;length&#x60; if the maximum number of tokens specified in the request was reached, or &#x60;content_filter&#x60; if content was omitted due to a flag from our content filters. 
    /// </summary>

    Finish_reasonEnum toFinish_reasonEnum(const utility::string_t& value) const;
    const utility::string_t fromFinish_reasonEnum(const Finish_reasonEnum value) const;


    /// <summary>
    /// The reason the model stopped generating tokens. This will be &#x60;stop&#x60; if the model hit a natural stop point or a provided stop sequence, &#x60;length&#x60; if the maximum number of tokens specified in the request was reached, or &#x60;content_filter&#x60; if content was omitted due to a flag from our content filters. 
    /// </summary>
    Finish_reasonEnum getFinishReason() const;
    bool finishReasonIsSet() const;
    void unsetFinish_reason();
    void setFinishReason(const Finish_reasonEnum value);

    int32_t getIndex() const;
    bool indexIsSet() const;
    void unsetIndex();
    void setIndex(int32_t value);

    std::shared_ptr<CreateCompletionResponse_choices_inner_logprobs> getLogprobs() const;
    bool logprobsIsSet() const;
    void unsetLogprobs();
    void setLogprobs(const std::shared_ptr<CreateCompletionResponse_choices_inner_logprobs>& value);

    utility::string_t getText() const;
    bool textIsSet() const;
    void unsetText();
    void setText(const utility::string_t& value);


protected:
    Finish_reasonEnum m_Finish_reason;
    bool m_Finish_reasonIsSet;

    int32_t m_Index;
    bool m_IndexIsSet;

    boost::optional<std::shared_ptr<CreateCompletionResponse_choices_inner_logprobs>> m_Logprobs;

    utility::string_t m_Text;
    bool m_TextIsSet;

};


}
}
}
}

#endif /* ORG_OPENAPITOOLS_CLIENT_MODEL_CreateCompletionResponse_choices_inner_H_ */
