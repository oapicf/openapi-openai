/**
 * OpenAI API
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.3.0
 * Contact: blah+oapicf@cliffano.com
 *
 * NOTE: This class is auto generated by OpenAPI-Generator 7.18.0.
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */

/*
 * TruncationObject.h
 *
 * Controls for how a thread will be truncated prior to the run. Use this to control the intial context window of the run.
 */

#ifndef ORG_OPENAPITOOLS_CLIENT_MODEL_TruncationObject_H_
#define ORG_OPENAPITOOLS_CLIENT_MODEL_TruncationObject_H_

#include <stdexcept>
#include <boost/optional.hpp>

#include "CppRestOpenAPIClient/ModelBase.h"

#include <cpprest/details/basic_types.h>

namespace org {
namespace openapitools {
namespace client {
namespace model {



/// <summary>
/// Controls for how a thread will be truncated prior to the run. Use this to control the intial context window of the run.
/// </summary>
class  TruncationObject
    : public ModelBase
{
public:
    TruncationObject();
    virtual ~TruncationObject();

    /////////////////////////////////////////////
    /// ModelBase overrides

    void validate() override;

    web::json::value toJson() const override;
    bool fromJson(const web::json::value& json) override;

    void toMultipart(std::shared_ptr<MultipartFormData> multipart, const utility::string_t& namePrefix) const override;
    bool fromMultiPart(std::shared_ptr<MultipartFormData> multipart, const utility::string_t& namePrefix) override;


    /////////////////////////////////////////////
    /// TruncationObject members

    enum class TypeEnum
    {
        AUTO,
        LAST_MESSAGES,
    };
    /// <summary>
    /// The truncation strategy to use for the thread. The default is &#x60;auto&#x60;. If set to &#x60;last_messages&#x60;, the thread will be truncated to the n most recent messages in the thread. When set to &#x60;auto&#x60;, messages in the middle of the thread will be dropped to fit the context length of the model, &#x60;max_prompt_tokens&#x60;.
    /// </summary>

    TypeEnum toTypeEnum(const utility::string_t& value) const;
    const utility::string_t fromTypeEnum(const TypeEnum value) const;


    /// <summary>
    /// The truncation strategy to use for the thread. The default is &#x60;auto&#x60;. If set to &#x60;last_messages&#x60;, the thread will be truncated to the n most recent messages in the thread. When set to &#x60;auto&#x60;, messages in the middle of the thread will be dropped to fit the context length of the model, &#x60;max_prompt_tokens&#x60;.
    /// </summary>
    TypeEnum getType() const;
    bool typeIsSet() const;
    void unsetType();
    void setType(const TypeEnum value);

    /// <summary>
    /// The number of most recent messages from the thread when constructing the context for the run.
    /// </summary>
    int32_t getLastMessages() const;
    bool lastMessagesIsSet() const;
    void unsetLast_messages();
    void setLastMessages(int32_t value);


protected:
    TypeEnum m_Type;
    bool m_TypeIsSet;

    boost::optional<int32_t> m_Last_messages;

};


}
}
}
}

#endif /* ORG_OPENAPITOOLS_CLIENT_MODEL_TruncationObject_H_ */
