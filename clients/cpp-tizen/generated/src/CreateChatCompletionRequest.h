/*
 * CreateChatCompletionRequest.h
 *
 * 
 */

#ifndef _CreateChatCompletionRequest_H_
#define _CreateChatCompletionRequest_H_


#include <string>
#include "ChatCompletionFunctions.h"
#include "ChatCompletionRequestMessage.h"
#include "ChatCompletionStreamOptions.h"
#include "ChatCompletionTool.h"
#include "ChatCompletionToolChoiceOption.h"
#include "CreateChatCompletionRequest_audio.h"
#include "CreateChatCompletionRequest_function_call.h"
#include "CreateChatCompletionRequest_model.h"
#include "CreateChatCompletionRequest_response_format.h"
#include "CreateChatCompletionRequest_stop.h"
#include "PredictionContent.h"
#include <list>
#include <map>
#include "Object.h"

/** \defgroup Models Data Structures for API
 *  Classes containing all the Data Structures needed for calling/returned by API endpoints
 *
 */

namespace Tizen {
namespace ArtikCloud {


/*! \brief 
 *
 *  \ingroup Models
 *
 */

class CreateChatCompletionRequest : public Object {
public:
	/*! \brief Constructor.
	 */
	CreateChatCompletionRequest();
	CreateChatCompletionRequest(char* str);

	/*! \brief Destructor.
	 */
	virtual ~CreateChatCompletionRequest();

	/*! \brief Retrieve a string JSON representation of this class.
	 */
	char* toJson();

	/*! \brief Fills in members of this class from JSON string representing it.
	 */
	void fromJson(char* jsonStr);

	/*! \brief Get A list of messages comprising the conversation so far. Depending on the [model](/docs/models) you use, different message types (modalities) are supported, like [text](/docs/guides/text-generation), [images](/docs/guides/vision), and [audio](/docs/guides/audio). 
	 */
	std::list<ChatCompletionRequestMessage> getMessages();

	/*! \brief Set A list of messages comprising the conversation so far. Depending on the [model](/docs/models) you use, different message types (modalities) are supported, like [text](/docs/guides/text-generation), [images](/docs/guides/vision), and [audio](/docs/guides/audio). 
	 */
	void setMessages(std::list <ChatCompletionRequestMessage> messages);
	/*! \brief Get 
	 */
	CreateChatCompletionRequest_model getModel();

	/*! \brief Set 
	 */
	void setModel(CreateChatCompletionRequest_model  model);
	/*! \brief Get Whether or not to store the output of this chat completion request for  use in our [model distillation](/docs/guides/distillation) or [evals](/docs/guides/evals) products. 
	 */
	bool getStore();

	/*! \brief Set Whether or not to store the output of this chat completion request for  use in our [model distillation](/docs/guides/distillation) or [evals](/docs/guides/evals) products. 
	 */
	void setStore(bool  store);
	/*! \brief Get **o1 models only**   Constrains effort on reasoning for  [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response. 
	 */
	std::string getReasoningEffort();

	/*! \brief Set **o1 models only**   Constrains effort on reasoning for  [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response. 
	 */
	void setReasoningEffort(std::string  reasoning_effort);
	/*! \brief Get Developer-defined tags and values used for filtering completions in the [dashboard](https://platform.openai.com/chat-completions). 
	 */
	std::map<std::string, std::string> getMetadata();

	/*! \brief Set Developer-defined tags and values used for filtering completions in the [dashboard](https://platform.openai.com/chat-completions). 
	 */
	void setMetadata(std::map <std::string, std::string> metadata);
	/*! \brief Get Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. 
	 */
	long long getFrequencyPenalty();

	/*! \brief Set Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. 
	 */
	void setFrequencyPenalty(long long  frequency_penalty);
	/*! \brief Get Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
	 */
	std::map<std::string, std::string> getLogitBias();

	/*! \brief Set Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
	 */
	void setLogitBias(std::map <std::string, std::string> logit_bias);
	/*! \brief Get Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. 
	 */
	bool getLogprobs();

	/*! \brief Set Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. 
	 */
	void setLogprobs(bool  logprobs);
	/*! \brief Get An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used. 
	 */
	int getTopLogprobs();

	/*! \brief Set An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used. 
	 */
	void setTopLogprobs(int  top_logprobs);
	/*! \brief Get The maximum number of [tokens](/tokenizer) that can be generated in the chat completion. This value can be used to control [costs](https://openai.com/api/pricing/) for text generated via API.  This value is now deprecated in favor of `max_completion_tokens`, and is not compatible with [o1 series models](/docs/guides/reasoning). 
	 */
	int getMaxTokens();

	/*! \brief Set The maximum number of [tokens](/tokenizer) that can be generated in the chat completion. This value can be used to control [costs](https://openai.com/api/pricing/) for text generated via API.  This value is now deprecated in favor of `max_completion_tokens`, and is not compatible with [o1 series models](/docs/guides/reasoning). 
	 */
	void setMaxTokens(int  max_tokens);
	/*! \brief Get An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning). 
	 */
	int getMaxCompletionTokens();

	/*! \brief Set An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning). 
	 */
	void setMaxCompletionTokens(int  max_completion_tokens);
	/*! \brief Get How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
	 */
	int getN();

	/*! \brief Set How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
	 */
	void setN(int  n);
	/*! \brief Get Output types that you would like the model to generate for this request. Most models are capable of generating text, which is the default:  `[\"text\"]`  The `gpt-4o-audio-preview` model can also be used to [generate audio](/docs/guides/audio). To request that this model generate both text and audio responses, you can use:  `[\"text\", \"audio\"]` 
	 */
	std::list<std::string> getModalities();

	/*! \brief Set Output types that you would like the model to generate for this request. Most models are capable of generating text, which is the default:  `[\"text\"]`  The `gpt-4o-audio-preview` model can also be used to [generate audio](/docs/guides/audio). To request that this model generate both text and audio responses, you can use:  `[\"text\", \"audio\"]` 
	 */
	void setModalities(std::list <std::string> modalities);
	/*! \brief Get 
	 */
	PredictionContent getPrediction();

	/*! \brief Set 
	 */
	void setPrediction(PredictionContent  prediction);
	/*! \brief Get 
	 */
	CreateChatCompletionRequest_audio getAudio();

	/*! \brief Set 
	 */
	void setAudio(CreateChatCompletionRequest_audio  audio);
	/*! \brief Get Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. 
	 */
	long long getPresencePenalty();

	/*! \brief Set Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. 
	 */
	void setPresencePenalty(long long  presence_penalty);
	/*! \brief Get 
	 */
	CreateChatCompletionRequest_response_format getResponseFormat();

	/*! \brief Set 
	 */
	void setResponseFormat(CreateChatCompletionRequest_response_format  response_format);
	/*! \brief Get This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. 
	 */
	int getSeed();

	/*! \brief Set This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. 
	 */
	void setSeed(int  seed);
	/*! \brief Get Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:    - If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.   - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - When not set, the default behavior is 'auto'.    When this parameter is set, the response body will include the `service_tier` utilized. 
	 */
	std::string getServiceTier();

	/*! \brief Set Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:    - If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.   - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - When not set, the default behavior is 'auto'.    When this parameter is set, the response body will include the `service_tier` utilized. 
	 */
	void setServiceTier(std::string  service_tier);
	/*! \brief Get 
	 */
	CreateChatCompletionRequest_stop getStop();

	/*! \brief Set 
	 */
	void setStop(CreateChatCompletionRequest_stop  stop);
	/*! \brief Get If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
	 */
	bool getStream();

	/*! \brief Set If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
	 */
	void setStream(bool  stream);
	/*! \brief Get 
	 */
	ChatCompletionStreamOptions getStreamOptions();

	/*! \brief Set 
	 */
	void setStreamOptions(ChatCompletionStreamOptions  stream_options);
	/*! \brief Get What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. 
	 */
	long long getTemperature();

	/*! \brief Set What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. 
	 */
	void setTemperature(long long  temperature);
	/*! \brief Get An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. 
	 */
	long long getTopP();

	/*! \brief Set An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. 
	 */
	void setTopP(long long  top_p);
	/*! \brief Get A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. 
	 */
	std::list<ChatCompletionTool> getTools();

	/*! \brief Set A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. 
	 */
	void setTools(std::list <ChatCompletionTool> tools);
	/*! \brief Get 
	 */
	ChatCompletionToolChoiceOption getToolChoice();

	/*! \brief Set 
	 */
	void setToolChoice(ChatCompletionToolChoiceOption  tool_choice);
	/*! \brief Get Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.
	 */
	bool getParallelToolCalls();

	/*! \brief Set Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.
	 */
	void setParallelToolCalls(bool  parallel_tool_calls);
	/*! \brief Get A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). 
	 */
	std::string getUser();

	/*! \brief Set A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). 
	 */
	void setUser(std::string  user);
	/*! \brief Get 
	 */
	CreateChatCompletionRequest_function_call getFunctionCall();

	/*! \brief Set 
	 */
	void setFunctionCall(CreateChatCompletionRequest_function_call  function_call);
	/*! \brief Get Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for. 
	 */
	std::list<ChatCompletionFunctions> getFunctions();

	/*! \brief Set Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for. 
	 */
	void setFunctions(std::list <ChatCompletionFunctions> functions);

private:
	std::list <ChatCompletionRequestMessage>messages;
	CreateChatCompletionRequest_model model;
	bool store;
	std::string reasoning_effort;
	std::map <std::string, std::string>metadata;
	long long frequency_penalty;
	std::map <std::string, std::string>logit_bias;
	bool logprobs;
	int top_logprobs;
	int max_tokens;
	int max_completion_tokens;
	int n;
	std::list <std::string>modalities;
	PredictionContent prediction;
	CreateChatCompletionRequest_audio audio;
	long long presence_penalty;
	CreateChatCompletionRequest_response_format response_format;
	int seed;
	std::string service_tier;
	CreateChatCompletionRequest_stop stop;
	bool stream;
	ChatCompletionStreamOptions stream_options;
	long long temperature;
	long long top_p;
	std::list <ChatCompletionTool>tools;
	ChatCompletionToolChoiceOption tool_choice;
	bool parallel_tool_calls;
	std::string user;
	CreateChatCompletionRequest_function_call function_call;
	std::list <ChatCompletionFunctions>functions;
	void __init();
	void __cleanup();

};
}
}

#endif /* _CreateChatCompletionRequest_H_ */
