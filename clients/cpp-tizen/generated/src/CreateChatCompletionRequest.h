/*
 * CreateChatCompletionRequest.h
 *
 * 
 */

#ifndef _CreateChatCompletionRequest_H_
#define _CreateChatCompletionRequest_H_


#include <string>
#include "ChatCompletionFunctions.h"
#include "ChatCompletionRequestMessage.h"
#include "ChatCompletionTool.h"
#include "ChatCompletionToolChoiceOption.h"
#include "CreateChatCompletionRequest_function_call.h"
#include "CreateChatCompletionRequest_model.h"
#include "CreateChatCompletionRequest_response_format.h"
#include "CreateChatCompletionRequest_stop.h"
#include <list>
#include <map>
#include "Object.h"

/** \defgroup Models Data Structures for API
 *  Classes containing all the Data Structures needed for calling/returned by API endpoints
 *
 */

namespace Tizen {
namespace ArtikCloud {


/*! \brief 
 *
 *  \ingroup Models
 *
 */

class CreateChatCompletionRequest : public Object {
public:
	/*! \brief Constructor.
	 */
	CreateChatCompletionRequest();
	CreateChatCompletionRequest(char* str);

	/*! \brief Destructor.
	 */
	virtual ~CreateChatCompletionRequest();

	/*! \brief Retrieve a string JSON representation of this class.
	 */
	char* toJson();

	/*! \brief Fills in members of this class from JSON string representing it.
	 */
	void fromJson(char* jsonStr);

	/*! \brief Get A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
	 */
	std::list<ChatCompletionRequestMessage> getMessages();

	/*! \brief Set A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
	 */
	void setMessages(std::list <ChatCompletionRequestMessage> messages);
	/*! \brief Get 
	 */
	CreateChatCompletionRequest_model getModel();

	/*! \brief Set 
	 */
	void setModel(CreateChatCompletionRequest_model  model);
	/*! \brief Get Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
	 */
	long long getFrequencyPenalty();

	/*! \brief Set Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
	 */
	void setFrequencyPenalty(long long  frequency_penalty);
	/*! \brief Get Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
	 */
	std::map<std::string, std::string> getLogitBias();

	/*! \brief Set Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
	 */
	void setLogitBias(std::map <std::string, std::string> logit_bias);
	/*! \brief Get Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.
	 */
	bool getLogprobs();

	/*! \brief Set Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.
	 */
	void setLogprobs(bool  logprobs);
	/*! \brief Get An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
	 */
	int getTopLogprobs();

	/*! \brief Set An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
	 */
	void setTopLogprobs(int  top_logprobs);
	/*! \brief Get The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.  The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. 
	 */
	int getMaxTokens();

	/*! \brief Set The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.  The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. 
	 */
	void setMaxTokens(int  max_tokens);
	/*! \brief Get How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
	 */
	int getN();

	/*! \brief Set How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
	 */
	void setN(int  n);
	/*! \brief Get Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
	 */
	long long getPresencePenalty();

	/*! \brief Set Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
	 */
	void setPresencePenalty(long long  presence_penalty);
	/*! \brief Get 
	 */
	CreateChatCompletionRequest_response_format getResponseFormat();

	/*! \brief Set 
	 */
	void setResponseFormat(CreateChatCompletionRequest_response_format  response_format);
	/*! \brief Get This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. 
	 */
	int getSeed();

	/*! \brief Set This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. 
	 */
	void setSeed(int  seed);
	/*! \brief Get 
	 */
	CreateChatCompletionRequest_stop getStop();

	/*! \brief Set 
	 */
	void setStop(CreateChatCompletionRequest_stop  stop);
	/*! \brief Get If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
	 */
	bool getStream();

	/*! \brief Set If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
	 */
	void setStream(bool  stream);
	/*! \brief Get What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both. 
	 */
	long long getTemperature();

	/*! \brief Set What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both. 
	 */
	void setTemperature(long long  temperature);
	/*! \brief Get An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. 
	 */
	long long getTopP();

	/*! \brief Set An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. 
	 */
	void setTopP(long long  top_p);
	/*! \brief Get A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. 
	 */
	std::list<ChatCompletionTool> getTools();

	/*! \brief Set A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. 
	 */
	void setTools(std::list <ChatCompletionTool> tools);
	/*! \brief Get 
	 */
	ChatCompletionToolChoiceOption getToolChoice();

	/*! \brief Set 
	 */
	void setToolChoice(ChatCompletionToolChoiceOption  tool_choice);
	/*! \brief Get A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). 
	 */
	std::string getUser();

	/*! \brief Set A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). 
	 */
	void setUser(std::string  user);
	/*! \brief Get 
	 */
	CreateChatCompletionRequest_function_call getFunctionCall();

	/*! \brief Set 
	 */
	void setFunctionCall(CreateChatCompletionRequest_function_call  function_call);
	/*! \brief Get Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for. 
	 */
	std::list<ChatCompletionFunctions> getFunctions();

	/*! \brief Set Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for. 
	 */
	void setFunctions(std::list <ChatCompletionFunctions> functions);

private:
	std::list <ChatCompletionRequestMessage>messages;
	CreateChatCompletionRequest_model model;
	long long frequency_penalty;
	std::map <std::string, std::string>logit_bias;
	bool logprobs;
	int top_logprobs;
	int max_tokens;
	int n;
	long long presence_penalty;
	CreateChatCompletionRequest_response_format response_format;
	int seed;
	CreateChatCompletionRequest_stop stop;
	bool stream;
	long long temperature;
	long long top_p;
	std::list <ChatCompletionTool>tools;
	ChatCompletionToolChoiceOption tool_choice;
	std::string user;
	CreateChatCompletionRequest_function_call function_call;
	std::list <ChatCompletionFunctions>functions;
	void __init();
	void __cleanup();

};
}
}

#endif /* _CreateChatCompletionRequest_H_ */
