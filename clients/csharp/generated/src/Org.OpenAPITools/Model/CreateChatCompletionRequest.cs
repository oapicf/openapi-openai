// <auto-generated>
/*
 * OpenAI API
 *
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.0.0
 * Contact: blah+oapicf@cliffano.com
 * Generated by: https://github.com/openapitools/openapi-generator.git
 */

#nullable enable

using System;
using System.Collections;
using System.Collections.Generic;
using System.Collections.ObjectModel;
using System.Linq;
using System.IO;
using System.Text;
using System.Text.RegularExpressions;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.ComponentModel.DataAnnotations;
using Org.OpenAPITools.Client;

namespace Org.OpenAPITools.Model
{
    /// <summary>
    /// CreateChatCompletionRequest
    /// </summary>
    public partial class CreateChatCompletionRequest : IValidatableObject
    {
        /// <summary>
        /// Initializes a new instance of the <see cref="CreateChatCompletionRequest" /> class.
        /// </summary>
        /// <param name="messages">A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).</param>
        /// <param name="model">model</param>
        /// <param name="frequencyPenalty">Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)  (default to 0M)</param>
        /// <param name="logitBias">Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. </param>
        /// <param name="logprobs">Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the &#x60;content&#x60; of &#x60;message&#x60;. (default to false)</param>
        /// <param name="topLogprobs">An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. &#x60;logprobs&#x60; must be set to &#x60;true&#x60; if this parameter is used.</param>
        /// <param name="maxTokens">The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.  The total length of input tokens and generated tokens is limited by the model&#39;s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. </param>
        /// <param name="n">How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep &#x60;n&#x60; as &#x60;1&#x60; to minimize costs. (default to 1)</param>
        /// <param name="presencePenalty">Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)  (default to 0M)</param>
        /// <param name="responseFormat">responseFormat</param>
        /// <param name="seed">This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same &#x60;seed&#x60; and parameters should return the same result. Determinism is not guaranteed, and you should refer to the &#x60;system_fingerprint&#x60; response parameter to monitor changes in the backend. </param>
        /// <param name="stop">stop</param>
        /// <param name="stream">If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).  (default to false)</param>
        /// <param name="temperature">What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or &#x60;top_p&#x60; but not both.  (default to 1M)</param>
        /// <param name="topP">An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or &#x60;temperature&#x60; but not both.  (default to 1M)</param>
        /// <param name="tools">A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. </param>
        /// <param name="toolChoice">toolChoice</param>
        /// <param name="user">A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). </param>
        /// <param name="functionCall">functionCall</param>
        /// <param name="functions">Deprecated in favor of &#x60;tools&#x60;.  A list of functions the model may generate JSON inputs for. </param>
        [JsonConstructor]
        public CreateChatCompletionRequest(List<ChatCompletionRequestMessage> messages, CreateChatCompletionRequestModel model, Option<decimal?> frequencyPenalty = default, Option<Dictionary<string, int>?> logitBias = default, Option<bool?> logprobs = default, Option<int?> topLogprobs = default, Option<int?> maxTokens = default, Option<int?> n = default, Option<decimal?> presencePenalty = default, Option<CreateChatCompletionRequestResponseFormat?> responseFormat = default, Option<long?> seed = default, Option<CreateChatCompletionRequestStop?> stop = default, Option<bool?> stream = default, Option<decimal?> temperature = default, Option<decimal?> topP = default, Option<List<ChatCompletionTool>?> tools = default, Option<ChatCompletionToolChoiceOption?> toolChoice = default, Option<string?> user = default, Option<CreateChatCompletionRequestFunctionCall?> functionCall = default, Option<List<ChatCompletionFunctions>?> functions = default)
        {
            Messages = messages;
            Model = model;
            FrequencyPenaltyOption = frequencyPenalty;
            LogitBiasOption = logitBias;
            LogprobsOption = logprobs;
            TopLogprobsOption = topLogprobs;
            MaxTokensOption = maxTokens;
            NOption = n;
            PresencePenaltyOption = presencePenalty;
            ResponseFormatOption = responseFormat;
            SeedOption = seed;
            StopOption = stop;
            StreamOption = stream;
            TemperatureOption = temperature;
            TopPOption = topP;
            ToolsOption = tools;
            ToolChoiceOption = toolChoice;
            UserOption = user;
            FunctionCallOption = functionCall;
            FunctionsOption = functions;
            OnCreated();
        }

        partial void OnCreated();

        /// <summary>
        /// A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
        /// </summary>
        /// <value>A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).</value>
        [JsonPropertyName("messages")]
        public List<ChatCompletionRequestMessage> Messages { get; set; }

        /// <summary>
        /// Gets or Sets Model
        /// </summary>
        [JsonPropertyName("model")]
        public CreateChatCompletionRequestModel Model { get; set; }

        /// <summary>
        /// Used to track the state of FrequencyPenalty
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<decimal?> FrequencyPenaltyOption { get; private set; }

        /// <summary>
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
        /// </summary>
        /// <value>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) </value>
        [JsonPropertyName("frequency_penalty")]
        public decimal? FrequencyPenalty { get { return this.FrequencyPenaltyOption; } set { this.FrequencyPenaltyOption = new(value); } }

        /// <summary>
        /// Used to track the state of LogitBias
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<Dictionary<string, int>?> LogitBiasOption { get; private set; }

        /// <summary>
        /// Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
        /// </summary>
        /// <value>Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. </value>
        [JsonPropertyName("logit_bias")]
        public Dictionary<string, int>? LogitBias { get { return this.LogitBiasOption; } set { this.LogitBiasOption = new(value); } }

        /// <summary>
        /// Used to track the state of Logprobs
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<bool?> LogprobsOption { get; private set; }

        /// <summary>
        /// Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the &#x60;content&#x60; of &#x60;message&#x60;.
        /// </summary>
        /// <value>Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the &#x60;content&#x60; of &#x60;message&#x60;.</value>
        [JsonPropertyName("logprobs")]
        public bool? Logprobs { get { return this.LogprobsOption; } set { this.LogprobsOption = new(value); } }

        /// <summary>
        /// Used to track the state of TopLogprobs
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<int?> TopLogprobsOption { get; private set; }

        /// <summary>
        /// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. &#x60;logprobs&#x60; must be set to &#x60;true&#x60; if this parameter is used.
        /// </summary>
        /// <value>An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. &#x60;logprobs&#x60; must be set to &#x60;true&#x60; if this parameter is used.</value>
        [JsonPropertyName("top_logprobs")]
        public int? TopLogprobs { get { return this.TopLogprobsOption; } set { this.TopLogprobsOption = new(value); } }

        /// <summary>
        /// Used to track the state of MaxTokens
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<int?> MaxTokensOption { get; private set; }

        /// <summary>
        /// The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.  The total length of input tokens and generated tokens is limited by the model&#39;s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. 
        /// </summary>
        /// <value>The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.  The total length of input tokens and generated tokens is limited by the model&#39;s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. </value>
        [JsonPropertyName("max_tokens")]
        public int? MaxTokens { get { return this.MaxTokensOption; } set { this.MaxTokensOption = new(value); } }

        /// <summary>
        /// Used to track the state of N
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<int?> NOption { get; private set; }

        /// <summary>
        /// How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep &#x60;n&#x60; as &#x60;1&#x60; to minimize costs.
        /// </summary>
        /// <value>How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep &#x60;n&#x60; as &#x60;1&#x60; to minimize costs.</value>
        /* <example>1</example> */
        [JsonPropertyName("n")]
        public int? N { get { return this.NOption; } set { this.NOption = new(value); } }

        /// <summary>
        /// Used to track the state of PresencePenalty
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<decimal?> PresencePenaltyOption { get; private set; }

        /// <summary>
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
        /// </summary>
        /// <value>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) </value>
        [JsonPropertyName("presence_penalty")]
        public decimal? PresencePenalty { get { return this.PresencePenaltyOption; } set { this.PresencePenaltyOption = new(value); } }

        /// <summary>
        /// Used to track the state of ResponseFormat
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<CreateChatCompletionRequestResponseFormat?> ResponseFormatOption { get; private set; }

        /// <summary>
        /// Gets or Sets ResponseFormat
        /// </summary>
        [JsonPropertyName("response_format")]
        public CreateChatCompletionRequestResponseFormat? ResponseFormat { get { return this.ResponseFormatOption; } set { this.ResponseFormatOption = new(value); } }

        /// <summary>
        /// Used to track the state of Seed
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<long?> SeedOption { get; private set; }

        /// <summary>
        /// This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same &#x60;seed&#x60; and parameters should return the same result. Determinism is not guaranteed, and you should refer to the &#x60;system_fingerprint&#x60; response parameter to monitor changes in the backend. 
        /// </summary>
        /// <value>This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same &#x60;seed&#x60; and parameters should return the same result. Determinism is not guaranteed, and you should refer to the &#x60;system_fingerprint&#x60; response parameter to monitor changes in the backend. </value>
        [JsonPropertyName("seed")]
        public long? Seed { get { return this.SeedOption; } set { this.SeedOption = new(value); } }

        /// <summary>
        /// Used to track the state of Stop
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<CreateChatCompletionRequestStop?> StopOption { get; private set; }

        /// <summary>
        /// Gets or Sets Stop
        /// </summary>
        [JsonPropertyName("stop")]
        public CreateChatCompletionRequestStop? Stop { get { return this.StopOption; } set { this.StopOption = new(value); } }

        /// <summary>
        /// Used to track the state of Stream
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<bool?> StreamOption { get; private set; }

        /// <summary>
        /// If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
        /// </summary>
        /// <value>If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). </value>
        [JsonPropertyName("stream")]
        public bool? Stream { get { return this.StreamOption; } set { this.StreamOption = new(value); } }

        /// <summary>
        /// Used to track the state of Temperature
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<decimal?> TemperatureOption { get; private set; }

        /// <summary>
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or &#x60;top_p&#x60; but not both. 
        /// </summary>
        /// <value>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or &#x60;top_p&#x60; but not both. </value>
        /* <example>1</example> */
        [JsonPropertyName("temperature")]
        public decimal? Temperature { get { return this.TemperatureOption; } set { this.TemperatureOption = new(value); } }

        /// <summary>
        /// Used to track the state of TopP
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<decimal?> TopPOption { get; private set; }

        /// <summary>
        /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or &#x60;temperature&#x60; but not both. 
        /// </summary>
        /// <value>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or &#x60;temperature&#x60; but not both. </value>
        /* <example>1</example> */
        [JsonPropertyName("top_p")]
        public decimal? TopP { get { return this.TopPOption; } set { this.TopPOption = new(value); } }

        /// <summary>
        /// Used to track the state of Tools
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<List<ChatCompletionTool>?> ToolsOption { get; private set; }

        /// <summary>
        /// A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. 
        /// </summary>
        /// <value>A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. </value>
        [JsonPropertyName("tools")]
        public List<ChatCompletionTool>? Tools { get { return this.ToolsOption; } set { this.ToolsOption = new(value); } }

        /// <summary>
        /// Used to track the state of ToolChoice
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<ChatCompletionToolChoiceOption?> ToolChoiceOption { get; private set; }

        /// <summary>
        /// Gets or Sets ToolChoice
        /// </summary>
        [JsonPropertyName("tool_choice")]
        public ChatCompletionToolChoiceOption? ToolChoice { get { return this.ToolChoiceOption; } set { this.ToolChoiceOption = new(value); } }

        /// <summary>
        /// Used to track the state of User
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<string?> UserOption { get; private set; }

        /// <summary>
        /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). 
        /// </summary>
        /// <value>A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). </value>
        /* <example>user-1234</example> */
        [JsonPropertyName("user")]
        public string? User { get { return this.UserOption; } set { this.UserOption = new(value); } }

        /// <summary>
        /// Used to track the state of FunctionCall
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<CreateChatCompletionRequestFunctionCall?> FunctionCallOption { get; private set; }

        /// <summary>
        /// Gets or Sets FunctionCall
        /// </summary>
        [JsonPropertyName("function_call")]
        [Obsolete]
        public CreateChatCompletionRequestFunctionCall? FunctionCall { get { return this.FunctionCallOption; } set { this.FunctionCallOption = new(value); } }

        /// <summary>
        /// Used to track the state of Functions
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<List<ChatCompletionFunctions>?> FunctionsOption { get; private set; }

        /// <summary>
        /// Deprecated in favor of &#x60;tools&#x60;.  A list of functions the model may generate JSON inputs for. 
        /// </summary>
        /// <value>Deprecated in favor of &#x60;tools&#x60;.  A list of functions the model may generate JSON inputs for. </value>
        [JsonPropertyName("functions")]
        [Obsolete]
        public List<ChatCompletionFunctions>? Functions { get { return this.FunctionsOption; } set { this.FunctionsOption = new(value); } }

        /// <summary>
        /// Returns the string presentation of the object
        /// </summary>
        /// <returns>String presentation of the object</returns>
        public override string ToString()
        {
            StringBuilder sb = new StringBuilder();
            sb.Append("class CreateChatCompletionRequest {\n");
            sb.Append("  Messages: ").Append(Messages).Append("\n");
            sb.Append("  Model: ").Append(Model).Append("\n");
            sb.Append("  FrequencyPenalty: ").Append(FrequencyPenalty).Append("\n");
            sb.Append("  LogitBias: ").Append(LogitBias).Append("\n");
            sb.Append("  Logprobs: ").Append(Logprobs).Append("\n");
            sb.Append("  TopLogprobs: ").Append(TopLogprobs).Append("\n");
            sb.Append("  MaxTokens: ").Append(MaxTokens).Append("\n");
            sb.Append("  N: ").Append(N).Append("\n");
            sb.Append("  PresencePenalty: ").Append(PresencePenalty).Append("\n");
            sb.Append("  ResponseFormat: ").Append(ResponseFormat).Append("\n");
            sb.Append("  Seed: ").Append(Seed).Append("\n");
            sb.Append("  Stop: ").Append(Stop).Append("\n");
            sb.Append("  Stream: ").Append(Stream).Append("\n");
            sb.Append("  Temperature: ").Append(Temperature).Append("\n");
            sb.Append("  TopP: ").Append(TopP).Append("\n");
            sb.Append("  Tools: ").Append(Tools).Append("\n");
            sb.Append("  ToolChoice: ").Append(ToolChoice).Append("\n");
            sb.Append("  User: ").Append(User).Append("\n");
            sb.Append("  FunctionCall: ").Append(FunctionCall).Append("\n");
            sb.Append("  Functions: ").Append(Functions).Append("\n");
            sb.Append("}\n");
            return sb.ToString();
        }

        /// <summary>
        /// To validate all properties of the instance
        /// </summary>
        /// <param name="validationContext">Validation context</param>
        /// <returns>Validation Result</returns>
        IEnumerable<ValidationResult> IValidatableObject.Validate(ValidationContext validationContext)
        {
            // FrequencyPenalty (decimal) maximum
            if (this.FrequencyPenaltyOption.IsSet && this.FrequencyPenaltyOption.Value > (decimal)2)
            {
                yield return new ValidationResult("Invalid value for FrequencyPenalty, must be a value less than or equal to 2.", new [] { "FrequencyPenalty" });
            }

            // FrequencyPenalty (decimal) minimum
            if (this.FrequencyPenaltyOption.IsSet && this.FrequencyPenaltyOption.Value < (decimal)-2)
            {
                yield return new ValidationResult("Invalid value for FrequencyPenalty, must be a value greater than or equal to -2.", new [] { "FrequencyPenalty" });
            }

            // TopLogprobs (int) maximum
            if (this.TopLogprobsOption.IsSet && this.TopLogprobsOption.Value > (int)20)
            {
                yield return new ValidationResult("Invalid value for TopLogprobs, must be a value less than or equal to 20.", new [] { "TopLogprobs" });
            }

            // TopLogprobs (int) minimum
            if (this.TopLogprobsOption.IsSet && this.TopLogprobsOption.Value < (int)0)
            {
                yield return new ValidationResult("Invalid value for TopLogprobs, must be a value greater than or equal to 0.", new [] { "TopLogprobs" });
            }

            // N (int) maximum
            if (this.NOption.IsSet && this.NOption.Value > (int)128)
            {
                yield return new ValidationResult("Invalid value for N, must be a value less than or equal to 128.", new [] { "N" });
            }

            // N (int) minimum
            if (this.NOption.IsSet && this.NOption.Value < (int)1)
            {
                yield return new ValidationResult("Invalid value for N, must be a value greater than or equal to 1.", new [] { "N" });
            }

            // PresencePenalty (decimal) maximum
            if (this.PresencePenaltyOption.IsSet && this.PresencePenaltyOption.Value > (decimal)2)
            {
                yield return new ValidationResult("Invalid value for PresencePenalty, must be a value less than or equal to 2.", new [] { "PresencePenalty" });
            }

            // PresencePenalty (decimal) minimum
            if (this.PresencePenaltyOption.IsSet && this.PresencePenaltyOption.Value < (decimal)-2)
            {
                yield return new ValidationResult("Invalid value for PresencePenalty, must be a value greater than or equal to -2.", new [] { "PresencePenalty" });
            }

            // Seed (long) maximum
            if (this.SeedOption.IsSet && this.SeedOption.Value > (long)9223372036854775807)
            {
                yield return new ValidationResult("Invalid value for Seed, must be a value less than or equal to 9223372036854775807.", new [] { "Seed" });
            }

            // Seed (long) minimum
            if (this.SeedOption.IsSet && this.SeedOption.Value < (long)-9223372036854775808)
            {
                yield return new ValidationResult("Invalid value for Seed, must be a value greater than or equal to -9223372036854775808.", new [] { "Seed" });
            }

            // Temperature (decimal) maximum
            if (this.TemperatureOption.IsSet && this.TemperatureOption.Value > (decimal)2)
            {
                yield return new ValidationResult("Invalid value for Temperature, must be a value less than or equal to 2.", new [] { "Temperature" });
            }

            // Temperature (decimal) minimum
            if (this.TemperatureOption.IsSet && this.TemperatureOption.Value < (decimal)0)
            {
                yield return new ValidationResult("Invalid value for Temperature, must be a value greater than or equal to 0.", new [] { "Temperature" });
            }

            // TopP (decimal) maximum
            if (this.TopPOption.IsSet && this.TopPOption.Value > (decimal)1)
            {
                yield return new ValidationResult("Invalid value for TopP, must be a value less than or equal to 1.", new [] { "TopP" });
            }

            // TopP (decimal) minimum
            if (this.TopPOption.IsSet && this.TopPOption.Value < (decimal)0)
            {
                yield return new ValidationResult("Invalid value for TopP, must be a value greater than or equal to 0.", new [] { "TopP" });
            }

            yield break;
        }
    }

    /// <summary>
    /// A Json converter for type <see cref="CreateChatCompletionRequest" />
    /// </summary>
    public class CreateChatCompletionRequestJsonConverter : JsonConverter<CreateChatCompletionRequest>
    {
        /// <summary>
        /// Deserializes json to <see cref="CreateChatCompletionRequest" />
        /// </summary>
        /// <param name="utf8JsonReader"></param>
        /// <param name="typeToConvert"></param>
        /// <param name="jsonSerializerOptions"></param>
        /// <returns></returns>
        /// <exception cref="JsonException"></exception>
        public override CreateChatCompletionRequest Read(ref Utf8JsonReader utf8JsonReader, Type typeToConvert, JsonSerializerOptions jsonSerializerOptions)
        {
            int currentDepth = utf8JsonReader.CurrentDepth;

            if (utf8JsonReader.TokenType != JsonTokenType.StartObject && utf8JsonReader.TokenType != JsonTokenType.StartArray)
                throw new JsonException();

            JsonTokenType startingTokenType = utf8JsonReader.TokenType;

            Option<List<ChatCompletionRequestMessage>?> messages = default;
            Option<CreateChatCompletionRequestModel?> model = default;
            Option<decimal?> frequencyPenalty = default;
            Option<Dictionary<string, int>?> logitBias = default;
            Option<bool?> logprobs = default;
            Option<int?> topLogprobs = default;
            Option<int?> maxTokens = default;
            Option<int?> n = default;
            Option<decimal?> presencePenalty = default;
            Option<CreateChatCompletionRequestResponseFormat?> responseFormat = default;
            Option<long?> seed = default;
            Option<CreateChatCompletionRequestStop?> stop = default;
            Option<bool?> stream = default;
            Option<decimal?> temperature = default;
            Option<decimal?> topP = default;
            Option<List<ChatCompletionTool>?> tools = default;
            Option<ChatCompletionToolChoiceOption?> toolChoice = default;
            Option<string?> user = default;
            Option<CreateChatCompletionRequestFunctionCall?> functionCall = default;
            Option<List<ChatCompletionFunctions>?> functions = default;

            while (utf8JsonReader.Read())
            {
                if (startingTokenType == JsonTokenType.StartObject && utf8JsonReader.TokenType == JsonTokenType.EndObject && currentDepth == utf8JsonReader.CurrentDepth)
                    break;

                if (startingTokenType == JsonTokenType.StartArray && utf8JsonReader.TokenType == JsonTokenType.EndArray && currentDepth == utf8JsonReader.CurrentDepth)
                    break;

                if (utf8JsonReader.TokenType == JsonTokenType.PropertyName && currentDepth == utf8JsonReader.CurrentDepth - 1)
                {
                    string? localVarJsonPropertyName = utf8JsonReader.GetString();
                    utf8JsonReader.Read();

                    switch (localVarJsonPropertyName)
                    {
                        case "messages":
                            messages = new Option<List<ChatCompletionRequestMessage>?>(JsonSerializer.Deserialize<List<ChatCompletionRequestMessage>>(ref utf8JsonReader, jsonSerializerOptions)!);
                            break;
                        case "model":
                            model = new Option<CreateChatCompletionRequestModel?>(JsonSerializer.Deserialize<CreateChatCompletionRequestModel>(ref utf8JsonReader, jsonSerializerOptions)!);
                            break;
                        case "frequency_penalty":
                            frequencyPenalty = new Option<decimal?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (decimal?)null : utf8JsonReader.GetDecimal());
                            break;
                        case "logit_bias":
                            logitBias = new Option<Dictionary<string, int>?>(JsonSerializer.Deserialize<Dictionary<string, int>>(ref utf8JsonReader, jsonSerializerOptions));
                            break;
                        case "logprobs":
                            logprobs = new Option<bool?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (bool?)null : utf8JsonReader.GetBoolean());
                            break;
                        case "top_logprobs":
                            topLogprobs = new Option<int?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (int?)null : utf8JsonReader.GetInt32());
                            break;
                        case "max_tokens":
                            maxTokens = new Option<int?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (int?)null : utf8JsonReader.GetInt32());
                            break;
                        case "n":
                            n = new Option<int?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (int?)null : utf8JsonReader.GetInt32());
                            break;
                        case "presence_penalty":
                            presencePenalty = new Option<decimal?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (decimal?)null : utf8JsonReader.GetDecimal());
                            break;
                        case "response_format":
                            responseFormat = new Option<CreateChatCompletionRequestResponseFormat?>(JsonSerializer.Deserialize<CreateChatCompletionRequestResponseFormat>(ref utf8JsonReader, jsonSerializerOptions)!);
                            break;
                        case "seed":
                            seed = new Option<long?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (int?)null : utf8JsonReader.GetInt32());
                            break;
                        case "stop":
                            stop = new Option<CreateChatCompletionRequestStop?>(JsonSerializer.Deserialize<CreateChatCompletionRequestStop>(ref utf8JsonReader, jsonSerializerOptions)!);
                            break;
                        case "stream":
                            stream = new Option<bool?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (bool?)null : utf8JsonReader.GetBoolean());
                            break;
                        case "temperature":
                            temperature = new Option<decimal?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (decimal?)null : utf8JsonReader.GetDecimal());
                            break;
                        case "top_p":
                            topP = new Option<decimal?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (decimal?)null : utf8JsonReader.GetDecimal());
                            break;
                        case "tools":
                            tools = new Option<List<ChatCompletionTool>?>(JsonSerializer.Deserialize<List<ChatCompletionTool>>(ref utf8JsonReader, jsonSerializerOptions)!);
                            break;
                        case "tool_choice":
                            toolChoice = new Option<ChatCompletionToolChoiceOption?>(JsonSerializer.Deserialize<ChatCompletionToolChoiceOption>(ref utf8JsonReader, jsonSerializerOptions)!);
                            break;
                        case "user":
                            user = new Option<string?>(utf8JsonReader.GetString()!);
                            break;
                        case "function_call":
                            functionCall = new Option<CreateChatCompletionRequestFunctionCall?>(JsonSerializer.Deserialize<CreateChatCompletionRequestFunctionCall>(ref utf8JsonReader, jsonSerializerOptions)!);
                            break;
                        case "functions":
                            functions = new Option<List<ChatCompletionFunctions>?>(JsonSerializer.Deserialize<List<ChatCompletionFunctions>>(ref utf8JsonReader, jsonSerializerOptions)!);
                            break;
                        default:
                            break;
                    }
                }
            }

            if (!messages.IsSet)
                throw new ArgumentException("Property is required for class CreateChatCompletionRequest.", nameof(messages));

            if (!model.IsSet)
                throw new ArgumentException("Property is required for class CreateChatCompletionRequest.", nameof(model));

            if (messages.IsSet && messages.Value == null)
                throw new ArgumentNullException(nameof(messages), "Property is not nullable for class CreateChatCompletionRequest.");

            if (model.IsSet && model.Value == null)
                throw new ArgumentNullException(nameof(model), "Property is not nullable for class CreateChatCompletionRequest.");

            if (responseFormat.IsSet && responseFormat.Value == null)
                throw new ArgumentNullException(nameof(responseFormat), "Property is not nullable for class CreateChatCompletionRequest.");

            if (stop.IsSet && stop.Value == null)
                throw new ArgumentNullException(nameof(stop), "Property is not nullable for class CreateChatCompletionRequest.");

            if (tools.IsSet && tools.Value == null)
                throw new ArgumentNullException(nameof(tools), "Property is not nullable for class CreateChatCompletionRequest.");

            if (toolChoice.IsSet && toolChoice.Value == null)
                throw new ArgumentNullException(nameof(toolChoice), "Property is not nullable for class CreateChatCompletionRequest.");

            if (user.IsSet && user.Value == null)
                throw new ArgumentNullException(nameof(user), "Property is not nullable for class CreateChatCompletionRequest.");

            if (functionCall.IsSet && functionCall.Value == null)
                throw new ArgumentNullException(nameof(functionCall), "Property is not nullable for class CreateChatCompletionRequest.");

            if (functions.IsSet && functions.Value == null)
                throw new ArgumentNullException(nameof(functions), "Property is not nullable for class CreateChatCompletionRequest.");

            return new CreateChatCompletionRequest(messages.Value!, model.Value!, frequencyPenalty, logitBias, logprobs, topLogprobs, maxTokens, n, presencePenalty, responseFormat, seed, stop, stream, temperature, topP, tools, toolChoice, user, functionCall, functions);
        }

        /// <summary>
        /// Serializes a <see cref="CreateChatCompletionRequest" />
        /// </summary>
        /// <param name="writer"></param>
        /// <param name="createChatCompletionRequest"></param>
        /// <param name="jsonSerializerOptions"></param>
        /// <exception cref="NotImplementedException"></exception>
        public override void Write(Utf8JsonWriter writer, CreateChatCompletionRequest createChatCompletionRequest, JsonSerializerOptions jsonSerializerOptions)
        {
            writer.WriteStartObject();

            WriteProperties(writer, createChatCompletionRequest, jsonSerializerOptions);
            writer.WriteEndObject();
        }

        /// <summary>
        /// Serializes the properties of <see cref="CreateChatCompletionRequest" />
        /// </summary>
        /// <param name="writer"></param>
        /// <param name="createChatCompletionRequest"></param>
        /// <param name="jsonSerializerOptions"></param>
        /// <exception cref="NotImplementedException"></exception>
        public void WriteProperties(Utf8JsonWriter writer, CreateChatCompletionRequest createChatCompletionRequest, JsonSerializerOptions jsonSerializerOptions)
        {
            if (createChatCompletionRequest.Messages == null)
                throw new ArgumentNullException(nameof(createChatCompletionRequest.Messages), "Property is required for class CreateChatCompletionRequest.");

            if (createChatCompletionRequest.Model == null)
                throw new ArgumentNullException(nameof(createChatCompletionRequest.Model), "Property is required for class CreateChatCompletionRequest.");

            if (createChatCompletionRequest.ResponseFormatOption.IsSet && createChatCompletionRequest.ResponseFormat == null)
                throw new ArgumentNullException(nameof(createChatCompletionRequest.ResponseFormat), "Property is required for class CreateChatCompletionRequest.");

            if (createChatCompletionRequest.StopOption.IsSet && createChatCompletionRequest.Stop == null)
                throw new ArgumentNullException(nameof(createChatCompletionRequest.Stop), "Property is required for class CreateChatCompletionRequest.");

            if (createChatCompletionRequest.ToolsOption.IsSet && createChatCompletionRequest.Tools == null)
                throw new ArgumentNullException(nameof(createChatCompletionRequest.Tools), "Property is required for class CreateChatCompletionRequest.");

            if (createChatCompletionRequest.ToolChoiceOption.IsSet && createChatCompletionRequest.ToolChoice == null)
                throw new ArgumentNullException(nameof(createChatCompletionRequest.ToolChoice), "Property is required for class CreateChatCompletionRequest.");

            if (createChatCompletionRequest.UserOption.IsSet && createChatCompletionRequest.User == null)
                throw new ArgumentNullException(nameof(createChatCompletionRequest.User), "Property is required for class CreateChatCompletionRequest.");

            if (createChatCompletionRequest.FunctionCallOption.IsSet && createChatCompletionRequest.FunctionCall == null)
                throw new ArgumentNullException(nameof(createChatCompletionRequest.FunctionCall), "Property is required for class CreateChatCompletionRequest.");

            if (createChatCompletionRequest.FunctionsOption.IsSet && createChatCompletionRequest.Functions == null)
                throw new ArgumentNullException(nameof(createChatCompletionRequest.Functions), "Property is required for class CreateChatCompletionRequest.");

            writer.WritePropertyName("messages");
            JsonSerializer.Serialize(writer, createChatCompletionRequest.Messages, jsonSerializerOptions);
            writer.WritePropertyName("model");
            JsonSerializer.Serialize(writer, createChatCompletionRequest.Model, jsonSerializerOptions);
            if (createChatCompletionRequest.FrequencyPenaltyOption.IsSet)
                if (createChatCompletionRequest.FrequencyPenaltyOption.Value != null)
                    writer.WriteNumber("frequency_penalty", createChatCompletionRequest.FrequencyPenaltyOption.Value!.Value);
                else
                    writer.WriteNull("frequency_penalty");

            if (createChatCompletionRequest.LogitBiasOption.IsSet)
                if (createChatCompletionRequest.LogitBiasOption.Value != null)
                {
                    writer.WritePropertyName("logit_bias");
                    JsonSerializer.Serialize(writer, createChatCompletionRequest.LogitBias, jsonSerializerOptions);
                }
                else
                    writer.WriteNull("logit_bias");
            if (createChatCompletionRequest.LogprobsOption.IsSet)
                if (createChatCompletionRequest.LogprobsOption.Value != null)
                    writer.WriteBoolean("logprobs", createChatCompletionRequest.LogprobsOption.Value!.Value);
                else
                    writer.WriteNull("logprobs");

            if (createChatCompletionRequest.TopLogprobsOption.IsSet)
                if (createChatCompletionRequest.TopLogprobsOption.Value != null)
                    writer.WriteNumber("top_logprobs", createChatCompletionRequest.TopLogprobsOption.Value!.Value);
                else
                    writer.WriteNull("top_logprobs");

            if (createChatCompletionRequest.MaxTokensOption.IsSet)
                if (createChatCompletionRequest.MaxTokensOption.Value != null)
                    writer.WriteNumber("max_tokens", createChatCompletionRequest.MaxTokensOption.Value!.Value);
                else
                    writer.WriteNull("max_tokens");

            if (createChatCompletionRequest.NOption.IsSet)
                if (createChatCompletionRequest.NOption.Value != null)
                    writer.WriteNumber("n", createChatCompletionRequest.NOption.Value!.Value);
                else
                    writer.WriteNull("n");

            if (createChatCompletionRequest.PresencePenaltyOption.IsSet)
                if (createChatCompletionRequest.PresencePenaltyOption.Value != null)
                    writer.WriteNumber("presence_penalty", createChatCompletionRequest.PresencePenaltyOption.Value!.Value);
                else
                    writer.WriteNull("presence_penalty");

            if (createChatCompletionRequest.ResponseFormatOption.IsSet)
            {
                writer.WritePropertyName("response_format");
                JsonSerializer.Serialize(writer, createChatCompletionRequest.ResponseFormat, jsonSerializerOptions);
            }
            if (createChatCompletionRequest.SeedOption.IsSet)
                if (createChatCompletionRequest.SeedOption.Value != null)
                    writer.WriteNumber("seed", createChatCompletionRequest.SeedOption.Value!.Value);
                else
                    writer.WriteNull("seed");

            if (createChatCompletionRequest.StopOption.IsSet)
            {
                writer.WritePropertyName("stop");
                JsonSerializer.Serialize(writer, createChatCompletionRequest.Stop, jsonSerializerOptions);
            }
            if (createChatCompletionRequest.StreamOption.IsSet)
                if (createChatCompletionRequest.StreamOption.Value != null)
                    writer.WriteBoolean("stream", createChatCompletionRequest.StreamOption.Value!.Value);
                else
                    writer.WriteNull("stream");

            if (createChatCompletionRequest.TemperatureOption.IsSet)
                if (createChatCompletionRequest.TemperatureOption.Value != null)
                    writer.WriteNumber("temperature", createChatCompletionRequest.TemperatureOption.Value!.Value);
                else
                    writer.WriteNull("temperature");

            if (createChatCompletionRequest.TopPOption.IsSet)
                if (createChatCompletionRequest.TopPOption.Value != null)
                    writer.WriteNumber("top_p", createChatCompletionRequest.TopPOption.Value!.Value);
                else
                    writer.WriteNull("top_p");

            if (createChatCompletionRequest.ToolsOption.IsSet)
            {
                writer.WritePropertyName("tools");
                JsonSerializer.Serialize(writer, createChatCompletionRequest.Tools, jsonSerializerOptions);
            }
            if (createChatCompletionRequest.ToolChoiceOption.IsSet)
            {
                writer.WritePropertyName("tool_choice");
                JsonSerializer.Serialize(writer, createChatCompletionRequest.ToolChoice, jsonSerializerOptions);
            }
            if (createChatCompletionRequest.UserOption.IsSet)
                writer.WriteString("user", createChatCompletionRequest.User);

            if (createChatCompletionRequest.FunctionCallOption.IsSet)
            {
                writer.WritePropertyName("function_call");
                JsonSerializer.Serialize(writer, createChatCompletionRequest.FunctionCall, jsonSerializerOptions);
            }
            if (createChatCompletionRequest.FunctionsOption.IsSet)
            {
                writer.WritePropertyName("functions");
                JsonSerializer.Serialize(writer, createChatCompletionRequest.Functions, jsonSerializerOptions);
            }
        }
    }
}
