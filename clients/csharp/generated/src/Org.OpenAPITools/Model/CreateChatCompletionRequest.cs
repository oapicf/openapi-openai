/*
 * OpenAI API
 *
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.0.0
 * Contact: blah+oapicf@cliffano.com
 * Generated by: https://github.com/openapitools/openapi-generator.git
 */


using System;
using System.Collections;
using System.Collections.Generic;
using System.Collections.ObjectModel;
using System.Linq;
using System.IO;
using System.Runtime.Serialization;
using System.Text;
using System.Text.RegularExpressions;
using Newtonsoft.Json;
using Newtonsoft.Json.Converters;
using Newtonsoft.Json.Linq;
using System.ComponentModel.DataAnnotations;
using OpenAPIDateConverter = Org.OpenAPITools.Client.OpenAPIDateConverter;

namespace Org.OpenAPITools.Model
{
    /// <summary>
    /// CreateChatCompletionRequest
    /// </summary>
    [DataContract(Name = "CreateChatCompletionRequest")]
    public partial class CreateChatCompletionRequest : IValidatableObject
    {
        /// <summary>
        /// Initializes a new instance of the <see cref="CreateChatCompletionRequest" /> class.
        /// </summary>
        [JsonConstructorAttribute]
        protected CreateChatCompletionRequest() { }
        /// <summary>
        /// Initializes a new instance of the <see cref="CreateChatCompletionRequest" /> class.
        /// </summary>
        /// <param name="messages">A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models). (required).</param>
        /// <param name="model">model (required).</param>
        /// <param name="frequencyPenalty">Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)  (default to 0M).</param>
        /// <param name="logitBias">Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. .</param>
        /// <param name="logprobs">Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the &#x60;content&#x60; of &#x60;message&#x60;. (default to false).</param>
        /// <param name="topLogprobs">An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. &#x60;logprobs&#x60; must be set to &#x60;true&#x60; if this parameter is used..</param>
        /// <param name="maxTokens">The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.  The total length of input tokens and generated tokens is limited by the model&#39;s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. .</param>
        /// <param name="n">How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep &#x60;n&#x60; as &#x60;1&#x60; to minimize costs. (default to 1).</param>
        /// <param name="presencePenalty">Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)  (default to 0M).</param>
        /// <param name="responseFormat">responseFormat.</param>
        /// <param name="seed">This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same &#x60;seed&#x60; and parameters should return the same result. Determinism is not guaranteed, and you should refer to the &#x60;system_fingerprint&#x60; response parameter to monitor changes in the backend. .</param>
        /// <param name="stop">stop.</param>
        /// <param name="stream">If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).  (default to false).</param>
        /// <param name="temperature">What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or &#x60;top_p&#x60; but not both.  (default to 1M).</param>
        /// <param name="topP">An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or &#x60;temperature&#x60; but not both.  (default to 1M).</param>
        /// <param name="tools">A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. .</param>
        /// <param name="toolChoice">toolChoice.</param>
        /// <param name="user">A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). .</param>
        /// <param name="functionCall">functionCall.</param>
        /// <param name="functions">Deprecated in favor of &#x60;tools&#x60;.  A list of functions the model may generate JSON inputs for. .</param>
        public CreateChatCompletionRequest(List<ChatCompletionRequestMessage> messages = default(List<ChatCompletionRequestMessage>), CreateChatCompletionRequestModel model = default(CreateChatCompletionRequestModel), decimal? frequencyPenalty = 0M, Dictionary<string, int> logitBias = default(Dictionary<string, int>), bool? logprobs = false, int? topLogprobs = default(int?), int? maxTokens = default(int?), int? n = 1, decimal? presencePenalty = 0M, CreateChatCompletionRequestResponseFormat responseFormat = default(CreateChatCompletionRequestResponseFormat), int? seed = default(int?), CreateChatCompletionRequestStop stop = default(CreateChatCompletionRequestStop), bool? stream = false, decimal? temperature = 1M, decimal? topP = 1M, List<ChatCompletionTool> tools = default(List<ChatCompletionTool>), ChatCompletionToolChoiceOption toolChoice = default(ChatCompletionToolChoiceOption), string user = default(string), CreateChatCompletionRequestFunctionCall functionCall = default(CreateChatCompletionRequestFunctionCall), List<ChatCompletionFunctions> functions = default(List<ChatCompletionFunctions>))
        {
            // to ensure "messages" is required (not null)
            if (messages == null)
            {
                throw new ArgumentNullException("messages is a required property for CreateChatCompletionRequest and cannot be null");
            }
            this.Messages = messages;
            // to ensure "model" is required (not null)
            if (model == null)
            {
                throw new ArgumentNullException("model is a required property for CreateChatCompletionRequest and cannot be null");
            }
            this.Model = model;
            // use default value if no "frequencyPenalty" provided
            this.FrequencyPenalty = frequencyPenalty ?? 0M;
            this.LogitBias = logitBias;
            // use default value if no "logprobs" provided
            this.Logprobs = logprobs ?? false;
            this.TopLogprobs = topLogprobs;
            this.MaxTokens = maxTokens;
            // use default value if no "n" provided
            this.N = n ?? 1;
            // use default value if no "presencePenalty" provided
            this.PresencePenalty = presencePenalty ?? 0M;
            this.ResponseFormat = responseFormat;
            this.Seed = seed;
            this.Stop = stop;
            // use default value if no "stream" provided
            this.Stream = stream ?? false;
            // use default value if no "temperature" provided
            this.Temperature = temperature ?? 1M;
            // use default value if no "topP" provided
            this.TopP = topP ?? 1M;
            this.Tools = tools;
            this.ToolChoice = toolChoice;
            this.User = user;
            this.FunctionCall = functionCall;
            this.Functions = functions;
        }

        /// <summary>
        /// A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
        /// </summary>
        /// <value>A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).</value>
        [DataMember(Name = "messages", IsRequired = true, EmitDefaultValue = true)]
        public List<ChatCompletionRequestMessage> Messages { get; set; }

        /// <summary>
        /// Gets or Sets Model
        /// </summary>
        [DataMember(Name = "model", IsRequired = true, EmitDefaultValue = true)]
        public CreateChatCompletionRequestModel Model { get; set; }

        /// <summary>
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
        /// </summary>
        /// <value>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) </value>
        [DataMember(Name = "frequency_penalty", EmitDefaultValue = true)]
        public decimal? FrequencyPenalty { get; set; }

        /// <summary>
        /// Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
        /// </summary>
        /// <value>Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. </value>
        [DataMember(Name = "logit_bias", EmitDefaultValue = true)]
        public Dictionary<string, int> LogitBias { get; set; }

        /// <summary>
        /// Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the &#x60;content&#x60; of &#x60;message&#x60;.
        /// </summary>
        /// <value>Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the &#x60;content&#x60; of &#x60;message&#x60;.</value>
        [DataMember(Name = "logprobs", EmitDefaultValue = true)]
        public bool? Logprobs { get; set; }

        /// <summary>
        /// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. &#x60;logprobs&#x60; must be set to &#x60;true&#x60; if this parameter is used.
        /// </summary>
        /// <value>An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. &#x60;logprobs&#x60; must be set to &#x60;true&#x60; if this parameter is used.</value>
        [DataMember(Name = "top_logprobs", EmitDefaultValue = true)]
        public int? TopLogprobs { get; set; }

        /// <summary>
        /// The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.  The total length of input tokens and generated tokens is limited by the model&#39;s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. 
        /// </summary>
        /// <value>The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.  The total length of input tokens and generated tokens is limited by the model&#39;s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. </value>
        [DataMember(Name = "max_tokens", EmitDefaultValue = true)]
        public int? MaxTokens { get; set; }

        /// <summary>
        /// How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep &#x60;n&#x60; as &#x60;1&#x60; to minimize costs.
        /// </summary>
        /// <value>How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep &#x60;n&#x60; as &#x60;1&#x60; to minimize costs.</value>
        /*
        <example>1</example>
        */
        [DataMember(Name = "n", EmitDefaultValue = true)]
        public int? N { get; set; }

        /// <summary>
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
        /// </summary>
        /// <value>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) </value>
        [DataMember(Name = "presence_penalty", EmitDefaultValue = true)]
        public decimal? PresencePenalty { get; set; }

        /// <summary>
        /// Gets or Sets ResponseFormat
        /// </summary>
        [DataMember(Name = "response_format", EmitDefaultValue = false)]
        public CreateChatCompletionRequestResponseFormat ResponseFormat { get; set; }

        /// <summary>
        /// This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same &#x60;seed&#x60; and parameters should return the same result. Determinism is not guaranteed, and you should refer to the &#x60;system_fingerprint&#x60; response parameter to monitor changes in the backend. 
        /// </summary>
        /// <value>This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same &#x60;seed&#x60; and parameters should return the same result. Determinism is not guaranteed, and you should refer to the &#x60;system_fingerprint&#x60; response parameter to monitor changes in the backend. </value>
        [DataMember(Name = "seed", EmitDefaultValue = true)]
        public int? Seed { get; set; }

        /// <summary>
        /// Gets or Sets Stop
        /// </summary>
        [DataMember(Name = "stop", EmitDefaultValue = false)]
        public CreateChatCompletionRequestStop Stop { get; set; }

        /// <summary>
        /// If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
        /// </summary>
        /// <value>If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). </value>
        [DataMember(Name = "stream", EmitDefaultValue = true)]
        public bool? Stream { get; set; }

        /// <summary>
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or &#x60;top_p&#x60; but not both. 
        /// </summary>
        /// <value>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or &#x60;top_p&#x60; but not both. </value>
        /*
        <example>1</example>
        */
        [DataMember(Name = "temperature", EmitDefaultValue = true)]
        public decimal? Temperature { get; set; }

        /// <summary>
        /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or &#x60;temperature&#x60; but not both. 
        /// </summary>
        /// <value>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or &#x60;temperature&#x60; but not both. </value>
        /*
        <example>1</example>
        */
        [DataMember(Name = "top_p", EmitDefaultValue = true)]
        public decimal? TopP { get; set; }

        /// <summary>
        /// A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. 
        /// </summary>
        /// <value>A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. </value>
        [DataMember(Name = "tools", EmitDefaultValue = false)]
        public List<ChatCompletionTool> Tools { get; set; }

        /// <summary>
        /// Gets or Sets ToolChoice
        /// </summary>
        [DataMember(Name = "tool_choice", EmitDefaultValue = false)]
        public ChatCompletionToolChoiceOption ToolChoice { get; set; }

        /// <summary>
        /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). 
        /// </summary>
        /// <value>A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). </value>
        /*
        <example>user-1234</example>
        */
        [DataMember(Name = "user", EmitDefaultValue = false)]
        public string User { get; set; }

        /// <summary>
        /// Gets or Sets FunctionCall
        /// </summary>
        [DataMember(Name = "function_call", EmitDefaultValue = false)]
        [Obsolete]
        public CreateChatCompletionRequestFunctionCall FunctionCall { get; set; }

        /// <summary>
        /// Deprecated in favor of &#x60;tools&#x60;.  A list of functions the model may generate JSON inputs for. 
        /// </summary>
        /// <value>Deprecated in favor of &#x60;tools&#x60;.  A list of functions the model may generate JSON inputs for. </value>
        [DataMember(Name = "functions", EmitDefaultValue = false)]
        [Obsolete]
        public List<ChatCompletionFunctions> Functions { get; set; }

        /// <summary>
        /// Returns the string presentation of the object
        /// </summary>
        /// <returns>String presentation of the object</returns>
        public override string ToString()
        {
            StringBuilder sb = new StringBuilder();
            sb.Append("class CreateChatCompletionRequest {\n");
            sb.Append("  Messages: ").Append(Messages).Append("\n");
            sb.Append("  Model: ").Append(Model).Append("\n");
            sb.Append("  FrequencyPenalty: ").Append(FrequencyPenalty).Append("\n");
            sb.Append("  LogitBias: ").Append(LogitBias).Append("\n");
            sb.Append("  Logprobs: ").Append(Logprobs).Append("\n");
            sb.Append("  TopLogprobs: ").Append(TopLogprobs).Append("\n");
            sb.Append("  MaxTokens: ").Append(MaxTokens).Append("\n");
            sb.Append("  N: ").Append(N).Append("\n");
            sb.Append("  PresencePenalty: ").Append(PresencePenalty).Append("\n");
            sb.Append("  ResponseFormat: ").Append(ResponseFormat).Append("\n");
            sb.Append("  Seed: ").Append(Seed).Append("\n");
            sb.Append("  Stop: ").Append(Stop).Append("\n");
            sb.Append("  Stream: ").Append(Stream).Append("\n");
            sb.Append("  Temperature: ").Append(Temperature).Append("\n");
            sb.Append("  TopP: ").Append(TopP).Append("\n");
            sb.Append("  Tools: ").Append(Tools).Append("\n");
            sb.Append("  ToolChoice: ").Append(ToolChoice).Append("\n");
            sb.Append("  User: ").Append(User).Append("\n");
            sb.Append("  FunctionCall: ").Append(FunctionCall).Append("\n");
            sb.Append("  Functions: ").Append(Functions).Append("\n");
            sb.Append("}\n");
            return sb.ToString();
        }

        /// <summary>
        /// Returns the JSON string presentation of the object
        /// </summary>
        /// <returns>JSON string presentation of the object</returns>
        public virtual string ToJson()
        {
            return Newtonsoft.Json.JsonConvert.SerializeObject(this, Newtonsoft.Json.Formatting.Indented);
        }

        /// <summary>
        /// To validate all properties of the instance
        /// </summary>
        /// <param name="validationContext">Validation context</param>
        /// <returns>Validation Result</returns>
        IEnumerable<ValidationResult> IValidatableObject.Validate(ValidationContext validationContext)
        {
            // FrequencyPenalty (decimal?) maximum
            if (this.FrequencyPenalty > (decimal?)2)
            {
                yield return new ValidationResult("Invalid value for FrequencyPenalty, must be a value less than or equal to 2.", new [] { "FrequencyPenalty" });
            }

            // FrequencyPenalty (decimal?) minimum
            if (this.FrequencyPenalty < (decimal?)-2)
            {
                yield return new ValidationResult("Invalid value for FrequencyPenalty, must be a value greater than or equal to -2.", new [] { "FrequencyPenalty" });
            }

            // TopLogprobs (int?) maximum
            if (this.TopLogprobs > (int?)20)
            {
                yield return new ValidationResult("Invalid value for TopLogprobs, must be a value less than or equal to 20.", new [] { "TopLogprobs" });
            }

            // TopLogprobs (int?) minimum
            if (this.TopLogprobs < (int?)0)
            {
                yield return new ValidationResult("Invalid value for TopLogprobs, must be a value greater than or equal to 0.", new [] { "TopLogprobs" });
            }

            // N (int?) maximum
            if (this.N > (int?)128)
            {
                yield return new ValidationResult("Invalid value for N, must be a value less than or equal to 128.", new [] { "N" });
            }

            // N (int?) minimum
            if (this.N < (int?)1)
            {
                yield return new ValidationResult("Invalid value for N, must be a value greater than or equal to 1.", new [] { "N" });
            }

            // PresencePenalty (decimal?) maximum
            if (this.PresencePenalty > (decimal?)2)
            {
                yield return new ValidationResult("Invalid value for PresencePenalty, must be a value less than or equal to 2.", new [] { "PresencePenalty" });
            }

            // PresencePenalty (decimal?) minimum
            if (this.PresencePenalty < (decimal?)-2)
            {
                yield return new ValidationResult("Invalid value for PresencePenalty, must be a value greater than or equal to -2.", new [] { "PresencePenalty" });
            }

            // Seed (int?) maximum
            if (this.Seed > (int?)9223372036854775807)
            {
                yield return new ValidationResult("Invalid value for Seed, must be a value less than or equal to 9223372036854775807.", new [] { "Seed" });
            }

            // Seed (int?) minimum
            if (this.Seed < (int?)-9223372036854775808)
            {
                yield return new ValidationResult("Invalid value for Seed, must be a value greater than or equal to -9223372036854775808.", new [] { "Seed" });
            }

            // Temperature (decimal?) maximum
            if (this.Temperature > (decimal?)2)
            {
                yield return new ValidationResult("Invalid value for Temperature, must be a value less than or equal to 2.", new [] { "Temperature" });
            }

            // Temperature (decimal?) minimum
            if (this.Temperature < (decimal?)0)
            {
                yield return new ValidationResult("Invalid value for Temperature, must be a value greater than or equal to 0.", new [] { "Temperature" });
            }

            // TopP (decimal?) maximum
            if (this.TopP > (decimal?)1)
            {
                yield return new ValidationResult("Invalid value for TopP, must be a value less than or equal to 1.", new [] { "TopP" });
            }

            // TopP (decimal?) minimum
            if (this.TopP < (decimal?)0)
            {
                yield return new ValidationResult("Invalid value for TopP, must be a value greater than or equal to 0.", new [] { "TopP" });
            }

            yield break;
        }
    }

}
