// <auto-generated>
/*
 * OpenAI API
 *
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.3.0
 * Contact: blah+oapicf@cliffano.com
 * Generated by: https://github.com/openapitools/openapi-generator.git
 */

#nullable enable

using System;
using System.Collections;
using System.Collections.Generic;
using System.Collections.ObjectModel;
using System.Linq;
using System.IO;
using System.Text;
using System.Text.RegularExpressions;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.ComponentModel.DataAnnotations;
using Org.OpenAPITools.Client;

namespace Org.OpenAPITools.Model
{
    /// <summary>
    /// CreateCompletionRequest
    /// </summary>
    public partial class CreateCompletionRequest : IValidatableObject
    {
        /// <summary>
        /// Initializes a new instance of the <see cref="CreateCompletionRequest" /> class.
        /// </summary>
        /// <param name="model">model</param>
        /// <param name="prompt">prompt</param>
        /// <param name="bestOf">Generates &#x60;best_of&#x60; completions server-side and returns the \&quot;best\&quot; (the one with the highest log probability per token). Results cannot be streamed.  When used with &#x60;n&#x60;, &#x60;best_of&#x60; controls the number of candidate completions and &#x60;n&#x60; specifies how many to return – &#x60;best_of&#x60; must be greater than &#x60;n&#x60;.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for &#x60;max_tokens&#x60; and &#x60;stop&#x60;.  (default to 1)</param>
        /// <param name="echo">Echo back the prompt in addition to the completion  (default to false)</param>
        /// <param name="frequencyPenalty">Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation)  (default to 0M)</param>
        /// <param name="logitBias">Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view&#x3D;bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.  As an example, you can pass &#x60;{\&quot;50256\&quot;: -100}&#x60; to prevent the &lt;|endoftext|&gt; token from being generated. </param>
        /// <param name="logprobs">Include the log probabilities on the &#x60;logprobs&#x60; most likely output tokens, as well the chosen tokens. For example, if &#x60;logprobs&#x60; is 5, the API will return a list of the 5 most likely tokens. The API will always return the &#x60;logprob&#x60; of the sampled token, so there may be up to &#x60;logprobs+1&#x60; elements in the response.  The maximum value for &#x60;logprobs&#x60; is 5. </param>
        /// <param name="maxTokens">The maximum number of [tokens](/tokenizer) that can be generated in the completion.  The token count of your prompt plus &#x60;max_tokens&#x60; cannot exceed the model&#39;s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.  (default to 16)</param>
        /// <param name="n">How many completions to generate for each prompt.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for &#x60;max_tokens&#x60; and &#x60;stop&#x60;.  (default to 1)</param>
        /// <param name="presencePenalty">Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation)  (default to 0M)</param>
        /// <param name="seed">If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same &#x60;seed&#x60; and parameters should return the same result.  Determinism is not guaranteed, and you should refer to the &#x60;system_fingerprint&#x60; response parameter to monitor changes in the backend. </param>
        /// <param name="stop">stop</param>
        /// <param name="stream">Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).  (default to false)</param>
        /// <param name="streamOptions">streamOptions</param>
        /// <param name="suffix">The suffix that comes after a completion of inserted text.  This parameter is only supported for &#x60;gpt-3.5-turbo-instruct&#x60;. </param>
        /// <param name="temperature">What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or &#x60;top_p&#x60; but not both.  (default to 1M)</param>
        /// <param name="topP">An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or &#x60;temperature&#x60; but not both.  (default to 1M)</param>
        /// <param name="user">A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). </param>
        [JsonConstructor]
        public CreateCompletionRequest(CreateCompletionRequestModel model, CreateCompletionRequestPrompt? prompt = default, Option<int?> bestOf = default, Option<bool?> echo = default, Option<decimal?> frequencyPenalty = default, Option<Dictionary<string, int>?> logitBias = default, Option<int?> logprobs = default, Option<int?> maxTokens = default, Option<int?> n = default, Option<decimal?> presencePenalty = default, Option<long?> seed = default, Option<CreateCompletionRequestStop?> stop = default, Option<bool?> stream = default, Option<ChatCompletionStreamOptions?> streamOptions = default, Option<string?> suffix = default, Option<decimal?> temperature = default, Option<decimal?> topP = default, Option<string?> user = default)
        {
            Model = model;
            Prompt = prompt;
            BestOfOption = bestOf;
            EchoOption = echo;
            FrequencyPenaltyOption = frequencyPenalty;
            LogitBiasOption = logitBias;
            LogprobsOption = logprobs;
            MaxTokensOption = maxTokens;
            NOption = n;
            PresencePenaltyOption = presencePenalty;
            SeedOption = seed;
            StopOption = stop;
            StreamOption = stream;
            StreamOptionsOption = streamOptions;
            SuffixOption = suffix;
            TemperatureOption = temperature;
            TopPOption = topP;
            UserOption = user;
            OnCreated();
        }

        partial void OnCreated();

        /// <summary>
        /// Gets or Sets Model
        /// </summary>
        [JsonPropertyName("model")]
        public CreateCompletionRequestModel Model { get; set; }

        /// <summary>
        /// Gets or Sets Prompt
        /// </summary>
        [JsonPropertyName("prompt")]
        public CreateCompletionRequestPrompt? Prompt { get; set; }

        /// <summary>
        /// Used to track the state of BestOf
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<int?> BestOfOption { get; private set; }

        /// <summary>
        /// Generates &#x60;best_of&#x60; completions server-side and returns the \&quot;best\&quot; (the one with the highest log probability per token). Results cannot be streamed.  When used with &#x60;n&#x60;, &#x60;best_of&#x60; controls the number of candidate completions and &#x60;n&#x60; specifies how many to return – &#x60;best_of&#x60; must be greater than &#x60;n&#x60;.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for &#x60;max_tokens&#x60; and &#x60;stop&#x60;. 
        /// </summary>
        /// <value>Generates &#x60;best_of&#x60; completions server-side and returns the \&quot;best\&quot; (the one with the highest log probability per token). Results cannot be streamed.  When used with &#x60;n&#x60;, &#x60;best_of&#x60; controls the number of candidate completions and &#x60;n&#x60; specifies how many to return – &#x60;best_of&#x60; must be greater than &#x60;n&#x60;.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for &#x60;max_tokens&#x60; and &#x60;stop&#x60;. </value>
        [JsonPropertyName("best_of")]
        public int? BestOf { get { return this.BestOfOption; } set { this.BestOfOption = new(value); } }

        /// <summary>
        /// Used to track the state of Echo
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<bool?> EchoOption { get; private set; }

        /// <summary>
        /// Echo back the prompt in addition to the completion 
        /// </summary>
        /// <value>Echo back the prompt in addition to the completion </value>
        [JsonPropertyName("echo")]
        public bool? Echo { get { return this.EchoOption; } set { this.EchoOption = new(value); } }

        /// <summary>
        /// Used to track the state of FrequencyPenalty
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<decimal?> FrequencyPenaltyOption { get; private set; }

        /// <summary>
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation) 
        /// </summary>
        /// <value>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation) </value>
        [JsonPropertyName("frequency_penalty")]
        public decimal? FrequencyPenalty { get { return this.FrequencyPenaltyOption; } set { this.FrequencyPenaltyOption = new(value); } }

        /// <summary>
        /// Used to track the state of LogitBias
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<Dictionary<string, int>?> LogitBiasOption { get; private set; }

        /// <summary>
        /// Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view&#x3D;bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.  As an example, you can pass &#x60;{\&quot;50256\&quot;: -100}&#x60; to prevent the &lt;|endoftext|&gt; token from being generated. 
        /// </summary>
        /// <value>Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view&#x3D;bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.  As an example, you can pass &#x60;{\&quot;50256\&quot;: -100}&#x60; to prevent the &lt;|endoftext|&gt; token from being generated. </value>
        [JsonPropertyName("logit_bias")]
        public Dictionary<string, int>? LogitBias { get { return this.LogitBiasOption; } set { this.LogitBiasOption = new(value); } }

        /// <summary>
        /// Used to track the state of Logprobs
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<int?> LogprobsOption { get; private set; }

        /// <summary>
        /// Include the log probabilities on the &#x60;logprobs&#x60; most likely output tokens, as well the chosen tokens. For example, if &#x60;logprobs&#x60; is 5, the API will return a list of the 5 most likely tokens. The API will always return the &#x60;logprob&#x60; of the sampled token, so there may be up to &#x60;logprobs+1&#x60; elements in the response.  The maximum value for &#x60;logprobs&#x60; is 5. 
        /// </summary>
        /// <value>Include the log probabilities on the &#x60;logprobs&#x60; most likely output tokens, as well the chosen tokens. For example, if &#x60;logprobs&#x60; is 5, the API will return a list of the 5 most likely tokens. The API will always return the &#x60;logprob&#x60; of the sampled token, so there may be up to &#x60;logprobs+1&#x60; elements in the response.  The maximum value for &#x60;logprobs&#x60; is 5. </value>
        [JsonPropertyName("logprobs")]
        public int? Logprobs { get { return this.LogprobsOption; } set { this.LogprobsOption = new(value); } }

        /// <summary>
        /// Used to track the state of MaxTokens
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<int?> MaxTokensOption { get; private set; }

        /// <summary>
        /// The maximum number of [tokens](/tokenizer) that can be generated in the completion.  The token count of your prompt plus &#x60;max_tokens&#x60; cannot exceed the model&#39;s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. 
        /// </summary>
        /// <value>The maximum number of [tokens](/tokenizer) that can be generated in the completion.  The token count of your prompt plus &#x60;max_tokens&#x60; cannot exceed the model&#39;s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. </value>
        /* <example>16</example> */
        [JsonPropertyName("max_tokens")]
        public int? MaxTokens { get { return this.MaxTokensOption; } set { this.MaxTokensOption = new(value); } }

        /// <summary>
        /// Used to track the state of N
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<int?> NOption { get; private set; }

        /// <summary>
        /// How many completions to generate for each prompt.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for &#x60;max_tokens&#x60; and &#x60;stop&#x60;. 
        /// </summary>
        /// <value>How many completions to generate for each prompt.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for &#x60;max_tokens&#x60; and &#x60;stop&#x60;. </value>
        /* <example>1</example> */
        [JsonPropertyName("n")]
        public int? N { get { return this.NOption; } set { this.NOption = new(value); } }

        /// <summary>
        /// Used to track the state of PresencePenalty
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<decimal?> PresencePenaltyOption { get; private set; }

        /// <summary>
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation) 
        /// </summary>
        /// <value>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation) </value>
        [JsonPropertyName("presence_penalty")]
        public decimal? PresencePenalty { get { return this.PresencePenaltyOption; } set { this.PresencePenaltyOption = new(value); } }

        /// <summary>
        /// Used to track the state of Seed
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<long?> SeedOption { get; private set; }

        /// <summary>
        /// If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same &#x60;seed&#x60; and parameters should return the same result.  Determinism is not guaranteed, and you should refer to the &#x60;system_fingerprint&#x60; response parameter to monitor changes in the backend. 
        /// </summary>
        /// <value>If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same &#x60;seed&#x60; and parameters should return the same result.  Determinism is not guaranteed, and you should refer to the &#x60;system_fingerprint&#x60; response parameter to monitor changes in the backend. </value>
        [JsonPropertyName("seed")]
        public long? Seed { get { return this.SeedOption; } set { this.SeedOption = new(value); } }

        /// <summary>
        /// Used to track the state of Stop
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<CreateCompletionRequestStop?> StopOption { get; private set; }

        /// <summary>
        /// Gets or Sets Stop
        /// </summary>
        [JsonPropertyName("stop")]
        public CreateCompletionRequestStop? Stop { get { return this.StopOption; } set { this.StopOption = new(value); } }

        /// <summary>
        /// Used to track the state of Stream
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<bool?> StreamOption { get; private set; }

        /// <summary>
        /// Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
        /// </summary>
        /// <value>Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). </value>
        [JsonPropertyName("stream")]
        public bool? Stream { get { return this.StreamOption; } set { this.StreamOption = new(value); } }

        /// <summary>
        /// Used to track the state of StreamOptions
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<ChatCompletionStreamOptions?> StreamOptionsOption { get; private set; }

        /// <summary>
        /// Gets or Sets StreamOptions
        /// </summary>
        [JsonPropertyName("stream_options")]
        public ChatCompletionStreamOptions? StreamOptions { get { return this.StreamOptionsOption; } set { this.StreamOptionsOption = new(value); } }

        /// <summary>
        /// Used to track the state of Suffix
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<string?> SuffixOption { get; private set; }

        /// <summary>
        /// The suffix that comes after a completion of inserted text.  This parameter is only supported for &#x60;gpt-3.5-turbo-instruct&#x60;. 
        /// </summary>
        /// <value>The suffix that comes after a completion of inserted text.  This parameter is only supported for &#x60;gpt-3.5-turbo-instruct&#x60;. </value>
        /* <example>test.</example> */
        [JsonPropertyName("suffix")]
        public string? Suffix { get { return this.SuffixOption; } set { this.SuffixOption = new(value); } }

        /// <summary>
        /// Used to track the state of Temperature
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<decimal?> TemperatureOption { get; private set; }

        /// <summary>
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or &#x60;top_p&#x60; but not both. 
        /// </summary>
        /// <value>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or &#x60;top_p&#x60; but not both. </value>
        /* <example>1</example> */
        [JsonPropertyName("temperature")]
        public decimal? Temperature { get { return this.TemperatureOption; } set { this.TemperatureOption = new(value); } }

        /// <summary>
        /// Used to track the state of TopP
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<decimal?> TopPOption { get; private set; }

        /// <summary>
        /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or &#x60;temperature&#x60; but not both. 
        /// </summary>
        /// <value>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or &#x60;temperature&#x60; but not both. </value>
        /* <example>1</example> */
        [JsonPropertyName("top_p")]
        public decimal? TopP { get { return this.TopPOption; } set { this.TopPOption = new(value); } }

        /// <summary>
        /// Used to track the state of User
        /// </summary>
        [JsonIgnore]
        [global::System.ComponentModel.EditorBrowsable(global::System.ComponentModel.EditorBrowsableState.Never)]
        public Option<string?> UserOption { get; private set; }

        /// <summary>
        /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). 
        /// </summary>
        /// <value>A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). </value>
        /* <example>user-1234</example> */
        [JsonPropertyName("user")]
        public string? User { get { return this.UserOption; } set { this.UserOption = new(value); } }

        /// <summary>
        /// Returns the string presentation of the object
        /// </summary>
        /// <returns>String presentation of the object</returns>
        public override string ToString()
        {
            StringBuilder sb = new StringBuilder();
            sb.Append("class CreateCompletionRequest {\n");
            sb.Append("  Model: ").Append(Model).Append("\n");
            sb.Append("  Prompt: ").Append(Prompt).Append("\n");
            sb.Append("  BestOf: ").Append(BestOf).Append("\n");
            sb.Append("  Echo: ").Append(Echo).Append("\n");
            sb.Append("  FrequencyPenalty: ").Append(FrequencyPenalty).Append("\n");
            sb.Append("  LogitBias: ").Append(LogitBias).Append("\n");
            sb.Append("  Logprobs: ").Append(Logprobs).Append("\n");
            sb.Append("  MaxTokens: ").Append(MaxTokens).Append("\n");
            sb.Append("  N: ").Append(N).Append("\n");
            sb.Append("  PresencePenalty: ").Append(PresencePenalty).Append("\n");
            sb.Append("  Seed: ").Append(Seed).Append("\n");
            sb.Append("  Stop: ").Append(Stop).Append("\n");
            sb.Append("  Stream: ").Append(Stream).Append("\n");
            sb.Append("  StreamOptions: ").Append(StreamOptions).Append("\n");
            sb.Append("  Suffix: ").Append(Suffix).Append("\n");
            sb.Append("  Temperature: ").Append(Temperature).Append("\n");
            sb.Append("  TopP: ").Append(TopP).Append("\n");
            sb.Append("  User: ").Append(User).Append("\n");
            sb.Append("}\n");
            return sb.ToString();
        }

        /// <summary>
        /// To validate all properties of the instance
        /// </summary>
        /// <param name="validationContext">Validation context</param>
        /// <returns>Validation Result</returns>
        IEnumerable<ValidationResult> IValidatableObject.Validate(ValidationContext validationContext)
        {
            // BestOf (int) maximum
            if (this.BestOfOption.IsSet && this.BestOfOption.Value > (int)20)
            {
                yield return new ValidationResult("Invalid value for BestOf, must be a value less than or equal to 20.", new [] { "BestOf" });
            }

            // BestOf (int) minimum
            if (this.BestOfOption.IsSet && this.BestOfOption.Value < (int)0)
            {
                yield return new ValidationResult("Invalid value for BestOf, must be a value greater than or equal to 0.", new [] { "BestOf" });
            }

            // FrequencyPenalty (decimal) maximum
            if (this.FrequencyPenaltyOption.IsSet && this.FrequencyPenaltyOption.Value > (decimal)2)
            {
                yield return new ValidationResult("Invalid value for FrequencyPenalty, must be a value less than or equal to 2.", new [] { "FrequencyPenalty" });
            }

            // FrequencyPenalty (decimal) minimum
            if (this.FrequencyPenaltyOption.IsSet && this.FrequencyPenaltyOption.Value < (decimal)-2)
            {
                yield return new ValidationResult("Invalid value for FrequencyPenalty, must be a value greater than or equal to -2.", new [] { "FrequencyPenalty" });
            }

            // Logprobs (int) maximum
            if (this.LogprobsOption.IsSet && this.LogprobsOption.Value > (int)5)
            {
                yield return new ValidationResult("Invalid value for Logprobs, must be a value less than or equal to 5.", new [] { "Logprobs" });
            }

            // Logprobs (int) minimum
            if (this.LogprobsOption.IsSet && this.LogprobsOption.Value < (int)0)
            {
                yield return new ValidationResult("Invalid value for Logprobs, must be a value greater than or equal to 0.", new [] { "Logprobs" });
            }

            // MaxTokens (int) minimum
            if (this.MaxTokensOption.IsSet && this.MaxTokensOption.Value < (int)0)
            {
                yield return new ValidationResult("Invalid value for MaxTokens, must be a value greater than or equal to 0.", new [] { "MaxTokens" });
            }

            // N (int) maximum
            if (this.NOption.IsSet && this.NOption.Value > (int)128)
            {
                yield return new ValidationResult("Invalid value for N, must be a value less than or equal to 128.", new [] { "N" });
            }

            // N (int) minimum
            if (this.NOption.IsSet && this.NOption.Value < (int)1)
            {
                yield return new ValidationResult("Invalid value for N, must be a value greater than or equal to 1.", new [] { "N" });
            }

            // PresencePenalty (decimal) maximum
            if (this.PresencePenaltyOption.IsSet && this.PresencePenaltyOption.Value > (decimal)2)
            {
                yield return new ValidationResult("Invalid value for PresencePenalty, must be a value less than or equal to 2.", new [] { "PresencePenalty" });
            }

            // PresencePenalty (decimal) minimum
            if (this.PresencePenaltyOption.IsSet && this.PresencePenaltyOption.Value < (decimal)-2)
            {
                yield return new ValidationResult("Invalid value for PresencePenalty, must be a value greater than or equal to -2.", new [] { "PresencePenalty" });
            }

            // Seed (long) maximum
            if (this.SeedOption.IsSet && this.SeedOption.Value > (long)9223372036854776000)
            {
                yield return new ValidationResult("Invalid value for Seed, must be a value less than or equal to 9223372036854776000.", new [] { "Seed" });
            }

            // Seed (long) minimum
            if (this.SeedOption.IsSet && this.SeedOption.Value < (long)-9223372036854776000)
            {
                yield return new ValidationResult("Invalid value for Seed, must be a value greater than or equal to -9223372036854776000.", new [] { "Seed" });
            }

            // Temperature (decimal) maximum
            if (this.TemperatureOption.IsSet && this.TemperatureOption.Value > (decimal)2)
            {
                yield return new ValidationResult("Invalid value for Temperature, must be a value less than or equal to 2.", new [] { "Temperature" });
            }

            // Temperature (decimal) minimum
            if (this.TemperatureOption.IsSet && this.TemperatureOption.Value < (decimal)0)
            {
                yield return new ValidationResult("Invalid value for Temperature, must be a value greater than or equal to 0.", new [] { "Temperature" });
            }

            // TopP (decimal) maximum
            if (this.TopPOption.IsSet && this.TopPOption.Value > (decimal)1)
            {
                yield return new ValidationResult("Invalid value for TopP, must be a value less than or equal to 1.", new [] { "TopP" });
            }

            // TopP (decimal) minimum
            if (this.TopPOption.IsSet && this.TopPOption.Value < (decimal)0)
            {
                yield return new ValidationResult("Invalid value for TopP, must be a value greater than or equal to 0.", new [] { "TopP" });
            }

            yield break;
        }
    }

    /// <summary>
    /// A Json converter for type <see cref="CreateCompletionRequest" />
    /// </summary>
    public class CreateCompletionRequestJsonConverter : JsonConverter<CreateCompletionRequest>
    {
        /// <summary>
        /// Deserializes json to <see cref="CreateCompletionRequest" />
        /// </summary>
        /// <param name="utf8JsonReader"></param>
        /// <param name="typeToConvert"></param>
        /// <param name="jsonSerializerOptions"></param>
        /// <returns></returns>
        /// <exception cref="JsonException"></exception>
        public override CreateCompletionRequest Read(ref Utf8JsonReader utf8JsonReader, Type typeToConvert, JsonSerializerOptions jsonSerializerOptions)
        {
            int currentDepth = utf8JsonReader.CurrentDepth;

            if (utf8JsonReader.TokenType != JsonTokenType.StartObject && utf8JsonReader.TokenType != JsonTokenType.StartArray)
                throw new JsonException();

            JsonTokenType startingTokenType = utf8JsonReader.TokenType;

            Option<CreateCompletionRequestModel?> model = default;
            Option<CreateCompletionRequestPrompt?> prompt = default;
            Option<int?> bestOf = default;
            Option<bool?> echo = default;
            Option<decimal?> frequencyPenalty = default;
            Option<Dictionary<string, int>?> logitBias = default;
            Option<int?> logprobs = default;
            Option<int?> maxTokens = default;
            Option<int?> n = default;
            Option<decimal?> presencePenalty = default;
            Option<long?> seed = default;
            Option<CreateCompletionRequestStop?> stop = default;
            Option<bool?> stream = default;
            Option<ChatCompletionStreamOptions?> streamOptions = default;
            Option<string?> suffix = default;
            Option<decimal?> temperature = default;
            Option<decimal?> topP = default;
            Option<string?> user = default;

            while (utf8JsonReader.Read())
            {
                if (startingTokenType == JsonTokenType.StartObject && utf8JsonReader.TokenType == JsonTokenType.EndObject && currentDepth == utf8JsonReader.CurrentDepth)
                    break;

                if (startingTokenType == JsonTokenType.StartArray && utf8JsonReader.TokenType == JsonTokenType.EndArray && currentDepth == utf8JsonReader.CurrentDepth)
                    break;

                if (utf8JsonReader.TokenType == JsonTokenType.PropertyName && currentDepth == utf8JsonReader.CurrentDepth - 1)
                {
                    string? localVarJsonPropertyName = utf8JsonReader.GetString();
                    utf8JsonReader.Read();

                    switch (localVarJsonPropertyName)
                    {
                        case "model":
                            model = new Option<CreateCompletionRequestModel?>(JsonSerializer.Deserialize<CreateCompletionRequestModel>(ref utf8JsonReader, jsonSerializerOptions)!);
                            break;
                        case "prompt":
                            prompt = new Option<CreateCompletionRequestPrompt?>(JsonSerializer.Deserialize<CreateCompletionRequestPrompt>(ref utf8JsonReader, jsonSerializerOptions));
                            break;
                        case "best_of":
                            bestOf = new Option<int?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (int?)null : utf8JsonReader.GetInt32());
                            break;
                        case "echo":
                            echo = new Option<bool?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (bool?)null : utf8JsonReader.GetBoolean());
                            break;
                        case "frequency_penalty":
                            frequencyPenalty = new Option<decimal?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (decimal?)null : utf8JsonReader.GetDecimal());
                            break;
                        case "logit_bias":
                            logitBias = new Option<Dictionary<string, int>?>(JsonSerializer.Deserialize<Dictionary<string, int>>(ref utf8JsonReader, jsonSerializerOptions));
                            break;
                        case "logprobs":
                            logprobs = new Option<int?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (int?)null : utf8JsonReader.GetInt32());
                            break;
                        case "max_tokens":
                            maxTokens = new Option<int?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (int?)null : utf8JsonReader.GetInt32());
                            break;
                        case "n":
                            n = new Option<int?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (int?)null : utf8JsonReader.GetInt32());
                            break;
                        case "presence_penalty":
                            presencePenalty = new Option<decimal?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (decimal?)null : utf8JsonReader.GetDecimal());
                            break;
                        case "seed":
                            seed = new Option<long?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (int?)null : utf8JsonReader.GetInt32());
                            break;
                        case "stop":
                            stop = new Option<CreateCompletionRequestStop?>(JsonSerializer.Deserialize<CreateCompletionRequestStop>(ref utf8JsonReader, jsonSerializerOptions));
                            break;
                        case "stream":
                            stream = new Option<bool?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (bool?)null : utf8JsonReader.GetBoolean());
                            break;
                        case "stream_options":
                            streamOptions = new Option<ChatCompletionStreamOptions?>(JsonSerializer.Deserialize<ChatCompletionStreamOptions>(ref utf8JsonReader, jsonSerializerOptions));
                            break;
                        case "suffix":
                            suffix = new Option<string?>(utf8JsonReader.GetString());
                            break;
                        case "temperature":
                            temperature = new Option<decimal?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (decimal?)null : utf8JsonReader.GetDecimal());
                            break;
                        case "top_p":
                            topP = new Option<decimal?>(utf8JsonReader.TokenType == JsonTokenType.Null ? (decimal?)null : utf8JsonReader.GetDecimal());
                            break;
                        case "user":
                            user = new Option<string?>(utf8JsonReader.GetString()!);
                            break;
                        default:
                            break;
                    }
                }
            }

            if (!model.IsSet)
                throw new ArgumentException("Property is required for class CreateCompletionRequest.", nameof(model));

            if (!prompt.IsSet)
                throw new ArgumentException("Property is required for class CreateCompletionRequest.", nameof(prompt));

            if (model.IsSet && model.Value == null)
                throw new ArgumentNullException(nameof(model), "Property is not nullable for class CreateCompletionRequest.");

            if (user.IsSet && user.Value == null)
                throw new ArgumentNullException(nameof(user), "Property is not nullable for class CreateCompletionRequest.");

            return new CreateCompletionRequest(model.Value!, prompt.Value!, bestOf, echo, frequencyPenalty, logitBias, logprobs, maxTokens, n, presencePenalty, seed, stop, stream, streamOptions, suffix, temperature, topP, user);
        }

        /// <summary>
        /// Serializes a <see cref="CreateCompletionRequest" />
        /// </summary>
        /// <param name="writer"></param>
        /// <param name="createCompletionRequest"></param>
        /// <param name="jsonSerializerOptions"></param>
        /// <exception cref="NotImplementedException"></exception>
        public override void Write(Utf8JsonWriter writer, CreateCompletionRequest createCompletionRequest, JsonSerializerOptions jsonSerializerOptions)
        {
            writer.WriteStartObject();

            WriteProperties(writer, createCompletionRequest, jsonSerializerOptions);
            writer.WriteEndObject();
        }

        /// <summary>
        /// Serializes the properties of <see cref="CreateCompletionRequest" />
        /// </summary>
        /// <param name="writer"></param>
        /// <param name="createCompletionRequest"></param>
        /// <param name="jsonSerializerOptions"></param>
        /// <exception cref="NotImplementedException"></exception>
        public void WriteProperties(Utf8JsonWriter writer, CreateCompletionRequest createCompletionRequest, JsonSerializerOptions jsonSerializerOptions)
        {
            if (createCompletionRequest.Model == null)
                throw new ArgumentNullException(nameof(createCompletionRequest.Model), "Property is required for class CreateCompletionRequest.");

            if (createCompletionRequest.UserOption.IsSet && createCompletionRequest.User == null)
                throw new ArgumentNullException(nameof(createCompletionRequest.User), "Property is required for class CreateCompletionRequest.");

            writer.WritePropertyName("model");
            JsonSerializer.Serialize(writer, createCompletionRequest.Model, jsonSerializerOptions);
            if (createCompletionRequest.Prompt != null)
            {
                writer.WritePropertyName("prompt");
                JsonSerializer.Serialize(writer, createCompletionRequest.Prompt, jsonSerializerOptions);
            }
            else
                writer.WriteNull("prompt");
            if (createCompletionRequest.BestOfOption.IsSet)
                if (createCompletionRequest.BestOfOption.Value != null)
                    writer.WriteNumber("best_of", createCompletionRequest.BestOfOption.Value!.Value);
                else
                    writer.WriteNull("best_of");

            if (createCompletionRequest.EchoOption.IsSet)
                if (createCompletionRequest.EchoOption.Value != null)
                    writer.WriteBoolean("echo", createCompletionRequest.EchoOption.Value!.Value);
                else
                    writer.WriteNull("echo");

            if (createCompletionRequest.FrequencyPenaltyOption.IsSet)
                if (createCompletionRequest.FrequencyPenaltyOption.Value != null)
                    writer.WriteNumber("frequency_penalty", createCompletionRequest.FrequencyPenaltyOption.Value!.Value);
                else
                    writer.WriteNull("frequency_penalty");

            if (createCompletionRequest.LogitBiasOption.IsSet)
                if (createCompletionRequest.LogitBiasOption.Value != null)
                {
                    writer.WritePropertyName("logit_bias");
                    JsonSerializer.Serialize(writer, createCompletionRequest.LogitBias, jsonSerializerOptions);
                }
                else
                    writer.WriteNull("logit_bias");
            if (createCompletionRequest.LogprobsOption.IsSet)
                if (createCompletionRequest.LogprobsOption.Value != null)
                    writer.WriteNumber("logprobs", createCompletionRequest.LogprobsOption.Value!.Value);
                else
                    writer.WriteNull("logprobs");

            if (createCompletionRequest.MaxTokensOption.IsSet)
                if (createCompletionRequest.MaxTokensOption.Value != null)
                    writer.WriteNumber("max_tokens", createCompletionRequest.MaxTokensOption.Value!.Value);
                else
                    writer.WriteNull("max_tokens");

            if (createCompletionRequest.NOption.IsSet)
                if (createCompletionRequest.NOption.Value != null)
                    writer.WriteNumber("n", createCompletionRequest.NOption.Value!.Value);
                else
                    writer.WriteNull("n");

            if (createCompletionRequest.PresencePenaltyOption.IsSet)
                if (createCompletionRequest.PresencePenaltyOption.Value != null)
                    writer.WriteNumber("presence_penalty", createCompletionRequest.PresencePenaltyOption.Value!.Value);
                else
                    writer.WriteNull("presence_penalty");

            if (createCompletionRequest.SeedOption.IsSet)
                if (createCompletionRequest.SeedOption.Value != null)
                    writer.WriteNumber("seed", createCompletionRequest.SeedOption.Value!.Value);
                else
                    writer.WriteNull("seed");

            if (createCompletionRequest.StopOption.IsSet)
                if (createCompletionRequest.StopOption.Value != null)
                {
                    writer.WritePropertyName("stop");
                    JsonSerializer.Serialize(writer, createCompletionRequest.Stop, jsonSerializerOptions);
                }
                else
                    writer.WriteNull("stop");
            if (createCompletionRequest.StreamOption.IsSet)
                if (createCompletionRequest.StreamOption.Value != null)
                    writer.WriteBoolean("stream", createCompletionRequest.StreamOption.Value!.Value);
                else
                    writer.WriteNull("stream");

            if (createCompletionRequest.StreamOptionsOption.IsSet)
                if (createCompletionRequest.StreamOptionsOption.Value != null)
                {
                    writer.WritePropertyName("stream_options");
                    JsonSerializer.Serialize(writer, createCompletionRequest.StreamOptions, jsonSerializerOptions);
                }
                else
                    writer.WriteNull("stream_options");
            if (createCompletionRequest.SuffixOption.IsSet)
                if (createCompletionRequest.SuffixOption.Value != null)
                    writer.WriteString("suffix", createCompletionRequest.Suffix);
                else
                    writer.WriteNull("suffix");

            if (createCompletionRequest.TemperatureOption.IsSet)
                if (createCompletionRequest.TemperatureOption.Value != null)
                    writer.WriteNumber("temperature", createCompletionRequest.TemperatureOption.Value!.Value);
                else
                    writer.WriteNull("temperature");

            if (createCompletionRequest.TopPOption.IsSet)
                if (createCompletionRequest.TopPOption.Value != null)
                    writer.WriteNumber("top_p", createCompletionRequest.TopPOption.Value!.Value);
                else
                    writer.WriteNull("top_p");

            if (createCompletionRequest.UserOption.IsSet)
                writer.WriteString("user", createCompletionRequest.User);
        }
    }
}
