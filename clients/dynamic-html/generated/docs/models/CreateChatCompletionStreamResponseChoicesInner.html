
<h2>CreateChatCompletionStreamResponseChoicesInner</h2>
<ul class="parameter">
  <li class="param-required-true">delta : ChatCompletionStreamResponseDelta
    <br/>
  </li>
</ul>
<ul class="parameter">
  <li class="param-required-false">logprobs : CreateChatCompletionResponse_choices_inner_logprobs
    <br/>
  </li>
</ul>
<ul class="parameter">
  <li class="param-required-true">finishUnderscorereason : String
    <br/>The reason the model stopped generating tokens. This will be &#x60;stop&#x60; if the model hit a natural stop point or a provided stop sequence, &#x60;length&#x60; if the maximum number of tokens specified in the request was reached, &#x60;content_filter&#x60; if content was omitted due to a flag from our content filters, &#x60;tool_calls&#x60; if the model called a tool, or &#x60;function_call&#x60; (deprecated) if the model called a function. 
      <dl class="param-enum">
        <dt>Enum:
            <dd>stop</dd>
            <dd>length</dd>
            <dd>tool_calls</dd>
            <dd>content_filter</dd>
            <dd>function_call</dd>
        </dt>
      </dl>
  </li>
</ul>
<ul class="parameter">
  <li class="param-required-true">index : Integer
    <br/>The index of the choice in the list of choices.
  </li>
</ul>
