note
 description:"[
		OpenAI API
 		APIs for sampling from and fine-tuning language models
  		The version of the OpenAPI document: 2.0.0
 	    Contact: blah+oapicf@cliffano.com

  	NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).

 		 Do not edit the class manually.
 	]"
	date: "$Date$"
	revision: "$Revision$"
	EIS:"Eiffel openapi generator", "src=https://openapi-generator.tech", "protocol=uri"
class CREATE_CHAT_COMPLETION_REQUEST




feature --Access

    model: detachable CREATE_CHAT_COMPLETION_REQUEST_MODEL
      
    messages: detachable LIST [CHAT_COMPLETION_REQUEST_MESSAGE]
      -- A list of messages comprising the conversation so far. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
    functions: detachable LIST [CHAT_COMPLETION_FUNCTIONS]
      -- A list of functions the model may generate JSON inputs for.
    function_call: detachable CREATE_CHAT_COMPLETION_REQUEST_FUNCTION_CALL
      
    temperature: REAL_32
      -- What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both. 
    top_p: REAL_32
      -- An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. 
    n: INTEGER_32
      -- How many chat completion choices to generate for each input message.
    stream: BOOLEAN
      -- If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb). 
    stop: detachable CREATE_CHAT_COMPLETION_REQUEST_STOP
      
    max_tokens: INTEGER_32
      -- The maximum number of [tokens](/tokenizer) to generate in the chat completion.  The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) for counting tokens. 
    presence_penalty: REAL_32
      -- Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details) 
    frequency_penalty: REAL_32
      -- Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details) 
    logit_bias: detachable ANY
      -- Modify the likelihood of specified tokens appearing in the completion.  Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
    user: detachable STRING_32
      -- A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). 

feature -- Change Element

    set_model (a_name: like model)
        -- Set 'model' with 'a_name'.
      do
        model := a_name
      ensure
        model_set: model = a_name
      end

    set_messages (a_name: like messages)
        -- Set 'messages' with 'a_name'.
      do
        messages := a_name
      ensure
        messages_set: messages = a_name
      end

    set_functions (a_name: like functions)
        -- Set 'functions' with 'a_name'.
      do
        functions := a_name
      ensure
        functions_set: functions = a_name
      end

    set_function_call (a_name: like function_call)
        -- Set 'function_call' with 'a_name'.
      do
        function_call := a_name
      ensure
        function_call_set: function_call = a_name
      end

    set_temperature (a_name: like temperature)
        -- Set 'temperature' with 'a_name'.
      do
        temperature := a_name
      ensure
        temperature_set: temperature = a_name
      end

    set_top_p (a_name: like top_p)
        -- Set 'top_p' with 'a_name'.
      do
        top_p := a_name
      ensure
        top_p_set: top_p = a_name
      end

    set_n (a_name: like n)
        -- Set 'n' with 'a_name'.
      do
        n := a_name
      ensure
        n_set: n = a_name
      end

    set_stream (a_name: like stream)
        -- Set 'stream' with 'a_name'.
      do
        stream := a_name
      ensure
        stream_set: stream = a_name
      end

    set_stop (a_name: like stop)
        -- Set 'stop' with 'a_name'.
      do
        stop := a_name
      ensure
        stop_set: stop = a_name
      end

    set_max_tokens (a_name: like max_tokens)
        -- Set 'max_tokens' with 'a_name'.
      do
        max_tokens := a_name
      ensure
        max_tokens_set: max_tokens = a_name
      end

    set_presence_penalty (a_name: like presence_penalty)
        -- Set 'presence_penalty' with 'a_name'.
      do
        presence_penalty := a_name
      ensure
        presence_penalty_set: presence_penalty = a_name
      end

    set_frequency_penalty (a_name: like frequency_penalty)
        -- Set 'frequency_penalty' with 'a_name'.
      do
        frequency_penalty := a_name
      ensure
        frequency_penalty_set: frequency_penalty = a_name
      end

    set_logit_bias (a_name: like logit_bias)
        -- Set 'logit_bias' with 'a_name'.
      do
        logit_bias := a_name
      ensure
        logit_bias_set: logit_bias = a_name
      end

    set_user (a_name: like user)
        -- Set 'user' with 'a_name'.
      do
        user := a_name
      ensure
        user_set: user = a_name
      end


 feature -- Status Report

    output: STRING
          -- <Precursor>
      do
        create Result.make_empty
        Result.append("%Nclass CREATE_CHAT_COMPLETION_REQUEST%N")
        if attached model as l_model then
          Result.append ("%Nmodel:")
          Result.append (l_model.out)
          Result.append ("%N")
        end
        if attached messages as l_messages then
          across l_messages as ic loop
            Result.append ("%N messages:")
            Result.append (ic.item.out)
            Result.append ("%N")
          end
        end
        if attached functions as l_functions then
          across l_functions as ic loop
            Result.append ("%N functions:")
            Result.append (ic.item.out)
            Result.append ("%N")
          end
        end
        if attached function_call as l_function_call then
          Result.append ("%Nfunction_call:")
          Result.append (l_function_call.out)
          Result.append ("%N")
        end
        if attached temperature as l_temperature then
          Result.append ("%Ntemperature:")
          Result.append (l_temperature.out)
          Result.append ("%N")
        end
        if attached top_p as l_top_p then
          Result.append ("%Ntop_p:")
          Result.append (l_top_p.out)
          Result.append ("%N")
        end
        if attached n as l_n then
          Result.append ("%Nn:")
          Result.append (l_n.out)
          Result.append ("%N")
        end
        if attached stream as l_stream then
          Result.append ("%Nstream:")
          Result.append (l_stream.out)
          Result.append ("%N")
        end
        if attached stop as l_stop then
          Result.append ("%Nstop:")
          Result.append (l_stop.out)
          Result.append ("%N")
        end
        if attached max_tokens as l_max_tokens then
          Result.append ("%Nmax_tokens:")
          Result.append (l_max_tokens.out)
          Result.append ("%N")
        end
        if attached presence_penalty as l_presence_penalty then
          Result.append ("%Npresence_penalty:")
          Result.append (l_presence_penalty.out)
          Result.append ("%N")
        end
        if attached frequency_penalty as l_frequency_penalty then
          Result.append ("%Nfrequency_penalty:")
          Result.append (l_frequency_penalty.out)
          Result.append ("%N")
        end
        if attached logit_bias as l_logit_bias then
          Result.append ("%Nlogit_bias:")
          Result.append (l_logit_bias.out)
          Result.append ("%N")
        end
        if attached user as l_user then
          Result.append ("%Nuser:")
          Result.append (l_user.out)
          Result.append ("%N")
        end
      end
end

