note
 description:"[
		OpenAI API
 		The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
  		The version of the OpenAPI document: 2.3.0
 	    Contact: blah+oapicf@cliffano.com

  	NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).

 		 Do not edit the class manually.
 	]"
	date: "$Date$"
	revision: "$Revision$"
	EIS:"Eiffel openapi generator", "src=https://openapi-generator.tech", "protocol=uri"
class CREATE_CHAT_COMPLETION_REQUEST




feature --Access

    messages: detachable LIST [CHAT_COMPLETION_REQUEST_MESSAGE]
      -- A list of messages comprising the conversation so far. Depending on the [model](/docs/models) you use, different message types (modalities) are supported, like [text](/docs/guides/text-generation), [images](/docs/guides/vision), and [audio](/docs/guides/audio). 
    model: detachable CREATE_CHAT_COMPLETION_REQUEST_MODEL
      
    store: BOOLEAN
      -- Whether or not to store the output of this chat completion request for  use in our [model distillation](/docs/guides/distillation) or [evals](/docs/guides/evals) products. 
    reasoning_effort: detachable STRING_32
      -- **o1 models only**   Constrains effort on reasoning for  [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response. 
    metadata: detachable STRING_TABLE [STRING_32]
      -- Developer-defined tags and values used for filtering completions in the [dashboard](https://platform.openai.com/chat-completions). 
    frequency_penalty: REAL_32
      -- Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. 
    logit_bias: detachable STRING_TABLE [INTEGER_32]
      -- Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
    logprobs: BOOLEAN
      -- Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. 
    top_logprobs: INTEGER_32
      -- An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used. 
    max_tokens: INTEGER_32
      -- The maximum number of [tokens](/tokenizer) that can be generated in the chat completion. This value can be used to control [costs](https://openai.com/api/pricing/) for text generated via API.  This value is now deprecated in favor of `max_completion_tokens`, and is not compatible with [o1 series models](/docs/guides/reasoning). 
    max_completion_tokens: INTEGER_32
      -- An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning). 
    n: INTEGER_32
      -- How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
    modalities: detachable LIST [STRING_32]
      -- Output types that you would like the model to generate for this request. Most models are capable of generating text, which is the default:  `[\"text\"]`  The `gpt-4o-audio-preview` model can also be used to [generate audio](/docs/guides/audio). To request that this model generate both text and audio responses, you can use:  `[\"text\", \"audio\"]` 
    prediction: detachable PREDICTION_CONTENT
      
    audio: detachable CREATE_CHAT_COMPLETION_REQUEST_AUDIO
      
    presence_penalty: REAL_32
      -- Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. 
    response_format: detachable CREATE_CHAT_COMPLETION_REQUEST_RESPONSE_FORMAT
      
    seed: INTEGER_32
      -- This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. 
    service_tier: detachable STRING_32
      -- Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:    - If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.   - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - When not set, the default behavior is 'auto'.    When this parameter is set, the response body will include the `service_tier` utilized. 
    stop: detachable CREATE_CHAT_COMPLETION_REQUEST_STOP
      
    stream: BOOLEAN
      -- If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
    stream_options: detachable CHAT_COMPLETION_STREAM_OPTIONS
      
    temperature: REAL_32
      -- What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. 
    top_p: REAL_32
      -- An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. 
    tools: detachable LIST [CHAT_COMPLETION_TOOL]
      -- A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. 
    tool_choice: detachable CHAT_COMPLETION_TOOL_CHOICE_OPTION
      
    parallel_tool_calls: BOOLEAN
      -- Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.
    user: detachable STRING_32
      -- A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). 
    function_call: detachable CREATE_CHAT_COMPLETION_REQUEST_FUNCTION_CALL
      
    functions: detachable LIST [CHAT_COMPLETION_FUNCTIONS]
      -- Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for. 

feature -- Change Element

    set_messages (a_name: like messages)
        -- Set 'messages' with 'a_name'.
      do
        messages := a_name
      ensure
        messages_set: messages = a_name
      end

    set_model (a_name: like model)
        -- Set 'model' with 'a_name'.
      do
        model := a_name
      ensure
        model_set: model = a_name
      end

    set_store (a_name: like store)
        -- Set 'store' with 'a_name'.
      do
        store := a_name
      ensure
        store_set: store = a_name
      end

    set_reasoning_effort (a_name: like reasoning_effort)
        -- Set 'reasoning_effort' with 'a_name'.
      do
        reasoning_effort := a_name
      ensure
        reasoning_effort_set: reasoning_effort = a_name
      end

    set_metadata (a_name: like metadata)
        -- Set 'metadata' with 'a_name'.
      do
        metadata := a_name
      ensure
        metadata_set: metadata = a_name
      end

    set_frequency_penalty (a_name: like frequency_penalty)
        -- Set 'frequency_penalty' with 'a_name'.
      do
        frequency_penalty := a_name
      ensure
        frequency_penalty_set: frequency_penalty = a_name
      end

    set_logit_bias (a_name: like logit_bias)
        -- Set 'logit_bias' with 'a_name'.
      do
        logit_bias := a_name
      ensure
        logit_bias_set: logit_bias = a_name
      end

    set_logprobs (a_name: like logprobs)
        -- Set 'logprobs' with 'a_name'.
      do
        logprobs := a_name
      ensure
        logprobs_set: logprobs = a_name
      end

    set_top_logprobs (a_name: like top_logprobs)
        -- Set 'top_logprobs' with 'a_name'.
      do
        top_logprobs := a_name
      ensure
        top_logprobs_set: top_logprobs = a_name
      end

    set_max_tokens (a_name: like max_tokens)
        -- Set 'max_tokens' with 'a_name'.
      do
        max_tokens := a_name
      ensure
        max_tokens_set: max_tokens = a_name
      end

    set_max_completion_tokens (a_name: like max_completion_tokens)
        -- Set 'max_completion_tokens' with 'a_name'.
      do
        max_completion_tokens := a_name
      ensure
        max_completion_tokens_set: max_completion_tokens = a_name
      end

    set_n (a_name: like n)
        -- Set 'n' with 'a_name'.
      do
        n := a_name
      ensure
        n_set: n = a_name
      end

    set_modalities (a_name: like modalities)
        -- Set 'modalities' with 'a_name'.
      do
        modalities := a_name
      ensure
        modalities_set: modalities = a_name
      end

    set_prediction (a_name: like prediction)
        -- Set 'prediction' with 'a_name'.
      do
        prediction := a_name
      ensure
        prediction_set: prediction = a_name
      end

    set_audio (a_name: like audio)
        -- Set 'audio' with 'a_name'.
      do
        audio := a_name
      ensure
        audio_set: audio = a_name
      end

    set_presence_penalty (a_name: like presence_penalty)
        -- Set 'presence_penalty' with 'a_name'.
      do
        presence_penalty := a_name
      ensure
        presence_penalty_set: presence_penalty = a_name
      end

    set_response_format (a_name: like response_format)
        -- Set 'response_format' with 'a_name'.
      do
        response_format := a_name
      ensure
        response_format_set: response_format = a_name
      end

    set_seed (a_name: like seed)
        -- Set 'seed' with 'a_name'.
      do
        seed := a_name
      ensure
        seed_set: seed = a_name
      end

    set_service_tier (a_name: like service_tier)
        -- Set 'service_tier' with 'a_name'.
      do
        service_tier := a_name
      ensure
        service_tier_set: service_tier = a_name
      end

    set_stop (a_name: like stop)
        -- Set 'stop' with 'a_name'.
      do
        stop := a_name
      ensure
        stop_set: stop = a_name
      end

    set_stream (a_name: like stream)
        -- Set 'stream' with 'a_name'.
      do
        stream := a_name
      ensure
        stream_set: stream = a_name
      end

    set_stream_options (a_name: like stream_options)
        -- Set 'stream_options' with 'a_name'.
      do
        stream_options := a_name
      ensure
        stream_options_set: stream_options = a_name
      end

    set_temperature (a_name: like temperature)
        -- Set 'temperature' with 'a_name'.
      do
        temperature := a_name
      ensure
        temperature_set: temperature = a_name
      end

    set_top_p (a_name: like top_p)
        -- Set 'top_p' with 'a_name'.
      do
        top_p := a_name
      ensure
        top_p_set: top_p = a_name
      end

    set_tools (a_name: like tools)
        -- Set 'tools' with 'a_name'.
      do
        tools := a_name
      ensure
        tools_set: tools = a_name
      end

    set_tool_choice (a_name: like tool_choice)
        -- Set 'tool_choice' with 'a_name'.
      do
        tool_choice := a_name
      ensure
        tool_choice_set: tool_choice = a_name
      end

    set_parallel_tool_calls (a_name: like parallel_tool_calls)
        -- Set 'parallel_tool_calls' with 'a_name'.
      do
        parallel_tool_calls := a_name
      ensure
        parallel_tool_calls_set: parallel_tool_calls = a_name
      end

    set_user (a_name: like user)
        -- Set 'user' with 'a_name'.
      do
        user := a_name
      ensure
        user_set: user = a_name
      end

    set_function_call (a_name: like function_call)
        -- Set 'function_call' with 'a_name'.
      do
        function_call := a_name
      ensure
        function_call_set: function_call = a_name
      end

    set_functions (a_name: like functions)
        -- Set 'functions' with 'a_name'.
      do
        functions := a_name
      ensure
        functions_set: functions = a_name
      end


 feature -- Status Report

    output: STRING
          -- <Precursor>
      do
        create Result.make_empty
        Result.append("%Nclass CREATE_CHAT_COMPLETION_REQUEST%N")
        if attached messages as l_messages then
          across l_messages as ic loop
            Result.append ("%N messages:")
            Result.append (ic.item.out)
            Result.append ("%N")
          end
        end
        if attached model as l_model then
          Result.append ("%Nmodel:")
          Result.append (l_model.out)
          Result.append ("%N")
        end
        if attached store as l_store then
          Result.append ("%Nstore:")
          Result.append (l_store.out)
          Result.append ("%N")
        end
        if attached reasoning_effort as l_reasoning_effort then
          Result.append ("%Nreasoning_effort:")
          Result.append (l_reasoning_effort.out)
          Result.append ("%N")
        end
        if attached metadata as l_metadata then
          Result.append ("%Nmetadata:")
          across l_metadata as ic loop
            Result.append ("%N")
            Result.append ("key:")
            Result.append (ic.key.out)
            Result.append (" - ")
            Result.append ("val:")
            Result.append (ic.item.out)
            Result.append ("%N")
          end
        end
        if attached frequency_penalty as l_frequency_penalty then
          Result.append ("%Nfrequency_penalty:")
          Result.append (l_frequency_penalty.out)
          Result.append ("%N")
        end
        if attached logit_bias as l_logit_bias then
          Result.append ("%Nlogit_bias:")
          across l_logit_bias as ic loop
            Result.append ("%N")
            Result.append ("key:")
            Result.append (ic.key.out)
            Result.append (" - ")
            Result.append ("val:")
            Result.append (ic.item.out)
            Result.append ("%N")
          end
        end
        if attached logprobs as l_logprobs then
          Result.append ("%Nlogprobs:")
          Result.append (l_logprobs.out)
          Result.append ("%N")
        end
        if attached top_logprobs as l_top_logprobs then
          Result.append ("%Ntop_logprobs:")
          Result.append (l_top_logprobs.out)
          Result.append ("%N")
        end
        if attached max_tokens as l_max_tokens then
          Result.append ("%Nmax_tokens:")
          Result.append (l_max_tokens.out)
          Result.append ("%N")
        end
        if attached max_completion_tokens as l_max_completion_tokens then
          Result.append ("%Nmax_completion_tokens:")
          Result.append (l_max_completion_tokens.out)
          Result.append ("%N")
        end
        if attached n as l_n then
          Result.append ("%Nn:")
          Result.append (l_n.out)
          Result.append ("%N")
        end
        if attached modalities as l_modalities then
          across l_modalities as ic loop
            Result.append ("%N modalities:")
            Result.append (ic.item.out)
            Result.append ("%N")
          end
        end
        if attached prediction as l_prediction then
          Result.append ("%Nprediction:")
          Result.append (l_prediction.out)
          Result.append ("%N")
        end
        if attached audio as l_audio then
          Result.append ("%Naudio:")
          Result.append (l_audio.out)
          Result.append ("%N")
        end
        if attached presence_penalty as l_presence_penalty then
          Result.append ("%Npresence_penalty:")
          Result.append (l_presence_penalty.out)
          Result.append ("%N")
        end
        if attached response_format as l_response_format then
          Result.append ("%Nresponse_format:")
          Result.append (l_response_format.out)
          Result.append ("%N")
        end
        if attached seed as l_seed then
          Result.append ("%Nseed:")
          Result.append (l_seed.out)
          Result.append ("%N")
        end
        if attached service_tier as l_service_tier then
          Result.append ("%Nservice_tier:")
          Result.append (l_service_tier.out)
          Result.append ("%N")
        end
        if attached stop as l_stop then
          Result.append ("%Nstop:")
          Result.append (l_stop.out)
          Result.append ("%N")
        end
        if attached stream as l_stream then
          Result.append ("%Nstream:")
          Result.append (l_stream.out)
          Result.append ("%N")
        end
        if attached stream_options as l_stream_options then
          Result.append ("%Nstream_options:")
          Result.append (l_stream_options.out)
          Result.append ("%N")
        end
        if attached temperature as l_temperature then
          Result.append ("%Ntemperature:")
          Result.append (l_temperature.out)
          Result.append ("%N")
        end
        if attached top_p as l_top_p then
          Result.append ("%Ntop_p:")
          Result.append (l_top_p.out)
          Result.append ("%N")
        end
        if attached tools as l_tools then
          across l_tools as ic loop
            Result.append ("%N tools:")
            Result.append (ic.item.out)
            Result.append ("%N")
          end
        end
        if attached tool_choice as l_tool_choice then
          Result.append ("%Ntool_choice:")
          Result.append (l_tool_choice.out)
          Result.append ("%N")
        end
        if attached parallel_tool_calls as l_parallel_tool_calls then
          Result.append ("%Nparallel_tool_calls:")
          Result.append (l_parallel_tool_calls.out)
          Result.append ("%N")
        end
        if attached user as l_user then
          Result.append ("%Nuser:")
          Result.append (l_user.out)
          Result.append ("%N")
        end
        if attached function_call as l_function_call then
          Result.append ("%Nfunction_call:")
          Result.append (l_function_call.out)
          Result.append ("%N")
        end
        if attached functions as l_functions then
          across l_functions as ic loop
            Result.append ("%N functions:")
            Result.append (ic.item.out)
            Result.append ("%N")
          end
        end
      end
end

