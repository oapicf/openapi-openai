note
 description:"[
		OpenAI API
 		The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
  		The version of the OpenAPI document: 2.0.0
 	    Contact: blah+oapicf@cliffano.com

  	NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).

 		 Do not edit the class manually.
 	]"
	date: "$Date$"
	revision: "$Revision$"
	EIS:"Eiffel openapi generator", "src=https://openapi-generator.tech", "protocol=uri"
class CREATE_COMPLETION_REQUEST




feature --Access

    model: detachable CREATE_COMPLETION_REQUEST_MODEL
      
    prompt: detachable CREATE_COMPLETION_REQUEST_PROMPT
      
    best_of: INTEGER_32
      -- Generates `best_of` completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.  When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return â€“ `best_of` must be greater than `n`.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`. 
    echo: BOOLEAN
      -- Echo back the prompt in addition to the completion 
    frequency_penalty: REAL_32
      -- Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
    logit_bias: detachable STRING_TABLE [INTEGER_32]
      -- Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.  As an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token from being generated. 
    logprobs: INTEGER_32
      -- Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.  The maximum value for `logprobs` is 5. 
    max_tokens: INTEGER_32
      -- The maximum number of [tokens](/tokenizer) that can be generated in the completion.  The token count of your prompt plus `max_tokens` cannot exceed the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. 
    n: INTEGER_32
      -- How many completions to generate for each prompt.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`. 
    presence_penalty: REAL_32
      -- Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
    seed: INTEGER_32
      -- If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.  Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. 
    stop: detachable CREATE_COMPLETION_REQUEST_STOP
      
    stream: BOOLEAN
      -- Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
    suffix: detachable STRING_32
      -- The suffix that comes after a completion of inserted text.  This parameter is only supported for `gpt-3.5-turbo-instruct`. 
    temperature: REAL_32
      -- What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both. 
    top_p: REAL_32
      -- An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. 
    user: detachable STRING_32
      -- A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). 

feature -- Change Element

    set_model (a_name: like model)
        -- Set 'model' with 'a_name'.
      do
        model := a_name
      ensure
        model_set: model = a_name
      end

    set_prompt (a_name: like prompt)
        -- Set 'prompt' with 'a_name'.
      do
        prompt := a_name
      ensure
        prompt_set: prompt = a_name
      end

    set_best_of (a_name: like best_of)
        -- Set 'best_of' with 'a_name'.
      do
        best_of := a_name
      ensure
        best_of_set: best_of = a_name
      end

    set_echo (a_name: like echo)
        -- Set 'echo' with 'a_name'.
      do
        echo := a_name
      ensure
        echo_set: echo = a_name
      end

    set_frequency_penalty (a_name: like frequency_penalty)
        -- Set 'frequency_penalty' with 'a_name'.
      do
        frequency_penalty := a_name
      ensure
        frequency_penalty_set: frequency_penalty = a_name
      end

    set_logit_bias (a_name: like logit_bias)
        -- Set 'logit_bias' with 'a_name'.
      do
        logit_bias := a_name
      ensure
        logit_bias_set: logit_bias = a_name
      end

    set_logprobs (a_name: like logprobs)
        -- Set 'logprobs' with 'a_name'.
      do
        logprobs := a_name
      ensure
        logprobs_set: logprobs = a_name
      end

    set_max_tokens (a_name: like max_tokens)
        -- Set 'max_tokens' with 'a_name'.
      do
        max_tokens := a_name
      ensure
        max_tokens_set: max_tokens = a_name
      end

    set_n (a_name: like n)
        -- Set 'n' with 'a_name'.
      do
        n := a_name
      ensure
        n_set: n = a_name
      end

    set_presence_penalty (a_name: like presence_penalty)
        -- Set 'presence_penalty' with 'a_name'.
      do
        presence_penalty := a_name
      ensure
        presence_penalty_set: presence_penalty = a_name
      end

    set_seed (a_name: like seed)
        -- Set 'seed' with 'a_name'.
      do
        seed := a_name
      ensure
        seed_set: seed = a_name
      end

    set_stop (a_name: like stop)
        -- Set 'stop' with 'a_name'.
      do
        stop := a_name
      ensure
        stop_set: stop = a_name
      end

    set_stream (a_name: like stream)
        -- Set 'stream' with 'a_name'.
      do
        stream := a_name
      ensure
        stream_set: stream = a_name
      end

    set_suffix (a_name: like suffix)
        -- Set 'suffix' with 'a_name'.
      do
        suffix := a_name
      ensure
        suffix_set: suffix = a_name
      end

    set_temperature (a_name: like temperature)
        -- Set 'temperature' with 'a_name'.
      do
        temperature := a_name
      ensure
        temperature_set: temperature = a_name
      end

    set_top_p (a_name: like top_p)
        -- Set 'top_p' with 'a_name'.
      do
        top_p := a_name
      ensure
        top_p_set: top_p = a_name
      end

    set_user (a_name: like user)
        -- Set 'user' with 'a_name'.
      do
        user := a_name
      ensure
        user_set: user = a_name
      end


 feature -- Status Report

    output: STRING
          -- <Precursor>
      do
        create Result.make_empty
        Result.append("%Nclass CREATE_COMPLETION_REQUEST%N")
        if attached model as l_model then
          Result.append ("%Nmodel:")
          Result.append (l_model.out)
          Result.append ("%N")
        end
        if attached prompt as l_prompt then
          Result.append ("%Nprompt:")
          Result.append (l_prompt.out)
          Result.append ("%N")
        end
        if attached best_of as l_best_of then
          Result.append ("%Nbest_of:")
          Result.append (l_best_of.out)
          Result.append ("%N")
        end
        if attached echo as l_echo then
          Result.append ("%Necho:")
          Result.append (l_echo.out)
          Result.append ("%N")
        end
        if attached frequency_penalty as l_frequency_penalty then
          Result.append ("%Nfrequency_penalty:")
          Result.append (l_frequency_penalty.out)
          Result.append ("%N")
        end
        if attached logit_bias as l_logit_bias then
          Result.append ("%Nlogit_bias:")
          across l_logit_bias as ic loop
            Result.append ("%N")
            Result.append ("key:")
            Result.append (ic.key.out)
            Result.append (" - ")
            Result.append ("val:")
            Result.append (ic.item.out)
            Result.append ("%N")
          end
        end
        if attached logprobs as l_logprobs then
          Result.append ("%Nlogprobs:")
          Result.append (l_logprobs.out)
          Result.append ("%N")
        end
        if attached max_tokens as l_max_tokens then
          Result.append ("%Nmax_tokens:")
          Result.append (l_max_tokens.out)
          Result.append ("%N")
        end
        if attached n as l_n then
          Result.append ("%Nn:")
          Result.append (l_n.out)
          Result.append ("%N")
        end
        if attached presence_penalty as l_presence_penalty then
          Result.append ("%Npresence_penalty:")
          Result.append (l_presence_penalty.out)
          Result.append ("%N")
        end
        if attached seed as l_seed then
          Result.append ("%Nseed:")
          Result.append (l_seed.out)
          Result.append ("%N")
        end
        if attached stop as l_stop then
          Result.append ("%Nstop:")
          Result.append (l_stop.out)
          Result.append ("%N")
        end
        if attached stream as l_stream then
          Result.append ("%Nstream:")
          Result.append (l_stream.out)
          Result.append ("%N")
        end
        if attached suffix as l_suffix then
          Result.append ("%Nsuffix:")
          Result.append (l_suffix.out)
          Result.append ("%N")
        end
        if attached temperature as l_temperature then
          Result.append ("%Ntemperature:")
          Result.append (l_temperature.out)
          Result.append ("%N")
        end
        if attached top_p as l_top_p then
          Result.append ("%Ntop_p:")
          Result.append (l_top_p.out)
          Result.append ("%N")
        end
        if attached user as l_user then
          Result.append ("%Nuser:")
          Result.append (l_user.out)
          Result.append ("%N")
        end
      end
end

