-module(openapi_create_chat_completion_request).

-export([encode/1]).

-export_type([openapi_create_chat_completion_request/0]).

-type openapi_create_chat_completion_request() ::
    #{ 'messages' := list(),
       'model' := openapi_create_chat_completion_request_model:openapi_create_chat_completion_request_model(),
       'store' => boolean(),
       'reasoning_effort' => binary(),
       'metadata' => maps:map(),
       'frequency_penalty' => integer(),
       'logit_bias' => maps:map(),
       'logprobs' => boolean(),
       'top_logprobs' => integer(),
       'max_tokens' => integer(),
       'max_completion_tokens' => integer(),
       'n' => integer(),
       'modalities' => list(),
       'prediction' => openapi_prediction_content:openapi_prediction_content(),
       'audio' => openapi_create_chat_completion_request_audio:openapi_create_chat_completion_request_audio(),
       'presence_penalty' => integer(),
       'response_format' => openapi_create_chat_completion_request_response_format:openapi_create_chat_completion_request_response_format(),
       'seed' => integer(),
       'service_tier' => binary(),
       'stop' => openapi_create_chat_completion_request_stop:openapi_create_chat_completion_request_stop(),
       'stream' => boolean(),
       'stream_options' => openapi_chat_completion_stream_options:openapi_chat_completion_stream_options(),
       'temperature' => integer(),
       'top_p' => integer(),
       'tools' => list(),
       'tool_choice' => openapi_chat_completion_tool_choice_option:openapi_chat_completion_tool_choice_option(),
       'parallel_tool_calls' => boolean(),
       'user' => binary(),
       'function_call' => openapi_create_chat_completion_request_function_call:openapi_create_chat_completion_request_function_call(),
       'functions' => list()
     }.

encode(#{ 'messages' := Messages,
          'model' := Model,
          'store' := Store,
          'reasoning_effort' := ReasoningEffort,
          'metadata' := Metadata,
          'frequency_penalty' := FrequencyPenalty,
          'logit_bias' := LogitBias,
          'logprobs' := Logprobs,
          'top_logprobs' := TopLogprobs,
          'max_tokens' := MaxTokens,
          'max_completion_tokens' := MaxCompletionTokens,
          'n' := N,
          'modalities' := Modalities,
          'prediction' := Prediction,
          'audio' := Audio,
          'presence_penalty' := PresencePenalty,
          'response_format' := ResponseFormat,
          'seed' := Seed,
          'service_tier' := ServiceTier,
          'stop' := Stop,
          'stream' := Stream,
          'stream_options' := StreamOptions,
          'temperature' := Temperature,
          'top_p' := TopP,
          'tools' := Tools,
          'tool_choice' := ToolChoice,
          'parallel_tool_calls' := ParallelToolCalls,
          'user' := User,
          'function_call' := FunctionCall,
          'functions' := Functions
        }) ->
    #{ 'messages' => Messages,
       'model' => Model,
       'store' => Store,
       'reasoning_effort' => ReasoningEffort,
       'metadata' => Metadata,
       'frequency_penalty' => FrequencyPenalty,
       'logit_bias' => LogitBias,
       'logprobs' => Logprobs,
       'top_logprobs' => TopLogprobs,
       'max_tokens' => MaxTokens,
       'max_completion_tokens' => MaxCompletionTokens,
       'n' => N,
       'modalities' => Modalities,
       'prediction' => Prediction,
       'audio' => Audio,
       'presence_penalty' => PresencePenalty,
       'response_format' => ResponseFormat,
       'seed' => Seed,
       'service_tier' => ServiceTier,
       'stop' => Stop,
       'stream' => Stream,
       'stream_options' => StreamOptions,
       'temperature' => Temperature,
       'top_p' => TopP,
       'tools' => Tools,
       'tool_choice' => ToolChoice,
       'parallel_tool_calls' => ParallelToolCalls,
       'user' => User,
       'function_call' => FunctionCall,
       'functions' => Functions
     }.
