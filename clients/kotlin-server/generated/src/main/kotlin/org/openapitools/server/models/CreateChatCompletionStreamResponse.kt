/**
* OpenAI API
* The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
*
* The version of the OpenAPI document: 2.3.0
* Contact: blah+oapicf@cliffano.com
*
* NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
* https://openapi-generator.tech
* Do not edit the class manually.
*/
package org.openapitools.server.models

import org.openapitools.server.models.CreateChatCompletionStreamResponseChoicesInner
import org.openapitools.server.models.CreateChatCompletionStreamResponseUsage

import kotlinx.serialization.Serializable
/**
 * Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
 * @param id A unique identifier for the chat completion. Each chunk has the same ID.
 * @param choices A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the last chunk if you set `stream_options: {\"include_usage\": true}`. 
 * @param created The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
 * @param model The model to generate the completion.
 * @param `object` The object type, which is always `chat.completion.chunk`.
 * @param serviceTier The service tier used for processing the request. This field is only included if the `service_tier` parameter is specified in the request.
 * @param systemFingerprint This fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism. 
 * @param usage 
 */
@Serializable
data class CreateChatCompletionStreamResponse(
    /* A unique identifier for the chat completion. Each chunk has the same ID. */
    val id: kotlin.String,
    /* A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the last chunk if you set `stream_options: {\"include_usage\": true}`.  */
    val choices: kotlin.collections.List<CreateChatCompletionStreamResponseChoicesInner>,
    /* The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp. */
    val created: kotlin.Int,
    /* The model to generate the completion. */
    val model: kotlin.String,
    /* The object type, which is always `chat.completion.chunk`. */
    val `object`: CreateChatCompletionStreamResponse.`Object`,
    /* The service tier used for processing the request. This field is only included if the `service_tier` parameter is specified in the request. */
    val serviceTier: CreateChatCompletionStreamResponse.ServiceTier? = null,
    /* This fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.  */
    val systemFingerprint: kotlin.String? = null,
    val usage: CreateChatCompletionStreamResponseUsage? = null
)
{
    /**
    * The object type, which is always `chat.completion.chunk`.
    * Values: chatPeriodCompletionPeriodChunk
    */
    enum class `Object`(val value: kotlin.String){
        chatPeriodCompletionPeriodChunk("chat.completion.chunk");
    }
    /**
    * The service tier used for processing the request. This field is only included if the `service_tier` parameter is specified in the request.
    * Values: scale,default
    */
    enum class ServiceTier(val value: kotlin.String){
        scale("scale"),
        default("default");
    }
}

