/**
* OpenAI API
* The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
*
* The version of the OpenAPI document: 2.3.0
* Contact: blah+oapicf@cliffano.com
*
* NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
* https://openapi-generator.tech
* Do not edit the class manually.
*/
package org.openapitools.server.api.model

import org.openapitools.server.api.model.ChatCompletionResponseMessage
import org.openapitools.server.api.model.CreateChatCompletionResponseChoicesInnerLogprobs

        
import com.google.gson.annotations.SerializedName
import com.fasterxml.jackson.annotation.JsonIgnoreProperties
import com.fasterxml.jackson.annotation.JsonInclude
/**
 * 
 * @param finishReason The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters, `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function. 
 * @param index The index of the choice in the list of choices.
 * @param message 
 * @param logprobs 
 */
@JsonInclude(JsonInclude.Include.NON_NULL)
@JsonIgnoreProperties(ignoreUnknown = true)
data class CreateChatCompletionResponseChoicesInner (
    /* The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters, `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.  */
    @SerializedName("finishReason") private val _finishReason: CreateChatCompletionResponseChoicesInner.FinishReason?,
    /* The index of the choice in the list of choices. */
    @SerializedName("index") private val _index: kotlin.Int?,
    @SerializedName("message") private val _message: ChatCompletionResponseMessage?,
    @SerializedName("logprobs") private val _logprobs: CreateChatCompletionResponseChoicesInnerLogprobs?
) {

    /**
    * The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters, `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function. 
    * Values: stop,length,tool_calls,content_filter,function_call
    */
    enum class FinishReason(val value: kotlin.String){
    
        stop("stop"),
    
        length("length"),
    
        tool_calls("tool_calls"),
    
        content_filter("content_filter"),
    
        function_call("function_call");
    
    }

        val finishReason get() = _finishReason ?: throw IllegalArgumentException("finishReason is required")
                    
        val index get() = _index ?: throw IllegalArgumentException("index is required")
                    
        val message get() = _message ?: throw IllegalArgumentException("message is required")
                    
        val logprobs get() = _logprobs ?: throw IllegalArgumentException("logprobs is required")
                    
}

