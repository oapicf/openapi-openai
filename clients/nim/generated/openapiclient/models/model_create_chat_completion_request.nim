#
# OpenAI API
# 
# The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
# The version of the OpenAPI document: 2.0.0
# Contact: blah+oapicf@cliffano.com
# Generated by: https://openapi-generator.tech
#

import json
import tables
import marshal
import options

import model_chat_completion_functions
import model_chat_completion_request_message
import model_chat_completion_tool
import model_chat_completion_tool_choice_option
import model_create_chat_completion_request_function_call
import model_create_chat_completion_request_model
import model_create_chat_completion_request_response_format
import model_create_chat_completion_request_stop

type CreateChatCompletionRequest* = object
  ## 
  messages*: seq[ChatCompletionRequestMessage] ## A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
  model*: CreateChatCompletionRequest_model
  frequencyPenalty*: Option[float] ## Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
  logitBias*: Option[Table[string, int]] ## Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
  logprobs*: Option[bool] ## Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.
  topLogprobs*: Option[int] ## An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
  maxTokens*: Option[int] ## The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.  The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. 
  n*: Option[int] ## How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
  presencePenalty*: Option[float] ## Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
  responseFormat*: Option[CreateChatCompletionRequest_response_format]
  seed*: Option[int] ## This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. 
  stop*: Option[CreateChatCompletionRequest_stop]
  stream*: Option[bool] ## If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
  temperature*: Option[float] ## What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both. 
  topP*: Option[float] ## An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. 
  tools*: Option[seq[ChatCompletionTool]] ## A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. 
  toolChoice*: Option[ChatCompletionToolChoiceOption]
  user*: Option[string] ## A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). 
  functionCall*: Option[CreateChatCompletionRequest_function_call]
  functions*: Option[seq[ChatCompletionFunctions]] ## Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for. 


# Custom JSON deserialization for CreateChatCompletionRequest with custom field names
proc to*(node: JsonNode, T: typedesc[CreateChatCompletionRequest]): CreateChatCompletionRequest =
  result = CreateChatCompletionRequest()
  if node.kind == JObject:
    if node.hasKey("messages"):
      # Array of types with custom JSON - manually iterate and deserialize
      let arrayNode = node["messages"]
      if arrayNode.kind == JArray:
        result.messages = @[]
        for item in arrayNode.items:
          result.messages.add(to(item, ChatCompletionRequestMessage))
    if node.hasKey("model"):
      result.model = to(node["model"], CreateChatCompletionRequest_model)
    if node.hasKey("frequency_penalty") and node["frequency_penalty"].kind != JNull:
      result.frequencyPenalty = some(to(node["frequency_penalty"], typeof(result.frequencyPenalty.get())))
    if node.hasKey("logit_bias") and node["logit_bias"].kind != JNull:
      result.logitBias = some(to(node["logit_bias"], typeof(result.logitBias.get())))
    if node.hasKey("logprobs") and node["logprobs"].kind != JNull:
      result.logprobs = some(to(node["logprobs"], typeof(result.logprobs.get())))
    if node.hasKey("top_logprobs") and node["top_logprobs"].kind != JNull:
      result.topLogprobs = some(to(node["top_logprobs"], typeof(result.topLogprobs.get())))
    if node.hasKey("max_tokens") and node["max_tokens"].kind != JNull:
      result.maxTokens = some(to(node["max_tokens"], typeof(result.maxTokens.get())))
    if node.hasKey("n") and node["n"].kind != JNull:
      result.n = some(to(node["n"], typeof(result.n.get())))
    if node.hasKey("presence_penalty") and node["presence_penalty"].kind != JNull:
      result.presencePenalty = some(to(node["presence_penalty"], typeof(result.presencePenalty.get())))
    if node.hasKey("response_format") and node["response_format"].kind != JNull:
      result.responseFormat = some(to(node["response_format"], typeof(result.responseFormat.get())))
    if node.hasKey("seed") and node["seed"].kind != JNull:
      result.seed = some(to(node["seed"], typeof(result.seed.get())))
    if node.hasKey("stop") and node["stop"].kind != JNull:
      result.stop = some(to(node["stop"], typeof(result.stop.get())))
    if node.hasKey("stream") and node["stream"].kind != JNull:
      result.stream = some(to(node["stream"], typeof(result.stream.get())))
    if node.hasKey("temperature") and node["temperature"].kind != JNull:
      result.temperature = some(to(node["temperature"], typeof(result.temperature.get())))
    if node.hasKey("top_p") and node["top_p"].kind != JNull:
      result.topP = some(to(node["top_p"], typeof(result.topP.get())))
    if node.hasKey("tools") and node["tools"].kind != JNull:
      result.tools = some(to(node["tools"], typeof(result.tools.get())))
    if node.hasKey("tool_choice") and node["tool_choice"].kind != JNull:
      result.toolChoice = some(to(node["tool_choice"], typeof(result.toolChoice.get())))
    if node.hasKey("user") and node["user"].kind != JNull:
      result.user = some(to(node["user"], typeof(result.user.get())))
    if node.hasKey("function_call") and node["function_call"].kind != JNull:
      result.functionCall = some(to(node["function_call"], typeof(result.functionCall.get())))
    if node.hasKey("functions") and node["functions"].kind != JNull:
      result.functions = some(to(node["functions"], typeof(result.functions.get())))

# Custom JSON serialization for CreateChatCompletionRequest with custom field names
proc `%`*(obj: CreateChatCompletionRequest): JsonNode =
  result = newJObject()
  result["messages"] = %obj.messages
  result["model"] = %obj.model
  if obj.frequencyPenalty.isSome():
    result["frequency_penalty"] = %obj.frequencyPenalty.get()
  if obj.logitBias.isSome():
    result["logit_bias"] = %obj.logitBias.get()
  if obj.logprobs.isSome():
    result["logprobs"] = %obj.logprobs.get()
  if obj.topLogprobs.isSome():
    result["top_logprobs"] = %obj.topLogprobs.get()
  if obj.maxTokens.isSome():
    result["max_tokens"] = %obj.maxTokens.get()
  if obj.n.isSome():
    result["n"] = %obj.n.get()
  if obj.presencePenalty.isSome():
    result["presence_penalty"] = %obj.presencePenalty.get()
  if obj.responseFormat.isSome():
    result["response_format"] = %obj.responseFormat.get()
  if obj.seed.isSome():
    result["seed"] = %obj.seed.get()
  if obj.stop.isSome():
    result["stop"] = %obj.stop.get()
  if obj.stream.isSome():
    result["stream"] = %obj.stream.get()
  if obj.temperature.isSome():
    result["temperature"] = %obj.temperature.get()
  if obj.topP.isSome():
    result["top_p"] = %obj.topP.get()
  if obj.tools.isSome():
    result["tools"] = %obj.tools.get()
  if obj.toolChoice.isSome():
    result["tool_choice"] = %obj.toolChoice.get()
  if obj.user.isSome():
    result["user"] = %obj.user.get()
  if obj.functionCall.isSome():
    result["function_call"] = %obj.functionCall.get()
  if obj.functions.isSome():
    result["functions"] = %obj.functions.get()

