#
# OpenAI API
# 
# The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
# The version of the OpenAPI document: 2.3.0
# Contact: blah+oapicf@cliffano.com
# Generated by: https://openapi-generator.tech
#

import json
import tables
import marshal
import options

import model_chat_completion_functions
import model_chat_completion_request_message
import model_chat_completion_stream_options
import model_chat_completion_tool
import model_chat_completion_tool_choice_option
import model_create_chat_completion_request_audio
import model_create_chat_completion_request_function_call
import model_create_chat_completion_request_model
import model_create_chat_completion_request_response_format
import model_create_chat_completion_request_stop
import model_prediction_content

type ReasoningEffort* {.pure.} = enum
  Low
  Medium
  High

type Modalities* {.pure.} = enum
  Text
  Audio

type ServiceTier* {.pure.} = enum
  Auto
  Default

type CreateChatCompletionRequest* = object
  ## 
  messages*: seq[ChatCompletionRequestMessage] ## A list of messages comprising the conversation so far. Depending on the [model](/docs/models) you use, different message types (modalities) are supported, like [text](/docs/guides/text-generation), [images](/docs/guides/vision), and [audio](/docs/guides/audio). 
  model*: CreateChatCompletionRequest_model
  store*: Option[bool] ## Whether or not to store the output of this chat completion request for  use in our [model distillation](/docs/guides/distillation) or [evals](/docs/guides/evals) products. 
  reasoningEffort*: Option[ReasoningEffort] ## **o1 models only**   Constrains effort on reasoning for  [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response. 
  metadata*: Option[Table[string, string]] ## Developer-defined tags and values used for filtering completions in the [dashboard](https://platform.openai.com/chat-completions). 
  frequencyPenalty*: Option[float] ## Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. 
  logitBias*: Option[Table[string, int]] ## Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
  logprobs*: Option[bool] ## Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. 
  topLogprobs*: Option[int] ## An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used. 
  maxTokens*: Option[int] ## The maximum number of [tokens](/tokenizer) that can be generated in the chat completion. This value can be used to control [costs](https://openai.com/api/pricing/) for text generated via API.  This value is now deprecated in favor of `max_completion_tokens`, and is not compatible with [o1 series models](/docs/guides/reasoning). 
  maxCompletionTokens*: Option[int] ## An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning). 
  n*: Option[int] ## How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
  modalities*: Option[Modalities] ## Output types that you would like the model to generate for this request. Most models are capable of generating text, which is the default:  `[\"text\"]`  The `gpt-4o-audio-preview` model can also be used to [generate audio](/docs/guides/audio). To request that this model generate both text and audio responses, you can use:  `[\"text\", \"audio\"]` 
  prediction*: Option[PredictionContent]
  audio*: Option[CreateChatCompletionRequest_audio]
  presencePenalty*: Option[float] ## Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. 
  responseFormat*: Option[CreateChatCompletionRequest_response_format]
  seed*: Option[int] ## This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. 
  serviceTier*: Option[ServiceTier] ## Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:    - If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.   - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - When not set, the default behavior is 'auto'.    When this parameter is set, the response body will include the `service_tier` utilized. 
  stop*: Option[CreateChatCompletionRequest_stop]
  stream*: Option[bool] ## If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
  streamOptions*: Option[ChatCompletionStreamOptions]
  temperature*: Option[float] ## What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. 
  topP*: Option[float] ## An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. 
  tools*: Option[seq[ChatCompletionTool]] ## A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. 
  toolChoice*: Option[ChatCompletionToolChoiceOption]
  parallelToolCalls*: Option[bool] ## Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.
  user*: Option[string] ## A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). 
  functionCall*: Option[CreateChatCompletionRequest_function_call]
  functions*: Option[seq[ChatCompletionFunctions]] ## Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for. 

func `%`*(v: ReasoningEffort): JsonNode =
  result = case v:
    of ReasoningEffort.Low: %"low"
    of ReasoningEffort.Medium: %"medium"
    of ReasoningEffort.High: %"high"
func `$`*(v: ReasoningEffort): string =
  result = case v:
    of ReasoningEffort.Low: $("low")
    of ReasoningEffort.Medium: $("medium")
    of ReasoningEffort.High: $("high")

proc to*(node: JsonNode, T: typedesc[ReasoningEffort]): ReasoningEffort =
  if node.kind != JString:
    raise newException(ValueError, "Expected string for enum ReasoningEffort, got " & $node.kind)
  let strVal = node.getStr()
  case strVal:
  of $("low"):
    return ReasoningEffort.Low
  of $("medium"):
    return ReasoningEffort.Medium
  of $("high"):
    return ReasoningEffort.High
  else:
    raise newException(ValueError, "Invalid enum value for ReasoningEffort: " & strVal)

func `%`*(v: Modalities): JsonNode =
  result = case v:
    of Modalities.Text: %"text"
    of Modalities.Audio: %"audio"
func `$`*(v: Modalities): string =
  result = case v:
    of Modalities.Text: $("text")
    of Modalities.Audio: $("audio")

proc to*(node: JsonNode, T: typedesc[Modalities]): Modalities =
  if node.kind != JString:
    raise newException(ValueError, "Expected string for enum Modalities, got " & $node.kind)
  let strVal = node.getStr()
  case strVal:
  of $("text"):
    return Modalities.Text
  of $("audio"):
    return Modalities.Audio
  else:
    raise newException(ValueError, "Invalid enum value for Modalities: " & strVal)

func `%`*(v: ServiceTier): JsonNode =
  result = case v:
    of ServiceTier.Auto: %"auto"
    of ServiceTier.Default: %"default"
func `$`*(v: ServiceTier): string =
  result = case v:
    of ServiceTier.Auto: $("auto")
    of ServiceTier.Default: $("default")

proc to*(node: JsonNode, T: typedesc[ServiceTier]): ServiceTier =
  if node.kind != JString:
    raise newException(ValueError, "Expected string for enum ServiceTier, got " & $node.kind)
  let strVal = node.getStr()
  case strVal:
  of $("auto"):
    return ServiceTier.Auto
  of $("default"):
    return ServiceTier.Default
  else:
    raise newException(ValueError, "Invalid enum value for ServiceTier: " & strVal)


# Custom JSON deserialization for CreateChatCompletionRequest with custom field names
proc to*(node: JsonNode, T: typedesc[CreateChatCompletionRequest]): CreateChatCompletionRequest =
  result = CreateChatCompletionRequest()
  if node.kind == JObject:
    if node.hasKey("messages"):
      # Array of types with custom JSON - manually iterate and deserialize
      let arrayNode = node["messages"]
      if arrayNode.kind == JArray:
        result.messages = @[]
        for item in arrayNode.items:
          result.messages.add(to(item, ChatCompletionRequestMessage))
    if node.hasKey("model"):
      result.model = to(node["model"], CreateChatCompletionRequest_model)
    if node.hasKey("store") and node["store"].kind != JNull:
      result.store = some(to(node["store"], typeof(result.store.get())))
    if node.hasKey("reasoning_effort") and node["reasoning_effort"].kind != JNull:
      result.reasoningEffort = some(to(node["reasoning_effort"], ReasoningEffort))
    if node.hasKey("metadata") and node["metadata"].kind != JNull:
      result.metadata = some(to(node["metadata"], typeof(result.metadata.get())))
    if node.hasKey("frequency_penalty") and node["frequency_penalty"].kind != JNull:
      result.frequencyPenalty = some(to(node["frequency_penalty"], typeof(result.frequencyPenalty.get())))
    if node.hasKey("logit_bias") and node["logit_bias"].kind != JNull:
      result.logitBias = some(to(node["logit_bias"], typeof(result.logitBias.get())))
    if node.hasKey("logprobs") and node["logprobs"].kind != JNull:
      result.logprobs = some(to(node["logprobs"], typeof(result.logprobs.get())))
    if node.hasKey("top_logprobs") and node["top_logprobs"].kind != JNull:
      result.topLogprobs = some(to(node["top_logprobs"], typeof(result.topLogprobs.get())))
    if node.hasKey("max_tokens") and node["max_tokens"].kind != JNull:
      result.maxTokens = some(to(node["max_tokens"], typeof(result.maxTokens.get())))
    if node.hasKey("max_completion_tokens") and node["max_completion_tokens"].kind != JNull:
      result.maxCompletionTokens = some(to(node["max_completion_tokens"], typeof(result.maxCompletionTokens.get())))
    if node.hasKey("n") and node["n"].kind != JNull:
      result.n = some(to(node["n"], typeof(result.n.get())))
    if node.hasKey("modalities") and node["modalities"].kind != JNull:
      result.modalities = some(to(node["modalities"], Modalities))
    if node.hasKey("prediction") and node["prediction"].kind != JNull:
      result.prediction = some(to(node["prediction"], typeof(result.prediction.get())))
    if node.hasKey("audio") and node["audio"].kind != JNull:
      result.audio = some(to(node["audio"], typeof(result.audio.get())))
    if node.hasKey("presence_penalty") and node["presence_penalty"].kind != JNull:
      result.presencePenalty = some(to(node["presence_penalty"], typeof(result.presencePenalty.get())))
    if node.hasKey("response_format") and node["response_format"].kind != JNull:
      result.responseFormat = some(to(node["response_format"], typeof(result.responseFormat.get())))
    if node.hasKey("seed") and node["seed"].kind != JNull:
      result.seed = some(to(node["seed"], typeof(result.seed.get())))
    if node.hasKey("service_tier") and node["service_tier"].kind != JNull:
      result.serviceTier = some(to(node["service_tier"], ServiceTier))
    if node.hasKey("stop") and node["stop"].kind != JNull:
      result.stop = some(to(node["stop"], typeof(result.stop.get())))
    if node.hasKey("stream") and node["stream"].kind != JNull:
      result.stream = some(to(node["stream"], typeof(result.stream.get())))
    if node.hasKey("stream_options") and node["stream_options"].kind != JNull:
      result.streamOptions = some(to(node["stream_options"], typeof(result.streamOptions.get())))
    if node.hasKey("temperature") and node["temperature"].kind != JNull:
      result.temperature = some(to(node["temperature"], typeof(result.temperature.get())))
    if node.hasKey("top_p") and node["top_p"].kind != JNull:
      result.topP = some(to(node["top_p"], typeof(result.topP.get())))
    if node.hasKey("tools") and node["tools"].kind != JNull:
      result.tools = some(to(node["tools"], typeof(result.tools.get())))
    if node.hasKey("tool_choice") and node["tool_choice"].kind != JNull:
      result.toolChoice = some(to(node["tool_choice"], typeof(result.toolChoice.get())))
    if node.hasKey("parallel_tool_calls") and node["parallel_tool_calls"].kind != JNull:
      result.parallelToolCalls = some(to(node["parallel_tool_calls"], typeof(result.parallelToolCalls.get())))
    if node.hasKey("user") and node["user"].kind != JNull:
      result.user = some(to(node["user"], typeof(result.user.get())))
    if node.hasKey("function_call") and node["function_call"].kind != JNull:
      result.functionCall = some(to(node["function_call"], typeof(result.functionCall.get())))
    if node.hasKey("functions") and node["functions"].kind != JNull:
      result.functions = some(to(node["functions"], typeof(result.functions.get())))

# Custom JSON serialization for CreateChatCompletionRequest with custom field names
proc `%`*(obj: CreateChatCompletionRequest): JsonNode =
  result = newJObject()
  result["messages"] = %obj.messages
  result["model"] = %obj.model
  if obj.store.isSome():
    result["store"] = %obj.store.get()
  if obj.reasoningEffort.isSome():
    result["reasoning_effort"] = %obj.reasoningEffort.get()
  if obj.metadata.isSome():
    result["metadata"] = %obj.metadata.get()
  if obj.frequencyPenalty.isSome():
    result["frequency_penalty"] = %obj.frequencyPenalty.get()
  if obj.logitBias.isSome():
    result["logit_bias"] = %obj.logitBias.get()
  if obj.logprobs.isSome():
    result["logprobs"] = %obj.logprobs.get()
  if obj.topLogprobs.isSome():
    result["top_logprobs"] = %obj.topLogprobs.get()
  if obj.maxTokens.isSome():
    result["max_tokens"] = %obj.maxTokens.get()
  if obj.maxCompletionTokens.isSome():
    result["max_completion_tokens"] = %obj.maxCompletionTokens.get()
  if obj.n.isSome():
    result["n"] = %obj.n.get()
  if obj.modalities.isSome():
    result["modalities"] = %obj.modalities.get()
  if obj.prediction.isSome():
    result["prediction"] = %obj.prediction.get()
  if obj.audio.isSome():
    result["audio"] = %obj.audio.get()
  if obj.presencePenalty.isSome():
    result["presence_penalty"] = %obj.presencePenalty.get()
  if obj.responseFormat.isSome():
    result["response_format"] = %obj.responseFormat.get()
  if obj.seed.isSome():
    result["seed"] = %obj.seed.get()
  if obj.serviceTier.isSome():
    result["service_tier"] = %obj.serviceTier.get()
  if obj.stop.isSome():
    result["stop"] = %obj.stop.get()
  if obj.stream.isSome():
    result["stream"] = %obj.stream.get()
  if obj.streamOptions.isSome():
    result["stream_options"] = %obj.streamOptions.get()
  if obj.temperature.isSome():
    result["temperature"] = %obj.temperature.get()
  if obj.topP.isSome():
    result["top_p"] = %obj.topP.get()
  if obj.tools.isSome():
    result["tools"] = %obj.tools.get()
  if obj.toolChoice.isSome():
    result["tool_choice"] = %obj.toolChoice.get()
  if obj.parallelToolCalls.isSome():
    result["parallel_tool_calls"] = %obj.parallelToolCalls.get()
  if obj.user.isSome():
    result["user"] = %obj.user.get()
  if obj.functionCall.isSome():
    result["function_call"] = %obj.functionCall.get()
  if obj.functions.isSome():
    result["functions"] = %obj.functions.get()

