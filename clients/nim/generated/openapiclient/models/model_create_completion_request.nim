#
# OpenAI API
# 
# The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
# The version of the OpenAPI document: 2.0.0
# Contact: blah+oapicf@cliffano.com
# Generated by: https://openapi-generator.tech
#

import json
import tables
import marshal
import options

import model_create_completion_request_model
import model_create_completion_request_prompt
import model_create_completion_request_stop

type CreateCompletionRequest* = object
  ## 
  model*: CreateCompletionRequest_model
  prompt*: Option[CreateCompletionRequest_prompt]
  bestOf*: Option[int] ## Generates `best_of` completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.  When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return â€“ `best_of` must be greater than `n`.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`. 
  echo*: Option[bool] ## Echo back the prompt in addition to the completion 
  frequencyPenalty*: Option[float] ## Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
  logitBias*: Option[Table[string, int]] ## Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.  As an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token from being generated. 
  logprobs*: Option[int] ## Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.  The maximum value for `logprobs` is 5. 
  maxTokens*: Option[int] ## The maximum number of [tokens](/tokenizer) that can be generated in the completion.  The token count of your prompt plus `max_tokens` cannot exceed the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. 
  n*: Option[int] ## How many completions to generate for each prompt.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`. 
  presencePenalty*: Option[float] ## Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) 
  seed*: Option[int] ## If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.  Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. 
  stop*: Option[CreateCompletionRequest_stop]
  stream*: Option[bool] ## Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
  suffix*: Option[string] ## The suffix that comes after a completion of inserted text.  This parameter is only supported for `gpt-3.5-turbo-instruct`. 
  temperature*: Option[float] ## What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both. 
  topP*: Option[float] ## An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. 
  user*: Option[string] ## A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). 


# Custom JSON deserialization for CreateCompletionRequest with custom field names
proc to*(node: JsonNode, T: typedesc[CreateCompletionRequest]): CreateCompletionRequest =
  result = CreateCompletionRequest()
  if node.kind == JObject:
    if node.hasKey("model"):
      result.model = to(node["model"], CreateCompletionRequest_model)
    if node.hasKey("prompt") and node["prompt"].kind != JNull:
      result.prompt = some(to(node["prompt"], typeof(result.prompt.get())))
    if node.hasKey("best_of") and node["best_of"].kind != JNull:
      result.bestOf = some(to(node["best_of"], typeof(result.bestOf.get())))
    if node.hasKey("echo") and node["echo"].kind != JNull:
      result.echo = some(to(node["echo"], typeof(result.echo.get())))
    if node.hasKey("frequency_penalty") and node["frequency_penalty"].kind != JNull:
      result.frequencyPenalty = some(to(node["frequency_penalty"], typeof(result.frequencyPenalty.get())))
    if node.hasKey("logit_bias") and node["logit_bias"].kind != JNull:
      result.logitBias = some(to(node["logit_bias"], typeof(result.logitBias.get())))
    if node.hasKey("logprobs") and node["logprobs"].kind != JNull:
      result.logprobs = some(to(node["logprobs"], typeof(result.logprobs.get())))
    if node.hasKey("max_tokens") and node["max_tokens"].kind != JNull:
      result.maxTokens = some(to(node["max_tokens"], typeof(result.maxTokens.get())))
    if node.hasKey("n") and node["n"].kind != JNull:
      result.n = some(to(node["n"], typeof(result.n.get())))
    if node.hasKey("presence_penalty") and node["presence_penalty"].kind != JNull:
      result.presencePenalty = some(to(node["presence_penalty"], typeof(result.presencePenalty.get())))
    if node.hasKey("seed") and node["seed"].kind != JNull:
      result.seed = some(to(node["seed"], typeof(result.seed.get())))
    if node.hasKey("stop") and node["stop"].kind != JNull:
      result.stop = some(to(node["stop"], typeof(result.stop.get())))
    if node.hasKey("stream") and node["stream"].kind != JNull:
      result.stream = some(to(node["stream"], typeof(result.stream.get())))
    if node.hasKey("suffix") and node["suffix"].kind != JNull:
      result.suffix = some(to(node["suffix"], typeof(result.suffix.get())))
    if node.hasKey("temperature") and node["temperature"].kind != JNull:
      result.temperature = some(to(node["temperature"], typeof(result.temperature.get())))
    if node.hasKey("top_p") and node["top_p"].kind != JNull:
      result.topP = some(to(node["top_p"], typeof(result.topP.get())))
    if node.hasKey("user") and node["user"].kind != JNull:
      result.user = some(to(node["user"], typeof(result.user.get())))

# Custom JSON serialization for CreateCompletionRequest with custom field names
proc `%`*(obj: CreateCompletionRequest): JsonNode =
  result = newJObject()
  result["model"] = %obj.model
  if obj.prompt.isSome():
    result["prompt"] = %obj.prompt.get()
  if obj.bestOf.isSome():
    result["best_of"] = %obj.bestOf.get()
  if obj.echo.isSome():
    result["echo"] = %obj.echo.get()
  if obj.frequencyPenalty.isSome():
    result["frequency_penalty"] = %obj.frequencyPenalty.get()
  if obj.logitBias.isSome():
    result["logit_bias"] = %obj.logitBias.get()
  if obj.logprobs.isSome():
    result["logprobs"] = %obj.logprobs.get()
  if obj.maxTokens.isSome():
    result["max_tokens"] = %obj.maxTokens.get()
  if obj.n.isSome():
    result["n"] = %obj.n.get()
  if obj.presencePenalty.isSome():
    result["presence_penalty"] = %obj.presencePenalty.get()
  if obj.seed.isSome():
    result["seed"] = %obj.seed.get()
  if obj.stop.isSome():
    result["stop"] = %obj.stop.get()
  if obj.stream.isSome():
    result["stream"] = %obj.stream.get()
  if obj.suffix.isSome():
    result["suffix"] = %obj.suffix.get()
  if obj.temperature.isSome():
    result["temperature"] = %obj.temperature.get()
  if obj.topP.isSome():
    result["top_p"] = %obj.topP.get()
  if obj.user.isSome():
    result["user"] = %obj.user.get()

