#' Create a new CreateChatCompletionRequest
#'
#' @description
#' CreateChatCompletionRequest Class
#'
#' @docType class
#' @title CreateChatCompletionRequest
#' @description CreateChatCompletionRequest Class
#' @format An \code{R6Class} generator object
#' @field messages A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models). list(\link{ChatCompletionRequestMessage})
#' @field model  \link{CreateChatCompletionRequestModel}
#' @field frequency_penalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) numeric [optional]
#' @field logit_bias Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. named list(integer) [optional]
#' @field logprobs Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. character [optional]
#' @field top_logprobs An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used. integer [optional]
#' @field max_tokens The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.  The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. integer [optional]
#' @field n How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs. integer [optional]
#' @field presence_penalty Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details) numeric [optional]
#' @field response_format  \link{CreateChatCompletionRequestResponseFormat} [optional]
#' @field seed This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. integer [optional]
#' @field stop  \link{CreateChatCompletionRequestStop} [optional]
#' @field stream If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). character [optional]
#' @field temperature What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both. numeric [optional]
#' @field top_p An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10\% probability mass are considered.  We generally recommend altering this or `temperature` but not both. numeric [optional]
#' @field tools A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. list(\link{ChatCompletionTool}) [optional]
#' @field tool_choice  \link{ChatCompletionToolChoiceOption} [optional]
#' @field user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). character [optional]
#' @field function_call  \link{CreateChatCompletionRequestFunctionCall} [optional]
#' @field functions Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for. list(\link{ChatCompletionFunctions}) [optional]
#' @importFrom R6 R6Class
#' @importFrom jsonlite fromJSON toJSON
#' @export
CreateChatCompletionRequest <- R6::R6Class(
  "CreateChatCompletionRequest",
  public = list(
    `messages` = NULL,
    `model` = NULL,
    `frequency_penalty` = NULL,
    `logit_bias` = NULL,
    `logprobs` = NULL,
    `top_logprobs` = NULL,
    `max_tokens` = NULL,
    `n` = NULL,
    `presence_penalty` = NULL,
    `response_format` = NULL,
    `seed` = NULL,
    `stop` = NULL,
    `stream` = NULL,
    `temperature` = NULL,
    `top_p` = NULL,
    `tools` = NULL,
    `tool_choice` = NULL,
    `user` = NULL,
    `function_call` = NULL,
    `functions` = NULL,

    #' @description
    #' Initialize a new CreateChatCompletionRequest class.
    #'
    #' @param messages A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
    #' @param model model
    #' @param frequency_penalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details). Default to 0.
    #' @param logit_bias Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
    #' @param logprobs Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.. Default to FALSE.
    #' @param top_logprobs An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
    #' @param max_tokens The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.  The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
    #' @param n How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.. Default to 1.
    #' @param presence_penalty Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details). Default to 0.
    #' @param response_format response_format
    #' @param seed This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
    #' @param stop stop
    #' @param stream If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).. Default to FALSE.
    #' @param temperature What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both.. Default to 1.
    #' @param top_p An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10\% probability mass are considered.  We generally recommend altering this or `temperature` but not both.. Default to 1.
    #' @param tools A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
    #' @param tool_choice tool_choice
    #' @param user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
    #' @param function_call function_call
    #' @param functions Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for.
    #' @param ... Other optional arguments.
    initialize = function(`messages`, `model`, `frequency_penalty` = 0, `logit_bias` = NULL, `logprobs` = FALSE, `top_logprobs` = NULL, `max_tokens` = NULL, `n` = 1, `presence_penalty` = 0, `response_format` = NULL, `seed` = NULL, `stop` = NULL, `stream` = FALSE, `temperature` = 1, `top_p` = 1, `tools` = NULL, `tool_choice` = NULL, `user` = NULL, `function_call` = NULL, `functions` = NULL, ...) {
      if (!missing(`messages`)) {
        stopifnot(is.vector(`messages`), length(`messages`) != 0)
        sapply(`messages`, function(x) stopifnot(R6::is.R6(x)))
        self$`messages` <- `messages`
      }
      if (!missing(`model`)) {
        stopifnot(R6::is.R6(`model`))
        self$`model` <- `model`
      }
      if (!is.null(`frequency_penalty`)) {
        self$`frequency_penalty` <- `frequency_penalty`
      }
      if (!is.null(`logit_bias`)) {
        stopifnot(is.vector(`logit_bias`), length(`logit_bias`) != 0)
        sapply(`logit_bias`, function(x) stopifnot(is.character(x)))
        self$`logit_bias` <- `logit_bias`
      }
      if (!is.null(`logprobs`)) {
        if (!(is.logical(`logprobs`) && length(`logprobs`) == 1)) {
          stop(paste("Error! Invalid data for `logprobs`. Must be a boolean:", `logprobs`))
        }
        self$`logprobs` <- `logprobs`
      }
      if (!is.null(`top_logprobs`)) {
        if (!(is.numeric(`top_logprobs`) && length(`top_logprobs`) == 1)) {
          stop(paste("Error! Invalid data for `top_logprobs`. Must be an integer:", `top_logprobs`))
        }
        self$`top_logprobs` <- `top_logprobs`
      }
      if (!is.null(`max_tokens`)) {
        if (!(is.numeric(`max_tokens`) && length(`max_tokens`) == 1)) {
          stop(paste("Error! Invalid data for `max_tokens`. Must be an integer:", `max_tokens`))
        }
        self$`max_tokens` <- `max_tokens`
      }
      if (!is.null(`n`)) {
        if (!(is.numeric(`n`) && length(`n`) == 1)) {
          stop(paste("Error! Invalid data for `n`. Must be an integer:", `n`))
        }
        self$`n` <- `n`
      }
      if (!is.null(`presence_penalty`)) {
        self$`presence_penalty` <- `presence_penalty`
      }
      if (!is.null(`response_format`)) {
        stopifnot(R6::is.R6(`response_format`))
        self$`response_format` <- `response_format`
      }
      if (!is.null(`seed`)) {
        if (!(is.numeric(`seed`) && length(`seed`) == 1)) {
          stop(paste("Error! Invalid data for `seed`. Must be an integer:", `seed`))
        }
        self$`seed` <- `seed`
      }
      if (!is.null(`stop`)) {
        stopifnot(R6::is.R6(`stop`))
        self$`stop` <- `stop`
      }
      if (!is.null(`stream`)) {
        if (!(is.logical(`stream`) && length(`stream`) == 1)) {
          stop(paste("Error! Invalid data for `stream`. Must be a boolean:", `stream`))
        }
        self$`stream` <- `stream`
      }
      if (!is.null(`temperature`)) {
        self$`temperature` <- `temperature`
      }
      if (!is.null(`top_p`)) {
        self$`top_p` <- `top_p`
      }
      if (!is.null(`tools`)) {
        stopifnot(is.vector(`tools`), length(`tools`) != 0)
        sapply(`tools`, function(x) stopifnot(R6::is.R6(x)))
        self$`tools` <- `tools`
      }
      if (!is.null(`tool_choice`)) {
        stopifnot(R6::is.R6(`tool_choice`))
        self$`tool_choice` <- `tool_choice`
      }
      if (!is.null(`user`)) {
        if (!(is.character(`user`) && length(`user`) == 1)) {
          stop(paste("Error! Invalid data for `user`. Must be a string:", `user`))
        }
        self$`user` <- `user`
      }
      if (!is.null(`function_call`)) {
        stopifnot(R6::is.R6(`function_call`))
        self$`function_call` <- `function_call`
      }
      if (!is.null(`functions`)) {
        stopifnot(is.vector(`functions`), length(`functions`) != 0)
        sapply(`functions`, function(x) stopifnot(R6::is.R6(x)))
        self$`functions` <- `functions`
      }
    },

    #' @description
    #' Convert to an R object. This method is deprecated. Use `toSimpleType()` instead.
    toJSON = function() {
      .Deprecated(new = "toSimpleType", msg = "Use the '$toSimpleType()' method instead since that is more clearly named. Use '$toJSONString()' to get a JSON string")
      return(self$toSimpleType())
    },

    #' @description
    #' Convert to a List
    #'
    #' Convert the R6 object to a list to work more easily with other tooling.
    #'
    #' @return CreateChatCompletionRequest as a base R list.
    #' @examples
    #' # convert array of CreateChatCompletionRequest (x) to a data frame
    #' \dontrun{
    #' library(purrr)
    #' library(tibble)
    #' df <- x |> map(\(y)y$toList()) |> map(as_tibble) |> list_rbind()
    #' df
    #' }
    toList = function() {
      return(self$toSimpleType())
    },

    #' @description
    #' Convert CreateChatCompletionRequest to a base R type
    #'
    #' @return A base R type, e.g. a list or numeric/character array.
    toSimpleType = function() {
      CreateChatCompletionRequestObject <- list()
      if (!is.null(self$`messages`)) {
        CreateChatCompletionRequestObject[["messages"]] <-
          lapply(self$`messages`, function(x) x$toSimpleType())
      }
      if (!is.null(self$`model`)) {
        CreateChatCompletionRequestObject[["model"]] <-
          self$`model`$toSimpleType()
      }
      if (!is.null(self$`frequency_penalty`)) {
        CreateChatCompletionRequestObject[["frequency_penalty"]] <-
          self$`frequency_penalty`
      }
      if (!is.null(self$`logit_bias`)) {
        CreateChatCompletionRequestObject[["logit_bias"]] <-
          self$`logit_bias`
      }
      if (!is.null(self$`logprobs`)) {
        CreateChatCompletionRequestObject[["logprobs"]] <-
          self$`logprobs`
      }
      if (!is.null(self$`top_logprobs`)) {
        CreateChatCompletionRequestObject[["top_logprobs"]] <-
          self$`top_logprobs`
      }
      if (!is.null(self$`max_tokens`)) {
        CreateChatCompletionRequestObject[["max_tokens"]] <-
          self$`max_tokens`
      }
      if (!is.null(self$`n`)) {
        CreateChatCompletionRequestObject[["n"]] <-
          self$`n`
      }
      if (!is.null(self$`presence_penalty`)) {
        CreateChatCompletionRequestObject[["presence_penalty"]] <-
          self$`presence_penalty`
      }
      if (!is.null(self$`response_format`)) {
        CreateChatCompletionRequestObject[["response_format"]] <-
          self$`response_format`$toSimpleType()
      }
      if (!is.null(self$`seed`)) {
        CreateChatCompletionRequestObject[["seed"]] <-
          self$`seed`
      }
      if (!is.null(self$`stop`)) {
        CreateChatCompletionRequestObject[["stop"]] <-
          self$`stop`$toSimpleType()
      }
      if (!is.null(self$`stream`)) {
        CreateChatCompletionRequestObject[["stream"]] <-
          self$`stream`
      }
      if (!is.null(self$`temperature`)) {
        CreateChatCompletionRequestObject[["temperature"]] <-
          self$`temperature`
      }
      if (!is.null(self$`top_p`)) {
        CreateChatCompletionRequestObject[["top_p"]] <-
          self$`top_p`
      }
      if (!is.null(self$`tools`)) {
        CreateChatCompletionRequestObject[["tools"]] <-
          lapply(self$`tools`, function(x) x$toSimpleType())
      }
      if (!is.null(self$`tool_choice`)) {
        CreateChatCompletionRequestObject[["tool_choice"]] <-
          self$`tool_choice`$toSimpleType()
      }
      if (!is.null(self$`user`)) {
        CreateChatCompletionRequestObject[["user"]] <-
          self$`user`
      }
      if (!is.null(self$`function_call`)) {
        CreateChatCompletionRequestObject[["function_call"]] <-
          self$`function_call`$toSimpleType()
      }
      if (!is.null(self$`functions`)) {
        CreateChatCompletionRequestObject[["functions"]] <-
          lapply(self$`functions`, function(x) x$toSimpleType())
      }
      return(CreateChatCompletionRequestObject)
    },

    #' @description
    #' Deserialize JSON string into an instance of CreateChatCompletionRequest
    #'
    #' @param input_json the JSON input
    #' @return the instance of CreateChatCompletionRequest
    fromJSON = function(input_json) {
      this_object <- jsonlite::fromJSON(input_json)
      if (!is.null(this_object$`messages`)) {
        self$`messages` <- ApiClient$new()$deserializeObj(this_object$`messages`, "array[ChatCompletionRequestMessage]", loadNamespace("openapi"))
      }
      if (!is.null(this_object$`model`)) {
        `model_object` <- CreateChatCompletionRequestModel$new()
        `model_object`$fromJSON(jsonlite::toJSON(this_object$`model`, auto_unbox = TRUE, digits = NA))
        self$`model` <- `model_object`
      }
      if (!is.null(this_object$`frequency_penalty`)) {
        self$`frequency_penalty` <- this_object$`frequency_penalty`
      }
      if (!is.null(this_object$`logit_bias`)) {
        self$`logit_bias` <- ApiClient$new()$deserializeObj(this_object$`logit_bias`, "map(integer)", loadNamespace("openapi"))
      }
      if (!is.null(this_object$`logprobs`)) {
        self$`logprobs` <- this_object$`logprobs`
      }
      if (!is.null(this_object$`top_logprobs`)) {
        self$`top_logprobs` <- this_object$`top_logprobs`
      }
      if (!is.null(this_object$`max_tokens`)) {
        self$`max_tokens` <- this_object$`max_tokens`
      }
      if (!is.null(this_object$`n`)) {
        self$`n` <- this_object$`n`
      }
      if (!is.null(this_object$`presence_penalty`)) {
        self$`presence_penalty` <- this_object$`presence_penalty`
      }
      if (!is.null(this_object$`response_format`)) {
        `response_format_object` <- CreateChatCompletionRequestResponseFormat$new()
        `response_format_object`$fromJSON(jsonlite::toJSON(this_object$`response_format`, auto_unbox = TRUE, digits = NA))
        self$`response_format` <- `response_format_object`
      }
      if (!is.null(this_object$`seed`)) {
        self$`seed` <- this_object$`seed`
      }
      if (!is.null(this_object$`stop`)) {
        `stop_object` <- CreateChatCompletionRequestStop$new()
        `stop_object`$fromJSON(jsonlite::toJSON(this_object$`stop`, auto_unbox = TRUE, digits = NA))
        self$`stop` <- `stop_object`
      }
      if (!is.null(this_object$`stream`)) {
        self$`stream` <- this_object$`stream`
      }
      if (!is.null(this_object$`temperature`)) {
        self$`temperature` <- this_object$`temperature`
      }
      if (!is.null(this_object$`top_p`)) {
        self$`top_p` <- this_object$`top_p`
      }
      if (!is.null(this_object$`tools`)) {
        self$`tools` <- ApiClient$new()$deserializeObj(this_object$`tools`, "array[ChatCompletionTool]", loadNamespace("openapi"))
      }
      if (!is.null(this_object$`tool_choice`)) {
        `tool_choice_object` <- ChatCompletionToolChoiceOption$new()
        `tool_choice_object`$fromJSON(jsonlite::toJSON(this_object$`tool_choice`, auto_unbox = TRUE, digits = NA))
        self$`tool_choice` <- `tool_choice_object`
      }
      if (!is.null(this_object$`user`)) {
        self$`user` <- this_object$`user`
      }
      if (!is.null(this_object$`function_call`)) {
        `function_call_object` <- CreateChatCompletionRequestFunctionCall$new()
        `function_call_object`$fromJSON(jsonlite::toJSON(this_object$`function_call`, auto_unbox = TRUE, digits = NA))
        self$`function_call` <- `function_call_object`
      }
      if (!is.null(this_object$`functions`)) {
        self$`functions` <- ApiClient$new()$deserializeObj(this_object$`functions`, "array[ChatCompletionFunctions]", loadNamespace("openapi"))
      }
      self
    },

    #' @description
    #' To JSON String
    #' 
    #' @param ... Parameters passed to `jsonlite::toJSON`
    #' @return CreateChatCompletionRequest in JSON format
    toJSONString = function(...) {
      simple <- self$toSimpleType()
      json <- jsonlite::toJSON(simple, auto_unbox = TRUE, digits = NA, ...)
      return(as.character(jsonlite::minify(json)))
    },

    #' @description
    #' Deserialize JSON string into an instance of CreateChatCompletionRequest
    #'
    #' @param input_json the JSON input
    #' @return the instance of CreateChatCompletionRequest
    fromJSONString = function(input_json) {
      this_object <- jsonlite::fromJSON(input_json)
      self$`messages` <- ApiClient$new()$deserializeObj(this_object$`messages`, "array[ChatCompletionRequestMessage]", loadNamespace("openapi"))
      self$`model` <- CreateChatCompletionRequestModel$new()$fromJSON(jsonlite::toJSON(this_object$`model`, auto_unbox = TRUE, digits = NA))
      self$`frequency_penalty` <- this_object$`frequency_penalty`
      self$`logit_bias` <- ApiClient$new()$deserializeObj(this_object$`logit_bias`, "map(integer)", loadNamespace("openapi"))
      self$`logprobs` <- this_object$`logprobs`
      self$`top_logprobs` <- this_object$`top_logprobs`
      self$`max_tokens` <- this_object$`max_tokens`
      self$`n` <- this_object$`n`
      self$`presence_penalty` <- this_object$`presence_penalty`
      self$`response_format` <- CreateChatCompletionRequestResponseFormat$new()$fromJSON(jsonlite::toJSON(this_object$`response_format`, auto_unbox = TRUE, digits = NA))
      self$`seed` <- this_object$`seed`
      self$`stop` <- CreateChatCompletionRequestStop$new()$fromJSON(jsonlite::toJSON(this_object$`stop`, auto_unbox = TRUE, digits = NA))
      self$`stream` <- this_object$`stream`
      self$`temperature` <- this_object$`temperature`
      self$`top_p` <- this_object$`top_p`
      self$`tools` <- ApiClient$new()$deserializeObj(this_object$`tools`, "array[ChatCompletionTool]", loadNamespace("openapi"))
      self$`tool_choice` <- ChatCompletionToolChoiceOption$new()$fromJSON(jsonlite::toJSON(this_object$`tool_choice`, auto_unbox = TRUE, digits = NA))
      self$`user` <- this_object$`user`
      self$`function_call` <- CreateChatCompletionRequestFunctionCall$new()$fromJSON(jsonlite::toJSON(this_object$`function_call`, auto_unbox = TRUE, digits = NA))
      self$`functions` <- ApiClient$new()$deserializeObj(this_object$`functions`, "array[ChatCompletionFunctions]", loadNamespace("openapi"))
      self
    },

    #' @description
    #' Validate JSON input with respect to CreateChatCompletionRequest and throw an exception if invalid
    #'
    #' @param input the JSON input
    validateJSON = function(input) {
      input_json <- jsonlite::fromJSON(input)
      # check the required field `messages`
      if (!is.null(input_json$`messages`)) {
        stopifnot(is.vector(input_json$`messages`), length(input_json$`messages`) != 0)
        tmp <- sapply(input_json$`messages`, function(x) stopifnot(R6::is.R6(x)))
      } else {
        stop(paste("The JSON input `", input, "` is invalid for CreateChatCompletionRequest: the required field `messages` is missing."))
      }
      # check the required field `model`
      if (!is.null(input_json$`model`)) {
        stopifnot(R6::is.R6(input_json$`model`))
      } else {
        stop(paste("The JSON input `", input, "` is invalid for CreateChatCompletionRequest: the required field `model` is missing."))
      }
    },

    #' @description
    #' To string (JSON format)
    #'
    #' @return String representation of CreateChatCompletionRequest
    toString = function() {
      self$toJSONString()
    },

    #' @description
    #' Return true if the values in all fields are valid.
    #'
    #' @return true if the values in all fields are valid.
    isValid = function() {
      # check if the required `messages` is null
      if (is.null(self$`messages`)) {
        return(FALSE)
      }

      if (length(self$`messages`) < 1) {
        return(FALSE)
      }

      # check if the required `model` is null
      if (is.null(self$`model`)) {
        return(FALSE)
      }

      if (self$`frequency_penalty` > 2) {
        return(FALSE)
      }
      if (self$`frequency_penalty` < -2) {
        return(FALSE)
      }

      if (self$`top_logprobs` > 20) {
        return(FALSE)
      }
      if (self$`top_logprobs` < 0) {
        return(FALSE)
      }

      if (self$`n` > 128) {
        return(FALSE)
      }
      if (self$`n` < 1) {
        return(FALSE)
      }

      if (self$`presence_penalty` > 2) {
        return(FALSE)
      }
      if (self$`presence_penalty` < -2) {
        return(FALSE)
      }

      if (self$`seed` > 9223372036854775807) {
        return(FALSE)
      }
      if (self$`seed` < -9223372036854775808) {
        return(FALSE)
      }

      if (self$`temperature` > 2) {
        return(FALSE)
      }
      if (self$`temperature` < 0) {
        return(FALSE)
      }

      if (self$`top_p` > 1) {
        return(FALSE)
      }
      if (self$`top_p` < 0) {
        return(FALSE)
      }

      if (length(self$`functions`) > 128) {
        return(FALSE)
      }
      if (length(self$`functions`) < 1) {
        return(FALSE)
      }

      TRUE
    },

    #' @description
    #' Return a list of invalid fields (if any).
    #'
    #' @return A list of invalid fields (if any).
    getInvalidFields = function() {
      invalid_fields <- list()
      # check if the required `messages` is null
      if (is.null(self$`messages`)) {
        invalid_fields["messages"] <- "Non-nullable required field `messages` cannot be null."
      }

      if (length(self$`messages`) < 1) {
        invalid_fields["messages"] <- "Invalid length for ``, number of items must be greater than or equal to 1."
      }

      # check if the required `model` is null
      if (is.null(self$`model`)) {
        invalid_fields["model"] <- "Non-nullable required field `model` cannot be null."
      }

      if (self$`frequency_penalty` > 2) {
        invalid_fields["frequency_penalty"] <- "Invalid value for `frequency_penalty`, must be smaller than or equal to 2."
      }
      if (self$`frequency_penalty` < -2) {
        invalid_fields["frequency_penalty"] <- "Invalid value for `frequency_penalty`, must be bigger than or equal to -2."
      }

      if (self$`top_logprobs` > 20) {
        invalid_fields["top_logprobs"] <- "Invalid value for `top_logprobs`, must be smaller than or equal to 20."
      }
      if (self$`top_logprobs` < 0) {
        invalid_fields["top_logprobs"] <- "Invalid value for `top_logprobs`, must be bigger than or equal to 0."
      }

      if (self$`n` > 128) {
        invalid_fields["n"] <- "Invalid value for `n`, must be smaller than or equal to 128."
      }
      if (self$`n` < 1) {
        invalid_fields["n"] <- "Invalid value for `n`, must be bigger than or equal to 1."
      }

      if (self$`presence_penalty` > 2) {
        invalid_fields["presence_penalty"] <- "Invalid value for `presence_penalty`, must be smaller than or equal to 2."
      }
      if (self$`presence_penalty` < -2) {
        invalid_fields["presence_penalty"] <- "Invalid value for `presence_penalty`, must be bigger than or equal to -2."
      }

      if (self$`seed` > 9223372036854775807) {
        invalid_fields["seed"] <- "Invalid value for `seed`, must be smaller than or equal to 9223372036854775807."
      }
      if (self$`seed` < -9223372036854775808) {
        invalid_fields["seed"] <- "Invalid value for `seed`, must be bigger than or equal to -9223372036854775808."
      }

      if (self$`temperature` > 2) {
        invalid_fields["temperature"] <- "Invalid value for `temperature`, must be smaller than or equal to 2."
      }
      if (self$`temperature` < 0) {
        invalid_fields["temperature"] <- "Invalid value for `temperature`, must be bigger than or equal to 0."
      }

      if (self$`top_p` > 1) {
        invalid_fields["top_p"] <- "Invalid value for `top_p`, must be smaller than or equal to 1."
      }
      if (self$`top_p` < 0) {
        invalid_fields["top_p"] <- "Invalid value for `top_p`, must be bigger than or equal to 0."
      }

      if (length(self$`functions`) > 128) {
        invalid_fields["functions"] <- "Invalid length for `functions`, number of items must be less than or equal to 128."
      }
      if (length(self$`functions`) < 1) {
        invalid_fields["functions"] <- "Invalid length for ``, number of items must be greater than or equal to 1."
      }

      invalid_fields
    },

    #' @description
    #' Print the object
    print = function() {
      print(jsonlite::prettify(self$toJSONString()))
      invisible(self)
    }
  ),
  # Lock the class to prevent modifications to the method or field
  lock_class = TRUE
)
## Uncomment below to unlock the class to allow modifications of the method or field
# CreateChatCompletionRequest$unlock()
#
## Below is an example to define the print function
# CreateChatCompletionRequest$set("public", "print", function(...) {
#   print(jsonlite::prettify(self$toJSONString()))
#   invisible(self)
# })
## Uncomment below to lock the class to prevent modifications to the method or field
# CreateChatCompletionRequest$lock()

