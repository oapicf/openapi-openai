#' Create a new CreateChatCompletionRequest
#'
#' @description
#' CreateChatCompletionRequest Class
#'
#' @docType class
#' @title CreateChatCompletionRequest
#' @description CreateChatCompletionRequest Class
#' @format An \code{R6Class} generator object
#' @field model  \link{CreateChatCompletionRequestModel}
#' @field messages A list of messages comprising the conversation so far. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb). list(\link{ChatCompletionRequestMessage})
#' @field functions A list of functions the model may generate JSON inputs for. list(\link{ChatCompletionFunctions}) [optional]
#' @field function_call  \link{CreateChatCompletionRequestFunctionCall} [optional]
#' @field temperature What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both. numeric [optional]
#' @field top_p An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10\% probability mass are considered.  We generally recommend altering this or `temperature` but not both. numeric [optional]
#' @field n How many chat completion choices to generate for each input message. integer [optional]
#' @field stream If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb). character [optional]
#' @field stop  \link{CreateChatCompletionRequestStop} [optional]
#' @field max_tokens The maximum number of [tokens](/tokenizer) to generate in the chat completion.  The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) for counting tokens. integer [optional]
#' @field presence_penalty Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details) numeric [optional]
#' @field frequency_penalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details) numeric [optional]
#' @field logit_bias Modify the likelihood of specified tokens appearing in the completion.  Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. object [optional]
#' @field user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). character [optional]
#' @importFrom R6 R6Class
#' @importFrom jsonlite fromJSON toJSON
#' @export
CreateChatCompletionRequest <- R6::R6Class(
  "CreateChatCompletionRequest",
  public = list(
    `model` = NULL,
    `messages` = NULL,
    `functions` = NULL,
    `function_call` = NULL,
    `temperature` = NULL,
    `top_p` = NULL,
    `n` = NULL,
    `stream` = NULL,
    `stop` = NULL,
    `max_tokens` = NULL,
    `presence_penalty` = NULL,
    `frequency_penalty` = NULL,
    `logit_bias` = NULL,
    `user` = NULL,
    #' Initialize a new CreateChatCompletionRequest class.
    #'
    #' @description
    #' Initialize a new CreateChatCompletionRequest class.
    #'
    #' @param model model
    #' @param messages A list of messages comprising the conversation so far. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
    #' @param functions A list of functions the model may generate JSON inputs for.
    #' @param function_call function_call
    #' @param temperature What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both.. Default to 1.
    #' @param top_p An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10\% probability mass are considered.  We generally recommend altering this or `temperature` but not both.. Default to 1.
    #' @param n How many chat completion choices to generate for each input message.. Default to 1.
    #' @param stream If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).. Default to FALSE.
    #' @param stop stop
    #' @param max_tokens The maximum number of [tokens](/tokenizer) to generate in the chat completion.  The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) for counting tokens.
    #' @param presence_penalty Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details). Default to 0.
    #' @param frequency_penalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details). Default to 0.
    #' @param logit_bias Modify the likelihood of specified tokens appearing in the completion.  Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
    #' @param user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
    #' @param ... Other optional arguments.
    #' @export
    initialize = function(`model`, `messages`, `functions` = NULL, `function_call` = NULL, `temperature` = 1, `top_p` = 1, `n` = 1, `stream` = FALSE, `stop` = NULL, `max_tokens` = NULL, `presence_penalty` = 0, `frequency_penalty` = 0, `logit_bias` = NULL, `user` = NULL, ...) {
      if (!missing(`model`)) {
        stopifnot(R6::is.R6(`model`))
        self$`model` <- `model`
      }
      if (!missing(`messages`)) {
        stopifnot(is.vector(`messages`), length(`messages`) != 0)
        sapply(`messages`, function(x) stopifnot(R6::is.R6(x)))
        self$`messages` <- `messages`
      }
      if (!is.null(`functions`)) {
        stopifnot(is.vector(`functions`), length(`functions`) != 0)
        sapply(`functions`, function(x) stopifnot(R6::is.R6(x)))
        self$`functions` <- `functions`
      }
      if (!is.null(`function_call`)) {
        stopifnot(R6::is.R6(`function_call`))
        self$`function_call` <- `function_call`
      }
      if (!is.null(`temperature`)) {
        self$`temperature` <- `temperature`
      }
      if (!is.null(`top_p`)) {
        self$`top_p` <- `top_p`
      }
      if (!is.null(`n`)) {
        if (!(is.numeric(`n`) && length(`n`) == 1)) {
          stop(paste("Error! Invalid data for `n`. Must be an integer:", `n`))
        }
        self$`n` <- `n`
      }
      if (!is.null(`stream`)) {
        if (!(is.logical(`stream`) && length(`stream`) == 1)) {
          stop(paste("Error! Invalid data for `stream`. Must be a boolean:", `stream`))
        }
        self$`stream` <- `stream`
      }
      if (!is.null(`stop`)) {
        stopifnot(R6::is.R6(`stop`))
        self$`stop` <- `stop`
      }
      if (!is.null(`max_tokens`)) {
        if (!(is.numeric(`max_tokens`) && length(`max_tokens`) == 1)) {
          stop(paste("Error! Invalid data for `max_tokens`. Must be an integer:", `max_tokens`))
        }
        self$`max_tokens` <- `max_tokens`
      }
      if (!is.null(`presence_penalty`)) {
        self$`presence_penalty` <- `presence_penalty`
      }
      if (!is.null(`frequency_penalty`)) {
        self$`frequency_penalty` <- `frequency_penalty`
      }
      if (!is.null(`logit_bias`)) {
        self$`logit_bias` <- `logit_bias`
      }
      if (!is.null(`user`)) {
        if (!(is.character(`user`) && length(`user`) == 1)) {
          stop(paste("Error! Invalid data for `user`. Must be a string:", `user`))
        }
        self$`user` <- `user`
      }
    },
    #' To JSON string
    #'
    #' @description
    #' To JSON String
    #'
    #' @return CreateChatCompletionRequest in JSON format
    #' @export
    toJSON = function() {
      CreateChatCompletionRequestObject <- list()
      if (!is.null(self$`model`)) {
        CreateChatCompletionRequestObject[["model"]] <-
          self$`model`$toJSON()
      }
      if (!is.null(self$`messages`)) {
        CreateChatCompletionRequestObject[["messages"]] <-
          lapply(self$`messages`, function(x) x$toJSON())
      }
      if (!is.null(self$`functions`)) {
        CreateChatCompletionRequestObject[["functions"]] <-
          lapply(self$`functions`, function(x) x$toJSON())
      }
      if (!is.null(self$`function_call`)) {
        CreateChatCompletionRequestObject[["function_call"]] <-
          self$`function_call`$toJSON()
      }
      if (!is.null(self$`temperature`)) {
        CreateChatCompletionRequestObject[["temperature"]] <-
          self$`temperature`
      }
      if (!is.null(self$`top_p`)) {
        CreateChatCompletionRequestObject[["top_p"]] <-
          self$`top_p`
      }
      if (!is.null(self$`n`)) {
        CreateChatCompletionRequestObject[["n"]] <-
          self$`n`
      }
      if (!is.null(self$`stream`)) {
        CreateChatCompletionRequestObject[["stream"]] <-
          self$`stream`
      }
      if (!is.null(self$`stop`)) {
        CreateChatCompletionRequestObject[["stop"]] <-
          self$`stop`$toJSON()
      }
      if (!is.null(self$`max_tokens`)) {
        CreateChatCompletionRequestObject[["max_tokens"]] <-
          self$`max_tokens`
      }
      if (!is.null(self$`presence_penalty`)) {
        CreateChatCompletionRequestObject[["presence_penalty"]] <-
          self$`presence_penalty`
      }
      if (!is.null(self$`frequency_penalty`)) {
        CreateChatCompletionRequestObject[["frequency_penalty"]] <-
          self$`frequency_penalty`
      }
      if (!is.null(self$`logit_bias`)) {
        CreateChatCompletionRequestObject[["logit_bias"]] <-
          self$`logit_bias`
      }
      if (!is.null(self$`user`)) {
        CreateChatCompletionRequestObject[["user"]] <-
          self$`user`
      }
      CreateChatCompletionRequestObject
    },
    #' Deserialize JSON string into an instance of CreateChatCompletionRequest
    #'
    #' @description
    #' Deserialize JSON string into an instance of CreateChatCompletionRequest
    #'
    #' @param input_json the JSON input
    #' @return the instance of CreateChatCompletionRequest
    #' @export
    fromJSON = function(input_json) {
      this_object <- jsonlite::fromJSON(input_json)
      if (!is.null(this_object$`model`)) {
        `model_object` <- CreateChatCompletionRequestModel$new()
        `model_object`$fromJSON(jsonlite::toJSON(this_object$`model`, auto_unbox = TRUE, digits = NA))
        self$`model` <- `model_object`
      }
      if (!is.null(this_object$`messages`)) {
        self$`messages` <- ApiClient$new()$deserializeObj(this_object$`messages`, "array[ChatCompletionRequestMessage]", loadNamespace("openapi"))
      }
      if (!is.null(this_object$`functions`)) {
        self$`functions` <- ApiClient$new()$deserializeObj(this_object$`functions`, "array[ChatCompletionFunctions]", loadNamespace("openapi"))
      }
      if (!is.null(this_object$`function_call`)) {
        `function_call_object` <- CreateChatCompletionRequestFunctionCall$new()
        `function_call_object`$fromJSON(jsonlite::toJSON(this_object$`function_call`, auto_unbox = TRUE, digits = NA))
        self$`function_call` <- `function_call_object`
      }
      if (!is.null(this_object$`temperature`)) {
        self$`temperature` <- this_object$`temperature`
      }
      if (!is.null(this_object$`top_p`)) {
        self$`top_p` <- this_object$`top_p`
      }
      if (!is.null(this_object$`n`)) {
        self$`n` <- this_object$`n`
      }
      if (!is.null(this_object$`stream`)) {
        self$`stream` <- this_object$`stream`
      }
      if (!is.null(this_object$`stop`)) {
        `stop_object` <- CreateChatCompletionRequestStop$new()
        `stop_object`$fromJSON(jsonlite::toJSON(this_object$`stop`, auto_unbox = TRUE, digits = NA))
        self$`stop` <- `stop_object`
      }
      if (!is.null(this_object$`max_tokens`)) {
        self$`max_tokens` <- this_object$`max_tokens`
      }
      if (!is.null(this_object$`presence_penalty`)) {
        self$`presence_penalty` <- this_object$`presence_penalty`
      }
      if (!is.null(this_object$`frequency_penalty`)) {
        self$`frequency_penalty` <- this_object$`frequency_penalty`
      }
      if (!is.null(this_object$`logit_bias`)) {
        self$`logit_bias` <- this_object$`logit_bias`
      }
      if (!is.null(this_object$`user`)) {
        self$`user` <- this_object$`user`
      }
      self
    },
    #' To JSON string
    #'
    #' @description
    #' To JSON String
    #'
    #' @return CreateChatCompletionRequest in JSON format
    #' @export
    toJSONString = function() {
      jsoncontent <- c(
        if (!is.null(self$`model`)) {
          sprintf(
          '"model":
          %s
          ',
          jsonlite::toJSON(self$`model`$toJSON(), auto_unbox = TRUE, digits = NA)
          )
        },
        if (!is.null(self$`messages`)) {
          sprintf(
          '"messages":
          [%s]
',
          paste(sapply(self$`messages`, function(x) jsonlite::toJSON(x$toJSON(), auto_unbox = TRUE, digits = NA)), collapse = ",")
          )
        },
        if (!is.null(self$`functions`)) {
          sprintf(
          '"functions":
          [%s]
',
          paste(sapply(self$`functions`, function(x) jsonlite::toJSON(x$toJSON(), auto_unbox = TRUE, digits = NA)), collapse = ",")
          )
        },
        if (!is.null(self$`function_call`)) {
          sprintf(
          '"function_call":
          %s
          ',
          jsonlite::toJSON(self$`function_call`$toJSON(), auto_unbox = TRUE, digits = NA)
          )
        },
        if (!is.null(self$`temperature`)) {
          sprintf(
          '"temperature":
            %d
                    ',
          self$`temperature`
          )
        },
        if (!is.null(self$`top_p`)) {
          sprintf(
          '"top_p":
            %d
                    ',
          self$`top_p`
          )
        },
        if (!is.null(self$`n`)) {
          sprintf(
          '"n":
            %d
                    ',
          self$`n`
          )
        },
        if (!is.null(self$`stream`)) {
          sprintf(
          '"stream":
            %s
                    ',
          tolower(self$`stream`)
          )
        },
        if (!is.null(self$`stop`)) {
          sprintf(
          '"stop":
          %s
          ',
          jsonlite::toJSON(self$`stop`$toJSON(), auto_unbox = TRUE, digits = NA)
          )
        },
        if (!is.null(self$`max_tokens`)) {
          sprintf(
          '"max_tokens":
            %d
                    ',
          self$`max_tokens`
          )
        },
        if (!is.null(self$`presence_penalty`)) {
          sprintf(
          '"presence_penalty":
            %d
                    ',
          self$`presence_penalty`
          )
        },
        if (!is.null(self$`frequency_penalty`)) {
          sprintf(
          '"frequency_penalty":
            %d
                    ',
          self$`frequency_penalty`
          )
        },
        if (!is.null(self$`logit_bias`)) {
          sprintf(
          '"logit_bias":
            "%s"
                    ',
          self$`logit_bias`
          )
        },
        if (!is.null(self$`user`)) {
          sprintf(
          '"user":
            "%s"
                    ',
          self$`user`
          )
        }
      )
      jsoncontent <- paste(jsoncontent, collapse = ",")
      json_string <- as.character(jsonlite::minify(paste("{", jsoncontent, "}", sep = "")))
    },
    #' Deserialize JSON string into an instance of CreateChatCompletionRequest
    #'
    #' @description
    #' Deserialize JSON string into an instance of CreateChatCompletionRequest
    #'
    #' @param input_json the JSON input
    #' @return the instance of CreateChatCompletionRequest
    #' @export
    fromJSONString = function(input_json) {
      this_object <- jsonlite::fromJSON(input_json)
      self$`model` <- CreateChatCompletionRequestModel$new()$fromJSON(jsonlite::toJSON(this_object$`model`, auto_unbox = TRUE, digits = NA))
      self$`messages` <- ApiClient$new()$deserializeObj(this_object$`messages`, "array[ChatCompletionRequestMessage]", loadNamespace("openapi"))
      self$`functions` <- ApiClient$new()$deserializeObj(this_object$`functions`, "array[ChatCompletionFunctions]", loadNamespace("openapi"))
      self$`function_call` <- CreateChatCompletionRequestFunctionCall$new()$fromJSON(jsonlite::toJSON(this_object$`function_call`, auto_unbox = TRUE, digits = NA))
      self$`temperature` <- this_object$`temperature`
      self$`top_p` <- this_object$`top_p`
      self$`n` <- this_object$`n`
      self$`stream` <- this_object$`stream`
      self$`stop` <- CreateChatCompletionRequestStop$new()$fromJSON(jsonlite::toJSON(this_object$`stop`, auto_unbox = TRUE, digits = NA))
      self$`max_tokens` <- this_object$`max_tokens`
      self$`presence_penalty` <- this_object$`presence_penalty`
      self$`frequency_penalty` <- this_object$`frequency_penalty`
      self$`logit_bias` <- this_object$`logit_bias`
      self$`user` <- this_object$`user`
      self
    },
    #' Validate JSON input with respect to CreateChatCompletionRequest
    #'
    #' @description
    #' Validate JSON input with respect to CreateChatCompletionRequest and throw an exception if invalid
    #'
    #' @param input the JSON input
    #' @export
    validateJSON = function(input) {
      input_json <- jsonlite::fromJSON(input)
      # check the required field `model`
      if (!is.null(input_json$`model`)) {
        stopifnot(R6::is.R6(input_json$`model`))
      } else {
        stop(paste("The JSON input `", input, "` is invalid for CreateChatCompletionRequest: the required field `model` is missing."))
      }
      # check the required field `messages`
      if (!is.null(input_json$`messages`)) {
        stopifnot(is.vector(input_json$`messages`), length(input_json$`messages`) != 0)
        tmp <- sapply(input_json$`messages`, function(x) stopifnot(R6::is.R6(x)))
      } else {
        stop(paste("The JSON input `", input, "` is invalid for CreateChatCompletionRequest: the required field `messages` is missing."))
      }
    },
    #' To string (JSON format)
    #'
    #' @description
    #' To string (JSON format)
    #'
    #' @return String representation of CreateChatCompletionRequest
    #' @export
    toString = function() {
      self$toJSONString()
    },
    #' Return true if the values in all fields are valid.
    #'
    #' @description
    #' Return true if the values in all fields are valid.
    #'
    #' @return true if the values in all fields are valid.
    #' @export
    isValid = function() {
      # check if the required `model` is null
      if (is.null(self$`model`)) {
        return(FALSE)
      }

      # check if the required `messages` is null
      if (is.null(self$`messages`)) {
        return(FALSE)
      }

      if (length(self$`messages`) < 1) {
        return(FALSE)
      }

      if (length(self$`functions`) < 1) {
        return(FALSE)
      }

      if (self$`temperature` > 2) {
        return(FALSE)
      }
      if (self$`temperature` < 0) {
        return(FALSE)
      }

      if (self$`top_p` > 1) {
        return(FALSE)
      }
      if (self$`top_p` < 0) {
        return(FALSE)
      }

      if (self$`n` > 128) {
        return(FALSE)
      }
      if (self$`n` < 1) {
        return(FALSE)
      }

      if (self$`presence_penalty` > 2) {
        return(FALSE)
      }
      if (self$`presence_penalty` < -2) {
        return(FALSE)
      }

      if (self$`frequency_penalty` > 2) {
        return(FALSE)
      }
      if (self$`frequency_penalty` < -2) {
        return(FALSE)
      }

      TRUE
    },
    #' Return a list of invalid fields (if any).
    #'
    #' @description
    #' Return a list of invalid fields (if any).
    #'
    #' @return A list of invalid fields (if any).
    #' @export
    getInvalidFields = function() {
      invalid_fields <- list()
      # check if the required `model` is null
      if (is.null(self$`model`)) {
        invalid_fields["model"] <- "Non-nullable required field `model` cannot be null."
      }

      # check if the required `messages` is null
      if (is.null(self$`messages`)) {
        invalid_fields["messages"] <- "Non-nullable required field `messages` cannot be null."
      }

      if (length(self$`messages`) < 1) {
        invalid_fields["messages"] <- "Invalid length for ``, number of items must be greater than or equal to 1."
      }

      if (length(self$`functions`) < 1) {
        invalid_fields["functions"] <- "Invalid length for ``, number of items must be greater than or equal to 1."
      }

      if (self$`temperature` > 2) {
        invalid_fields["temperature"] <- "Invalid value for `temperature`, must be smaller than or equal to 2."
      }
      if (self$`temperature` < 0) {
        invalid_fields["temperature"] <- "Invalid value for `temperature`, must be bigger than or equal to 0."
      }

      if (self$`top_p` > 1) {
        invalid_fields["top_p"] <- "Invalid value for `top_p`, must be smaller than or equal to 1."
      }
      if (self$`top_p` < 0) {
        invalid_fields["top_p"] <- "Invalid value for `top_p`, must be bigger than or equal to 0."
      }

      if (self$`n` > 128) {
        invalid_fields["n"] <- "Invalid value for `n`, must be smaller than or equal to 128."
      }
      if (self$`n` < 1) {
        invalid_fields["n"] <- "Invalid value for `n`, must be bigger than or equal to 1."
      }

      if (self$`presence_penalty` > 2) {
        invalid_fields["presence_penalty"] <- "Invalid value for `presence_penalty`, must be smaller than or equal to 2."
      }
      if (self$`presence_penalty` < -2) {
        invalid_fields["presence_penalty"] <- "Invalid value for `presence_penalty`, must be bigger than or equal to -2."
      }

      if (self$`frequency_penalty` > 2) {
        invalid_fields["frequency_penalty"] <- "Invalid value for `frequency_penalty`, must be smaller than or equal to 2."
      }
      if (self$`frequency_penalty` < -2) {
        invalid_fields["frequency_penalty"] <- "Invalid value for `frequency_penalty`, must be bigger than or equal to -2."
      }

      invalid_fields
    },
    #' Print the object
    #'
    #' @description
    #' Print the object
    #'
    #' @export
    print = function() {
      print(jsonlite::prettify(self$toJSONString()))
      invisible(self)
    }
  ),
  # Lock the class to prevent modifications to the method or field
  lock_class = TRUE
)
## Uncomment below to unlock the class to allow modifications of the method or field
# CreateChatCompletionRequest$unlock()
#
## Below is an example to define the print function
# CreateChatCompletionRequest$set("public", "print", function(...) {
#   print(jsonlite::prettify(self$toJSONString()))
#   invisible(self)
# })
## Uncomment below to lock the class to prevent modifications to the method or field
# CreateChatCompletionRequest$lock()

