#' Create a new CreateChatCompletionRequest
#'
#' @description
#' CreateChatCompletionRequest Class
#'
#' @docType class
#' @title CreateChatCompletionRequest
#' @description CreateChatCompletionRequest Class
#' @format An \code{R6Class} generator object
#' @field messages A list of messages comprising the conversation so far. Depending on the [model](/docs/models) you use, different message types (modalities) are supported, like [text](/docs/guides/text-generation), [images](/docs/guides/vision), and [audio](/docs/guides/audio). list(\link{ChatCompletionRequestMessage})
#' @field model  \link{CreateChatCompletionRequestModel}
#' @field store Whether or not to store the output of this chat completion request for  use in our [model distillation](/docs/guides/distillation) or [evals](/docs/guides/evals) products. character [optional]
#' @field reasoning_effort **o1 models only**   Constrains effort on reasoning for  [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response. character [optional]
#' @field metadata Developer-defined tags and values used for filtering completions in the [dashboard](https://platform.openai.com/chat-completions). named list(character) [optional]
#' @field frequency_penalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. numeric [optional]
#' @field logit_bias Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. named list(integer) [optional]
#' @field logprobs Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. character [optional]
#' @field top_logprobs An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used. integer [optional]
#' @field max_tokens The maximum number of [tokens](/tokenizer) that can be generated in the chat completion. This value can be used to control [costs](https://openai.com/api/pricing/) for text generated via API.  This value is now deprecated in favor of `max_completion_tokens`, and is not compatible with [o1 series models](/docs/guides/reasoning). integer [optional]
#' @field max_completion_tokens An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning). integer [optional]
#' @field n How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs. integer [optional]
#' @field modalities Output types that you would like the model to generate for this request. Most models are capable of generating text, which is the default:  `[\"text\"]`  The `gpt-4o-audio-preview` model can also be used to [generate audio](/docs/guides/audio). To request that this model generate both text and audio responses, you can use:  `[\"text\", \"audio\"]` list(character) [optional]
#' @field prediction  \link{PredictionContent} [optional]
#' @field audio  \link{CreateChatCompletionRequestAudio} [optional]
#' @field presence_penalty Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. numeric [optional]
#' @field response_format  \link{CreateChatCompletionRequestResponseFormat} [optional]
#' @field seed This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. integer [optional]
#' @field service_tier Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:    - If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.   - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - When not set, the default behavior is 'auto'.    When this parameter is set, the response body will include the `service_tier` utilized. character [optional]
#' @field stop  \link{CreateChatCompletionRequestStop} [optional]
#' @field stream If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). character [optional]
#' @field stream_options  \link{ChatCompletionStreamOptions} [optional]
#' @field temperature What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. numeric [optional]
#' @field top_p An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10\% probability mass are considered.  We generally recommend altering this or `temperature` but not both. numeric [optional]
#' @field tools A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. list(\link{ChatCompletionTool}) [optional]
#' @field tool_choice  \link{ChatCompletionToolChoiceOption} [optional]
#' @field parallel_tool_calls Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use. character [optional]
#' @field user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). character [optional]
#' @field function_call  \link{CreateChatCompletionRequestFunctionCall} [optional]
#' @field functions Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for. list(\link{ChatCompletionFunctions}) [optional]
#' @importFrom R6 R6Class
#' @importFrom jsonlite fromJSON toJSON
#' @export
CreateChatCompletionRequest <- R6::R6Class(
  "CreateChatCompletionRequest",
  public = list(
    `messages` = NULL,
    `model` = NULL,
    `store` = NULL,
    `reasoning_effort` = NULL,
    `metadata` = NULL,
    `frequency_penalty` = NULL,
    `logit_bias` = NULL,
    `logprobs` = NULL,
    `top_logprobs` = NULL,
    `max_tokens` = NULL,
    `max_completion_tokens` = NULL,
    `n` = NULL,
    `modalities` = NULL,
    `prediction` = NULL,
    `audio` = NULL,
    `presence_penalty` = NULL,
    `response_format` = NULL,
    `seed` = NULL,
    `service_tier` = NULL,
    `stop` = NULL,
    `stream` = NULL,
    `stream_options` = NULL,
    `temperature` = NULL,
    `top_p` = NULL,
    `tools` = NULL,
    `tool_choice` = NULL,
    `parallel_tool_calls` = NULL,
    `user` = NULL,
    `function_call` = NULL,
    `functions` = NULL,

    #' @description
    #' Initialize a new CreateChatCompletionRequest class.
    #'
    #' @param messages A list of messages comprising the conversation so far. Depending on the [model](/docs/models) you use, different message types (modalities) are supported, like [text](/docs/guides/text-generation), [images](/docs/guides/vision), and [audio](/docs/guides/audio).
    #' @param model model
    #' @param store Whether or not to store the output of this chat completion request for  use in our [model distillation](/docs/guides/distillation) or [evals](/docs/guides/evals) products.. Default to FALSE.
    #' @param reasoning_effort **o1 models only**   Constrains effort on reasoning for  [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.. Default to "medium".
    #' @param metadata Developer-defined tags and values used for filtering completions in the [dashboard](https://platform.openai.com/chat-completions).
    #' @param frequency_penalty Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.. Default to 0.
    #' @param logit_bias Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
    #' @param logprobs Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.. Default to FALSE.
    #' @param top_logprobs An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
    #' @param max_tokens The maximum number of [tokens](/tokenizer) that can be generated in the chat completion. This value can be used to control [costs](https://openai.com/api/pricing/) for text generated via API.  This value is now deprecated in favor of `max_completion_tokens`, and is not compatible with [o1 series models](/docs/guides/reasoning).
    #' @param max_completion_tokens An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).
    #' @param n How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.. Default to 1.
    #' @param modalities Output types that you would like the model to generate for this request. Most models are capable of generating text, which is the default:  `[\"text\"]`  The `gpt-4o-audio-preview` model can also be used to [generate audio](/docs/guides/audio). To request that this model generate both text and audio responses, you can use:  `[\"text\", \"audio\"]`
    #' @param prediction prediction
    #' @param audio audio
    #' @param presence_penalty Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.. Default to 0.
    #' @param response_format response_format
    #' @param seed This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
    #' @param service_tier Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:    - If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.   - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - When not set, the default behavior is 'auto'.    When this parameter is set, the response body will include the `service_tier` utilized.. Default to "auto".
    #' @param stop stop
    #' @param stream If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).. Default to FALSE.
    #' @param stream_options stream_options
    #' @param temperature What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.. Default to 1.
    #' @param top_p An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10\% probability mass are considered.  We generally recommend altering this or `temperature` but not both.. Default to 1.
    #' @param tools A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
    #' @param tool_choice tool_choice
    #' @param parallel_tool_calls Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.. Default to TRUE.
    #' @param user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).
    #' @param function_call function_call
    #' @param functions Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for.
    #' @param ... Other optional arguments.
    initialize = function(`messages`, `model`, `store` = FALSE, `reasoning_effort` = "medium", `metadata` = NULL, `frequency_penalty` = 0, `logit_bias` = NULL, `logprobs` = FALSE, `top_logprobs` = NULL, `max_tokens` = NULL, `max_completion_tokens` = NULL, `n` = 1, `modalities` = NULL, `prediction` = NULL, `audio` = NULL, `presence_penalty` = 0, `response_format` = NULL, `seed` = NULL, `service_tier` = "auto", `stop` = NULL, `stream` = FALSE, `stream_options` = NULL, `temperature` = 1, `top_p` = 1, `tools` = NULL, `tool_choice` = NULL, `parallel_tool_calls` = TRUE, `user` = NULL, `function_call` = NULL, `functions` = NULL, ...) {
      if (!missing(`messages`)) {
        stopifnot(is.vector(`messages`), length(`messages`) != 0)
        sapply(`messages`, function(x) stopifnot(R6::is.R6(x)))
        self$`messages` <- `messages`
      }
      if (!missing(`model`)) {
        stopifnot(R6::is.R6(`model`))
        self$`model` <- `model`
      }
      if (!is.null(`store`)) {
        if (!(is.logical(`store`) && length(`store`) == 1)) {
          stop(paste("Error! Invalid data for `store`. Must be a boolean:", `store`))
        }
        self$`store` <- `store`
      }
      if (!is.null(`reasoning_effort`)) {
        if (!(`reasoning_effort` %in% c("low", "medium", "high"))) {
          stop(paste("Error! \"", `reasoning_effort`, "\" cannot be assigned to `reasoning_effort`. Must be \"low\", \"medium\", \"high\".", sep = ""))
        }
        if (!(is.character(`reasoning_effort`) && length(`reasoning_effort`) == 1)) {
          stop(paste("Error! Invalid data for `reasoning_effort`. Must be a string:", `reasoning_effort`))
        }
        self$`reasoning_effort` <- `reasoning_effort`
      }
      if (!is.null(`metadata`)) {
        stopifnot(is.vector(`metadata`), length(`metadata`) != 0)
        sapply(`metadata`, function(x) stopifnot(is.character(x)))
        self$`metadata` <- `metadata`
      }
      if (!is.null(`frequency_penalty`)) {
        self$`frequency_penalty` <- `frequency_penalty`
      }
      if (!is.null(`logit_bias`)) {
        stopifnot(is.vector(`logit_bias`), length(`logit_bias`) != 0)
        sapply(`logit_bias`, function(x) stopifnot(is.character(x)))
        self$`logit_bias` <- `logit_bias`
      }
      if (!is.null(`logprobs`)) {
        if (!(is.logical(`logprobs`) && length(`logprobs`) == 1)) {
          stop(paste("Error! Invalid data for `logprobs`. Must be a boolean:", `logprobs`))
        }
        self$`logprobs` <- `logprobs`
      }
      if (!is.null(`top_logprobs`)) {
        if (!(is.numeric(`top_logprobs`) && length(`top_logprobs`) == 1)) {
          stop(paste("Error! Invalid data for `top_logprobs`. Must be an integer:", `top_logprobs`))
        }
        self$`top_logprobs` <- `top_logprobs`
      }
      if (!is.null(`max_tokens`)) {
        if (!(is.numeric(`max_tokens`) && length(`max_tokens`) == 1)) {
          stop(paste("Error! Invalid data for `max_tokens`. Must be an integer:", `max_tokens`))
        }
        self$`max_tokens` <- `max_tokens`
      }
      if (!is.null(`max_completion_tokens`)) {
        if (!(is.numeric(`max_completion_tokens`) && length(`max_completion_tokens`) == 1)) {
          stop(paste("Error! Invalid data for `max_completion_tokens`. Must be an integer:", `max_completion_tokens`))
        }
        self$`max_completion_tokens` <- `max_completion_tokens`
      }
      if (!is.null(`n`)) {
        if (!(is.numeric(`n`) && length(`n`) == 1)) {
          stop(paste("Error! Invalid data for `n`. Must be an integer:", `n`))
        }
        self$`n` <- `n`
      }
      if (!is.null(`modalities`)) {
        stopifnot(is.vector(`modalities`), length(`modalities`) != 0)
        sapply(`modalities`, function(x) stopifnot(is.character(x)))
        self$`modalities` <- `modalities`
      }
      if (!is.null(`prediction`)) {
        stopifnot(R6::is.R6(`prediction`))
        self$`prediction` <- `prediction`
      }
      if (!is.null(`audio`)) {
        stopifnot(R6::is.R6(`audio`))
        self$`audio` <- `audio`
      }
      if (!is.null(`presence_penalty`)) {
        self$`presence_penalty` <- `presence_penalty`
      }
      if (!is.null(`response_format`)) {
        stopifnot(R6::is.R6(`response_format`))
        self$`response_format` <- `response_format`
      }
      if (!is.null(`seed`)) {
        if (!(is.numeric(`seed`) && length(`seed`) == 1)) {
          stop(paste("Error! Invalid data for `seed`. Must be an integer:", `seed`))
        }
        self$`seed` <- `seed`
      }
      if (!is.null(`service_tier`)) {
        if (!(`service_tier` %in% c("auto", "default"))) {
          stop(paste("Error! \"", `service_tier`, "\" cannot be assigned to `service_tier`. Must be \"auto\", \"default\".", sep = ""))
        }
        if (!(is.character(`service_tier`) && length(`service_tier`) == 1)) {
          stop(paste("Error! Invalid data for `service_tier`. Must be a string:", `service_tier`))
        }
        self$`service_tier` <- `service_tier`
      }
      if (!is.null(`stop`)) {
        stopifnot(R6::is.R6(`stop`))
        self$`stop` <- `stop`
      }
      if (!is.null(`stream`)) {
        if (!(is.logical(`stream`) && length(`stream`) == 1)) {
          stop(paste("Error! Invalid data for `stream`. Must be a boolean:", `stream`))
        }
        self$`stream` <- `stream`
      }
      if (!is.null(`stream_options`)) {
        stopifnot(R6::is.R6(`stream_options`))
        self$`stream_options` <- `stream_options`
      }
      if (!is.null(`temperature`)) {
        self$`temperature` <- `temperature`
      }
      if (!is.null(`top_p`)) {
        self$`top_p` <- `top_p`
      }
      if (!is.null(`tools`)) {
        stopifnot(is.vector(`tools`), length(`tools`) != 0)
        sapply(`tools`, function(x) stopifnot(R6::is.R6(x)))
        self$`tools` <- `tools`
      }
      if (!is.null(`tool_choice`)) {
        stopifnot(R6::is.R6(`tool_choice`))
        self$`tool_choice` <- `tool_choice`
      }
      if (!is.null(`parallel_tool_calls`)) {
        if (!(is.logical(`parallel_tool_calls`) && length(`parallel_tool_calls`) == 1)) {
          stop(paste("Error! Invalid data for `parallel_tool_calls`. Must be a boolean:", `parallel_tool_calls`))
        }
        self$`parallel_tool_calls` <- `parallel_tool_calls`
      }
      if (!is.null(`user`)) {
        if (!(is.character(`user`) && length(`user`) == 1)) {
          stop(paste("Error! Invalid data for `user`. Must be a string:", `user`))
        }
        self$`user` <- `user`
      }
      if (!is.null(`function_call`)) {
        stopifnot(R6::is.R6(`function_call`))
        self$`function_call` <- `function_call`
      }
      if (!is.null(`functions`)) {
        stopifnot(is.vector(`functions`), length(`functions`) != 0)
        sapply(`functions`, function(x) stopifnot(R6::is.R6(x)))
        self$`functions` <- `functions`
      }
    },

    #' @description
    #' Convert to an R object. This method is deprecated. Use `toSimpleType()` instead.
    toJSON = function() {
      .Deprecated(new = "toSimpleType", msg = "Use the '$toSimpleType()' method instead since that is more clearly named. Use '$toJSONString()' to get a JSON string")
      return(self$toSimpleType())
    },

    #' @description
    #' Convert to a List
    #'
    #' Convert the R6 object to a list to work more easily with other tooling.
    #'
    #' @return CreateChatCompletionRequest as a base R list.
    #' @examples
    #' # convert array of CreateChatCompletionRequest (x) to a data frame
    #' \dontrun{
    #' library(purrr)
    #' library(tibble)
    #' df <- x |> map(\(y)y$toList()) |> map(as_tibble) |> list_rbind()
    #' df
    #' }
    toList = function() {
      return(self$toSimpleType())
    },

    #' @description
    #' Convert CreateChatCompletionRequest to a base R type
    #'
    #' @return A base R type, e.g. a list or numeric/character array.
    toSimpleType = function() {
      CreateChatCompletionRequestObject <- list()
      if (!is.null(self$`messages`)) {
        CreateChatCompletionRequestObject[["messages"]] <-
          lapply(self$`messages`, function(x) x$toSimpleType())
      }
      if (!is.null(self$`model`)) {
        CreateChatCompletionRequestObject[["model"]] <-
          self$`model`$toSimpleType()
      }
      if (!is.null(self$`store`)) {
        CreateChatCompletionRequestObject[["store"]] <-
          self$`store`
      }
      if (!is.null(self$`reasoning_effort`)) {
        CreateChatCompletionRequestObject[["reasoning_effort"]] <-
          self$`reasoning_effort`
      }
      if (!is.null(self$`metadata`)) {
        CreateChatCompletionRequestObject[["metadata"]] <-
          self$`metadata`
      }
      if (!is.null(self$`frequency_penalty`)) {
        CreateChatCompletionRequestObject[["frequency_penalty"]] <-
          self$`frequency_penalty`
      }
      if (!is.null(self$`logit_bias`)) {
        CreateChatCompletionRequestObject[["logit_bias"]] <-
          self$`logit_bias`
      }
      if (!is.null(self$`logprobs`)) {
        CreateChatCompletionRequestObject[["logprobs"]] <-
          self$`logprobs`
      }
      if (!is.null(self$`top_logprobs`)) {
        CreateChatCompletionRequestObject[["top_logprobs"]] <-
          self$`top_logprobs`
      }
      if (!is.null(self$`max_tokens`)) {
        CreateChatCompletionRequestObject[["max_tokens"]] <-
          self$`max_tokens`
      }
      if (!is.null(self$`max_completion_tokens`)) {
        CreateChatCompletionRequestObject[["max_completion_tokens"]] <-
          self$`max_completion_tokens`
      }
      if (!is.null(self$`n`)) {
        CreateChatCompletionRequestObject[["n"]] <-
          self$`n`
      }
      if (!is.null(self$`modalities`)) {
        CreateChatCompletionRequestObject[["modalities"]] <-
          self$`modalities`
      }
      if (!is.null(self$`prediction`)) {
        CreateChatCompletionRequestObject[["prediction"]] <-
          self$`prediction`$toSimpleType()
      }
      if (!is.null(self$`audio`)) {
        CreateChatCompletionRequestObject[["audio"]] <-
          self$`audio`$toSimpleType()
      }
      if (!is.null(self$`presence_penalty`)) {
        CreateChatCompletionRequestObject[["presence_penalty"]] <-
          self$`presence_penalty`
      }
      if (!is.null(self$`response_format`)) {
        CreateChatCompletionRequestObject[["response_format"]] <-
          self$`response_format`$toSimpleType()
      }
      if (!is.null(self$`seed`)) {
        CreateChatCompletionRequestObject[["seed"]] <-
          self$`seed`
      }
      if (!is.null(self$`service_tier`)) {
        CreateChatCompletionRequestObject[["service_tier"]] <-
          self$`service_tier`
      }
      if (!is.null(self$`stop`)) {
        CreateChatCompletionRequestObject[["stop"]] <-
          self$`stop`$toSimpleType()
      }
      if (!is.null(self$`stream`)) {
        CreateChatCompletionRequestObject[["stream"]] <-
          self$`stream`
      }
      if (!is.null(self$`stream_options`)) {
        CreateChatCompletionRequestObject[["stream_options"]] <-
          self$`stream_options`$toSimpleType()
      }
      if (!is.null(self$`temperature`)) {
        CreateChatCompletionRequestObject[["temperature"]] <-
          self$`temperature`
      }
      if (!is.null(self$`top_p`)) {
        CreateChatCompletionRequestObject[["top_p"]] <-
          self$`top_p`
      }
      if (!is.null(self$`tools`)) {
        CreateChatCompletionRequestObject[["tools"]] <-
          lapply(self$`tools`, function(x) x$toSimpleType())
      }
      if (!is.null(self$`tool_choice`)) {
        CreateChatCompletionRequestObject[["tool_choice"]] <-
          self$`tool_choice`$toSimpleType()
      }
      if (!is.null(self$`parallel_tool_calls`)) {
        CreateChatCompletionRequestObject[["parallel_tool_calls"]] <-
          self$`parallel_tool_calls`
      }
      if (!is.null(self$`user`)) {
        CreateChatCompletionRequestObject[["user"]] <-
          self$`user`
      }
      if (!is.null(self$`function_call`)) {
        CreateChatCompletionRequestObject[["function_call"]] <-
          self$`function_call`$toSimpleType()
      }
      if (!is.null(self$`functions`)) {
        CreateChatCompletionRequestObject[["functions"]] <-
          lapply(self$`functions`, function(x) x$toSimpleType())
      }
      return(CreateChatCompletionRequestObject)
    },

    #' @description
    #' Deserialize JSON string into an instance of CreateChatCompletionRequest
    #'
    #' @param input_json the JSON input
    #' @return the instance of CreateChatCompletionRequest
    fromJSON = function(input_json) {
      this_object <- jsonlite::fromJSON(input_json)
      if (!is.null(this_object$`messages`)) {
        self$`messages` <- ApiClient$new()$deserializeObj(this_object$`messages`, "array[ChatCompletionRequestMessage]", loadNamespace("openapi"))
      }
      if (!is.null(this_object$`model`)) {
        `model_object` <- CreateChatCompletionRequestModel$new()
        `model_object`$fromJSON(jsonlite::toJSON(this_object$`model`, auto_unbox = TRUE, digits = NA))
        self$`model` <- `model_object`
      }
      if (!is.null(this_object$`store`)) {
        self$`store` <- this_object$`store`
      }
      if (!is.null(this_object$`reasoning_effort`)) {
        if (!is.null(this_object$`reasoning_effort`) && !(this_object$`reasoning_effort` %in% c("low", "medium", "high"))) {
          stop(paste("Error! \"", this_object$`reasoning_effort`, "\" cannot be assigned to `reasoning_effort`. Must be \"low\", \"medium\", \"high\".", sep = ""))
        }
        self$`reasoning_effort` <- this_object$`reasoning_effort`
      }
      if (!is.null(this_object$`metadata`)) {
        self$`metadata` <- ApiClient$new()$deserializeObj(this_object$`metadata`, "map(character)", loadNamespace("openapi"))
      }
      if (!is.null(this_object$`frequency_penalty`)) {
        self$`frequency_penalty` <- this_object$`frequency_penalty`
      }
      if (!is.null(this_object$`logit_bias`)) {
        self$`logit_bias` <- ApiClient$new()$deserializeObj(this_object$`logit_bias`, "map(integer)", loadNamespace("openapi"))
      }
      if (!is.null(this_object$`logprobs`)) {
        self$`logprobs` <- this_object$`logprobs`
      }
      if (!is.null(this_object$`top_logprobs`)) {
        self$`top_logprobs` <- this_object$`top_logprobs`
      }
      if (!is.null(this_object$`max_tokens`)) {
        self$`max_tokens` <- this_object$`max_tokens`
      }
      if (!is.null(this_object$`max_completion_tokens`)) {
        self$`max_completion_tokens` <- this_object$`max_completion_tokens`
      }
      if (!is.null(this_object$`n`)) {
        self$`n` <- this_object$`n`
      }
      if (!is.null(this_object$`modalities`)) {
        self$`modalities` <- ApiClient$new()$deserializeObj(this_object$`modalities`, "array[character]", loadNamespace("openapi"))
      }
      if (!is.null(this_object$`prediction`)) {
        `prediction_object` <- PredictionContent$new()
        `prediction_object`$fromJSON(jsonlite::toJSON(this_object$`prediction`, auto_unbox = TRUE, digits = NA))
        self$`prediction` <- `prediction_object`
      }
      if (!is.null(this_object$`audio`)) {
        `audio_object` <- CreateChatCompletionRequestAudio$new()
        `audio_object`$fromJSON(jsonlite::toJSON(this_object$`audio`, auto_unbox = TRUE, digits = NA))
        self$`audio` <- `audio_object`
      }
      if (!is.null(this_object$`presence_penalty`)) {
        self$`presence_penalty` <- this_object$`presence_penalty`
      }
      if (!is.null(this_object$`response_format`)) {
        `response_format_object` <- CreateChatCompletionRequestResponseFormat$new()
        `response_format_object`$fromJSON(jsonlite::toJSON(this_object$`response_format`, auto_unbox = TRUE, digits = NA))
        self$`response_format` <- `response_format_object`
      }
      if (!is.null(this_object$`seed`)) {
        self$`seed` <- this_object$`seed`
      }
      if (!is.null(this_object$`service_tier`)) {
        if (!is.null(this_object$`service_tier`) && !(this_object$`service_tier` %in% c("auto", "default"))) {
          stop(paste("Error! \"", this_object$`service_tier`, "\" cannot be assigned to `service_tier`. Must be \"auto\", \"default\".", sep = ""))
        }
        self$`service_tier` <- this_object$`service_tier`
      }
      if (!is.null(this_object$`stop`)) {
        `stop_object` <- CreateChatCompletionRequestStop$new()
        `stop_object`$fromJSON(jsonlite::toJSON(this_object$`stop`, auto_unbox = TRUE, digits = NA))
        self$`stop` <- `stop_object`
      }
      if (!is.null(this_object$`stream`)) {
        self$`stream` <- this_object$`stream`
      }
      if (!is.null(this_object$`stream_options`)) {
        `stream_options_object` <- ChatCompletionStreamOptions$new()
        `stream_options_object`$fromJSON(jsonlite::toJSON(this_object$`stream_options`, auto_unbox = TRUE, digits = NA))
        self$`stream_options` <- `stream_options_object`
      }
      if (!is.null(this_object$`temperature`)) {
        self$`temperature` <- this_object$`temperature`
      }
      if (!is.null(this_object$`top_p`)) {
        self$`top_p` <- this_object$`top_p`
      }
      if (!is.null(this_object$`tools`)) {
        self$`tools` <- ApiClient$new()$deserializeObj(this_object$`tools`, "array[ChatCompletionTool]", loadNamespace("openapi"))
      }
      if (!is.null(this_object$`tool_choice`)) {
        `tool_choice_object` <- ChatCompletionToolChoiceOption$new()
        `tool_choice_object`$fromJSON(jsonlite::toJSON(this_object$`tool_choice`, auto_unbox = TRUE, digits = NA))
        self$`tool_choice` <- `tool_choice_object`
      }
      if (!is.null(this_object$`parallel_tool_calls`)) {
        self$`parallel_tool_calls` <- this_object$`parallel_tool_calls`
      }
      if (!is.null(this_object$`user`)) {
        self$`user` <- this_object$`user`
      }
      if (!is.null(this_object$`function_call`)) {
        `function_call_object` <- CreateChatCompletionRequestFunctionCall$new()
        `function_call_object`$fromJSON(jsonlite::toJSON(this_object$`function_call`, auto_unbox = TRUE, digits = NA))
        self$`function_call` <- `function_call_object`
      }
      if (!is.null(this_object$`functions`)) {
        self$`functions` <- ApiClient$new()$deserializeObj(this_object$`functions`, "array[ChatCompletionFunctions]", loadNamespace("openapi"))
      }
      self
    },

    #' @description
    #' To JSON String
    #' 
    #' @param ... Parameters passed to `jsonlite::toJSON`
    #' @return CreateChatCompletionRequest in JSON format
    toJSONString = function(...) {
      simple <- self$toSimpleType()
      json <- jsonlite::toJSON(simple, auto_unbox = TRUE, digits = NA, ...)
      return(as.character(jsonlite::minify(json)))
    },

    #' @description
    #' Deserialize JSON string into an instance of CreateChatCompletionRequest
    #'
    #' @param input_json the JSON input
    #' @return the instance of CreateChatCompletionRequest
    fromJSONString = function(input_json) {
      this_object <- jsonlite::fromJSON(input_json)
      self$`messages` <- ApiClient$new()$deserializeObj(this_object$`messages`, "array[ChatCompletionRequestMessage]", loadNamespace("openapi"))
      self$`model` <- CreateChatCompletionRequestModel$new()$fromJSON(jsonlite::toJSON(this_object$`model`, auto_unbox = TRUE, digits = NA))
      self$`store` <- this_object$`store`
      if (!is.null(this_object$`reasoning_effort`) && !(this_object$`reasoning_effort` %in% c("low", "medium", "high"))) {
        stop(paste("Error! \"", this_object$`reasoning_effort`, "\" cannot be assigned to `reasoning_effort`. Must be \"low\", \"medium\", \"high\".", sep = ""))
      }
      self$`reasoning_effort` <- this_object$`reasoning_effort`
      self$`metadata` <- ApiClient$new()$deserializeObj(this_object$`metadata`, "map(character)", loadNamespace("openapi"))
      self$`frequency_penalty` <- this_object$`frequency_penalty`
      self$`logit_bias` <- ApiClient$new()$deserializeObj(this_object$`logit_bias`, "map(integer)", loadNamespace("openapi"))
      self$`logprobs` <- this_object$`logprobs`
      self$`top_logprobs` <- this_object$`top_logprobs`
      self$`max_tokens` <- this_object$`max_tokens`
      self$`max_completion_tokens` <- this_object$`max_completion_tokens`
      self$`n` <- this_object$`n`
      self$`modalities` <- ApiClient$new()$deserializeObj(this_object$`modalities`, "array[character]", loadNamespace("openapi"))
      self$`prediction` <- PredictionContent$new()$fromJSON(jsonlite::toJSON(this_object$`prediction`, auto_unbox = TRUE, digits = NA))
      self$`audio` <- CreateChatCompletionRequestAudio$new()$fromJSON(jsonlite::toJSON(this_object$`audio`, auto_unbox = TRUE, digits = NA))
      self$`presence_penalty` <- this_object$`presence_penalty`
      self$`response_format` <- CreateChatCompletionRequestResponseFormat$new()$fromJSON(jsonlite::toJSON(this_object$`response_format`, auto_unbox = TRUE, digits = NA))
      self$`seed` <- this_object$`seed`
      if (!is.null(this_object$`service_tier`) && !(this_object$`service_tier` %in% c("auto", "default"))) {
        stop(paste("Error! \"", this_object$`service_tier`, "\" cannot be assigned to `service_tier`. Must be \"auto\", \"default\".", sep = ""))
      }
      self$`service_tier` <- this_object$`service_tier`
      self$`stop` <- CreateChatCompletionRequestStop$new()$fromJSON(jsonlite::toJSON(this_object$`stop`, auto_unbox = TRUE, digits = NA))
      self$`stream` <- this_object$`stream`
      self$`stream_options` <- ChatCompletionStreamOptions$new()$fromJSON(jsonlite::toJSON(this_object$`stream_options`, auto_unbox = TRUE, digits = NA))
      self$`temperature` <- this_object$`temperature`
      self$`top_p` <- this_object$`top_p`
      self$`tools` <- ApiClient$new()$deserializeObj(this_object$`tools`, "array[ChatCompletionTool]", loadNamespace("openapi"))
      self$`tool_choice` <- ChatCompletionToolChoiceOption$new()$fromJSON(jsonlite::toJSON(this_object$`tool_choice`, auto_unbox = TRUE, digits = NA))
      self$`parallel_tool_calls` <- this_object$`parallel_tool_calls`
      self$`user` <- this_object$`user`
      self$`function_call` <- CreateChatCompletionRequestFunctionCall$new()$fromJSON(jsonlite::toJSON(this_object$`function_call`, auto_unbox = TRUE, digits = NA))
      self$`functions` <- ApiClient$new()$deserializeObj(this_object$`functions`, "array[ChatCompletionFunctions]", loadNamespace("openapi"))
      self
    },

    #' @description
    #' Validate JSON input with respect to CreateChatCompletionRequest and throw an exception if invalid
    #'
    #' @param input the JSON input
    validateJSON = function(input) {
      input_json <- jsonlite::fromJSON(input)
      # check the required field `messages`
      if (!is.null(input_json$`messages`)) {
        stopifnot(is.vector(input_json$`messages`), length(input_json$`messages`) != 0)
        tmp <- sapply(input_json$`messages`, function(x) stopifnot(R6::is.R6(x)))
      } else {
        stop(paste("The JSON input `", input, "` is invalid for CreateChatCompletionRequest: the required field `messages` is missing."))
      }
      # check the required field `model`
      if (!is.null(input_json$`model`)) {
        stopifnot(R6::is.R6(input_json$`model`))
      } else {
        stop(paste("The JSON input `", input, "` is invalid for CreateChatCompletionRequest: the required field `model` is missing."))
      }
    },

    #' @description
    #' To string (JSON format)
    #'
    #' @return String representation of CreateChatCompletionRequest
    toString = function() {
      self$toJSONString()
    },

    #' @description
    #' Return true if the values in all fields are valid.
    #'
    #' @return true if the values in all fields are valid.
    isValid = function() {
      # check if the required `messages` is null
      if (is.null(self$`messages`)) {
        return(FALSE)
      }

      if (length(self$`messages`) < 1) {
        return(FALSE)
      }

      # check if the required `model` is null
      if (is.null(self$`model`)) {
        return(FALSE)
      }

      if (self$`frequency_penalty` > 2) {
        return(FALSE)
      }
      if (self$`frequency_penalty` < -2) {
        return(FALSE)
      }

      if (self$`top_logprobs` > 20) {
        return(FALSE)
      }
      if (self$`top_logprobs` < 0) {
        return(FALSE)
      }

      if (self$`n` > 128) {
        return(FALSE)
      }
      if (self$`n` < 1) {
        return(FALSE)
      }

      if (self$`presence_penalty` > 2) {
        return(FALSE)
      }
      if (self$`presence_penalty` < -2) {
        return(FALSE)
      }

      if (self$`seed` > 9223372036854776000) {
        return(FALSE)
      }
      if (self$`seed` < -9223372036854776000) {
        return(FALSE)
      }

      if (self$`temperature` > 2) {
        return(FALSE)
      }
      if (self$`temperature` < 0) {
        return(FALSE)
      }

      if (self$`top_p` > 1) {
        return(FALSE)
      }
      if (self$`top_p` < 0) {
        return(FALSE)
      }

      if (length(self$`functions`) > 128) {
        return(FALSE)
      }
      if (length(self$`functions`) < 1) {
        return(FALSE)
      }

      TRUE
    },

    #' @description
    #' Return a list of invalid fields (if any).
    #'
    #' @return A list of invalid fields (if any).
    getInvalidFields = function() {
      invalid_fields <- list()
      # check if the required `messages` is null
      if (is.null(self$`messages`)) {
        invalid_fields["messages"] <- "Non-nullable required field `messages` cannot be null."
      }

      if (length(self$`messages`) < 1) {
        invalid_fields["messages"] <- "Invalid length for ``, number of items must be greater than or equal to 1."
      }

      # check if the required `model` is null
      if (is.null(self$`model`)) {
        invalid_fields["model"] <- "Non-nullable required field `model` cannot be null."
      }

      if (self$`frequency_penalty` > 2) {
        invalid_fields["frequency_penalty"] <- "Invalid value for `frequency_penalty`, must be smaller than or equal to 2."
      }
      if (self$`frequency_penalty` < -2) {
        invalid_fields["frequency_penalty"] <- "Invalid value for `frequency_penalty`, must be bigger than or equal to -2."
      }

      if (self$`top_logprobs` > 20) {
        invalid_fields["top_logprobs"] <- "Invalid value for `top_logprobs`, must be smaller than or equal to 20."
      }
      if (self$`top_logprobs` < 0) {
        invalid_fields["top_logprobs"] <- "Invalid value for `top_logprobs`, must be bigger than or equal to 0."
      }

      if (self$`n` > 128) {
        invalid_fields["n"] <- "Invalid value for `n`, must be smaller than or equal to 128."
      }
      if (self$`n` < 1) {
        invalid_fields["n"] <- "Invalid value for `n`, must be bigger than or equal to 1."
      }

      if (self$`presence_penalty` > 2) {
        invalid_fields["presence_penalty"] <- "Invalid value for `presence_penalty`, must be smaller than or equal to 2."
      }
      if (self$`presence_penalty` < -2) {
        invalid_fields["presence_penalty"] <- "Invalid value for `presence_penalty`, must be bigger than or equal to -2."
      }

      if (self$`seed` > 9223372036854776000) {
        invalid_fields["seed"] <- "Invalid value for `seed`, must be smaller than or equal to 9223372036854776000."
      }
      if (self$`seed` < -9223372036854776000) {
        invalid_fields["seed"] <- "Invalid value for `seed`, must be bigger than or equal to -9223372036854776000."
      }

      if (self$`temperature` > 2) {
        invalid_fields["temperature"] <- "Invalid value for `temperature`, must be smaller than or equal to 2."
      }
      if (self$`temperature` < 0) {
        invalid_fields["temperature"] <- "Invalid value for `temperature`, must be bigger than or equal to 0."
      }

      if (self$`top_p` > 1) {
        invalid_fields["top_p"] <- "Invalid value for `top_p`, must be smaller than or equal to 1."
      }
      if (self$`top_p` < 0) {
        invalid_fields["top_p"] <- "Invalid value for `top_p`, must be bigger than or equal to 0."
      }

      if (length(self$`functions`) > 128) {
        invalid_fields["functions"] <- "Invalid length for `functions`, number of items must be less than or equal to 128."
      }
      if (length(self$`functions`) < 1) {
        invalid_fields["functions"] <- "Invalid length for ``, number of items must be greater than or equal to 1."
      }

      invalid_fields
    },

    #' @description
    #' Print the object
    print = function() {
      print(jsonlite::prettify(self$toJSONString()))
      invisible(self)
    }
  ),
  # Lock the class to prevent modifications to the method or field
  lock_class = TRUE
)
## Uncomment below to unlock the class to allow modifications of the method or field
# CreateChatCompletionRequest$unlock()
#
## Below is an example to define the print function
# CreateChatCompletionRequest$set("public", "print", function(...) {
#   print(jsonlite::prettify(self$toJSONString()))
#   invisible(self)
# })
## Uncomment below to lock the class to prevent modifications to the method or field
# CreateChatCompletionRequest$lock()

