# Automatically generated by openapi-generator (https://openapi-generator.tech)
# Please update as you see appropriate

context("Test CreateFineTuningJobRequest")

model_instance <- CreateFineTuningJobRequest$new()

test_that("model", {
  # tests for the property `model` (CreateFineTuningJobRequestModel)

  # uncomment below to test the property
  #expect_equal(model.instance$`model`, "EXPECTED_RESULT")
})

test_that("training_file", {
  # tests for the property `training_file` (character)
  # The ID of an uploaded file that contains training data.  See [upload file](/docs/api-reference/files/create) for how to upload a file.  Your dataset must be formatted as a JSONL file. Additionally, you must upload your file with the purpose &#x60;fine-tune&#x60;.  The contents of the file should differ depending on if the model uses the [chat](/docs/api-reference/fine-tuning/chat-input), [completions](/docs/api-reference/fine-tuning/completions-input) format, or if the fine-tuning method uses the [preference](/docs/api-reference/fine-tuning/preference-input) format.  See the [fine-tuning guide](/docs/guides/fine-tuning) for more details. 

  # uncomment below to test the property
  #expect_equal(model.instance$`training_file`, "EXPECTED_RESULT")
})

test_that("hyperparameters", {
  # tests for the property `hyperparameters` (CreateFineTuningJobRequestHyperparameters)

  # uncomment below to test the property
  #expect_equal(model.instance$`hyperparameters`, "EXPECTED_RESULT")
})

test_that("suffix", {
  # tests for the property `suffix` (character)
  # A string of up to 64 characters that will be added to your fine-tuned model name.  For example, a &#x60;suffix&#x60; of \&quot;custom-model-name\&quot; would produce a model name like &#x60;ft:gpt-4o-mini:openai:custom-model-name:7p4lURel&#x60;. 

  # uncomment below to test the property
  #expect_equal(model.instance$`suffix`, "EXPECTED_RESULT")
})

test_that("validation_file", {
  # tests for the property `validation_file` (character)
  # The ID of an uploaded file that contains validation data.  If you provide this file, the data is used to generate validation metrics periodically during fine-tuning. These metrics can be viewed in the fine-tuning results file. The same data should not be present in both train and validation files.  Your dataset must be formatted as a JSONL file. You must upload your file with the purpose &#x60;fine-tune&#x60;.  See the [fine-tuning guide](/docs/guides/fine-tuning) for more details. 

  # uncomment below to test the property
  #expect_equal(model.instance$`validation_file`, "EXPECTED_RESULT")
})

test_that("integrations", {
  # tests for the property `integrations` (array[CreateFineTuningJobRequestIntegrationsInner])
  # A list of integrations to enable for your fine-tuning job.

  # uncomment below to test the property
  #expect_equal(model.instance$`integrations`, "EXPECTED_RESULT")
})

test_that("seed", {
  # tests for the property `seed` (integer)
  # The seed controls the reproducibility of the job. Passing in the same seed and job parameters should produce the same results, but may differ in rare cases. If a seed is not specified, one will be generated for you. 

  # uncomment below to test the property
  #expect_equal(model.instance$`seed`, "EXPECTED_RESULT")
})

test_that("method", {
  # tests for the property `method` (FineTuneMethod)

  # uncomment below to test the property
  #expect_equal(model.instance$`method`, "EXPECTED_RESULT")
})
