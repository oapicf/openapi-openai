/**
 * OpenAI API
 * APIs for sampling from and fine-tuning language models
 *
 * The version of the OpenAPI document: 2.0.0
 * Contact: blah+oapicf@cliffano.com
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */
package org.openapitools.client.api

import org.openapitools.client.model.CreateChatCompletionRequest
import org.openapitools.client.model.CreateChatCompletionResponse
import org.openapitools.client.model.CreateCompletionRequest
import org.openapitools.client.model.CreateCompletionResponse
import org.openapitools.client.model.CreateEditRequest
import org.openapitools.client.model.CreateEditResponse
import org.openapitools.client.model.CreateEmbeddingRequest
import org.openapitools.client.model.CreateEmbeddingResponse
import org.openapitools.client.model.CreateFineTuneRequest
import org.openapitools.client.model.CreateImageRequest
import org.openapitools.client.model.CreateModerationRequest
import org.openapitools.client.model.CreateModerationResponse
import org.openapitools.client.model.CreateTranscriptionRequestModel
import org.openapitools.client.model.CreateTranscriptionResponse
import org.openapitools.client.model.CreateTranslationResponse
import org.openapitools.client.model.DeleteFileResponse
import org.openapitools.client.model.DeleteModelResponse
import java.io.File
import org.openapitools.client.model.FineTune
import org.openapitools.client.model.ImagesResponse
import org.openapitools.client.model.ListFilesResponse
import org.openapitools.client.model.ListFineTuneEventsResponse
import org.openapitools.client.model.ListFineTunesResponse
import org.openapitools.client.model.ListModelsResponse
import org.openapitools.client.model.Model
import org.openapitools.client.model.OpenAIFile
import org.openapitools.client.core.JsonSupport._
import sttp.client3._
import sttp.model.Method

object OpenAIApi {

def apply(baseUrl: String = "https://api.openai.com/v1") = new OpenAIApi(baseUrl)
}

class OpenAIApi(baseUrl: String) {

  /**
   * Expected answers:
   *   code 200 : FineTune (OK)
   * 
   * @param fineTuneId The ID of the fine-tune job to cancel 
   */
  def cancelFineTune(fineTuneId: String
): Request[Either[ResponseException[String, Exception], FineTune], Any] =
    basicRequest
      .method(Method.POST, uri"$baseUrl/fine-tunes/${fineTuneId}/cancel")
      .contentType("application/json")
      .response(asJson[FineTune])

  /**
   * Expected answers:
   *   code 200 : CreateChatCompletionResponse (OK)
   * 
   * @param createChatCompletionRequest 
   */
  def createChatCompletion(createChatCompletionRequest: CreateChatCompletionRequest
): Request[Either[ResponseException[String, Exception], CreateChatCompletionResponse], Any] =
    basicRequest
      .method(Method.POST, uri"$baseUrl/chat/completions")
      .contentType("application/json")
      .body(createChatCompletionRequest)
      .response(asJson[CreateChatCompletionResponse])

  /**
   * Expected answers:
   *   code 200 : CreateCompletionResponse (OK)
   * 
   * @param createCompletionRequest 
   */
  def createCompletion(createCompletionRequest: CreateCompletionRequest
): Request[Either[ResponseException[String, Exception], CreateCompletionResponse], Any] =
    basicRequest
      .method(Method.POST, uri"$baseUrl/completions")
      .contentType("application/json")
      .body(createCompletionRequest)
      .response(asJson[CreateCompletionResponse])

  /**
   * Expected answers:
   *   code 200 : CreateEditResponse (OK)
   * 
   * @param createEditRequest 
   */
  def createEdit(createEditRequest: CreateEditRequest
): Request[Either[ResponseException[String, Exception], CreateEditResponse], Any] =
    basicRequest
      .method(Method.POST, uri"$baseUrl/edits")
      .contentType("application/json")
      .body(createEditRequest)
      .response(asJson[CreateEditResponse])

  /**
   * Expected answers:
   *   code 200 : CreateEmbeddingResponse (OK)
   * 
   * @param createEmbeddingRequest 
   */
  def createEmbedding(createEmbeddingRequest: CreateEmbeddingRequest
): Request[Either[ResponseException[String, Exception], CreateEmbeddingResponse], Any] =
    basicRequest
      .method(Method.POST, uri"$baseUrl/embeddings")
      .contentType("application/json")
      .body(createEmbeddingRequest)
      .response(asJson[CreateEmbeddingResponse])

  /**
   * Expected answers:
   *   code 200 : OpenAIFile (OK)
   * 
   * @param file Name of the [JSON Lines](https://jsonlines.readthedocs.io/en/latest/) file to be uploaded.  If the `purpose` is set to \\\"fine-tune\\\", each line is a JSON record with \\\"prompt\\\" and \\\"completion\\\" fields representing your [training examples](/docs/guides/fine-tuning/prepare-training-data). 
   * @param purpose The intended purpose of the uploaded documents.  Use \\\"fine-tune\\\" for [Fine-tuning](/docs/api-reference/fine-tunes). This allows us to validate the format of the uploaded file. 
   */
  def createFile(file: File, purpose: String
): Request[Either[ResponseException[String, Exception], OpenAIFile], Any] =
    basicRequest
      .method(Method.POST, uri"$baseUrl/files")
      .contentType("multipart/form-data")
      .multipartBody(Seq(
                multipartFile("file", file)
, 
                multipart("purpose", purpose)

      ).flatten)
      .response(asJson[OpenAIFile])

  /**
   * Expected answers:
   *   code 200 : FineTune (OK)
   * 
   * @param createFineTuneRequest 
   */
  def createFineTune(createFineTuneRequest: CreateFineTuneRequest
): Request[Either[ResponseException[String, Exception], FineTune], Any] =
    basicRequest
      .method(Method.POST, uri"$baseUrl/fine-tunes")
      .contentType("application/json")
      .body(createFineTuneRequest)
      .response(asJson[FineTune])

  /**
   * Expected answers:
   *   code 200 : ImagesResponse (OK)
   * 
   * @param createImageRequest 
   */
  def createImage(createImageRequest: CreateImageRequest
): Request[Either[ResponseException[String, Exception], ImagesResponse], Any] =
    basicRequest
      .method(Method.POST, uri"$baseUrl/images/generations")
      .contentType("application/json")
      .body(createImageRequest)
      .response(asJson[ImagesResponse])

  /**
   * Expected answers:
   *   code 200 : ImagesResponse (OK)
   * 
   * @param image The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask is not provided, image must have transparency, which will be used as the mask.
   * @param prompt A text description of the desired image(s). The maximum length is 1000 characters.
   * @param mask An additional image whose fully transparent areas (e.g. where alpha is zero) indicate where `image` should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions as `image`.
   * @param `n` The number of images to generate. Must be between 1 and 10.
   * @param size The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`.
   * @param responseFormat The format in which the generated images are returned. Must be one of `url` or `b64_json`.
   * @param user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). 
   */
  def createImageEdit(image: File, prompt: String, mask: Option[File] = None, `n`: Option[Int] = None, size: Option[String] = None, responseFormat: Option[String] = None, user: Option[String] = None
): Request[Either[ResponseException[String, Exception], ImagesResponse], Any] =
    basicRequest
      .method(Method.POST, uri"$baseUrl/images/edits")
      .contentType("multipart/form-data")
      .multipartBody(Seq(
                multipartFile("image", image)
, 
                mask.map(multipartFile("mask", _))
, 
                multipart("prompt", prompt)
, 
                `n`.map(multipart("n", _))
, 
                size.map(multipart("size", _))
, 
                responseFormat.map(multipart("response_format", _))
, 
                user.map(multipart("user", _))

      ).flatten)
      .response(asJson[ImagesResponse])

  /**
   * Expected answers:
   *   code 200 : ImagesResponse (OK)
   * 
   * @param image The image to use as the basis for the variation(s). Must be a valid PNG file, less than 4MB, and square.
   * @param `n` The number of images to generate. Must be between 1 and 10.
   * @param size The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`.
   * @param responseFormat The format in which the generated images are returned. Must be one of `url` or `b64_json`.
   * @param user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). 
   */
  def createImageVariation(image: File, `n`: Option[Int] = None, size: Option[String] = None, responseFormat: Option[String] = None, user: Option[String] = None
): Request[Either[ResponseException[String, Exception], ImagesResponse], Any] =
    basicRequest
      .method(Method.POST, uri"$baseUrl/images/variations")
      .contentType("multipart/form-data")
      .multipartBody(Seq(
                multipartFile("image", image)
, 
                `n`.map(multipart("n", _))
, 
                size.map(multipart("size", _))
, 
                responseFormat.map(multipart("response_format", _))
, 
                user.map(multipart("user", _))

      ).flatten)
      .response(asJson[ImagesResponse])

  /**
   * Expected answers:
   *   code 200 : CreateModerationResponse (OK)
   * 
   * @param createModerationRequest 
   */
  def createModeration(createModerationRequest: CreateModerationRequest
): Request[Either[ResponseException[String, Exception], CreateModerationResponse], Any] =
    basicRequest
      .method(Method.POST, uri"$baseUrl/moderations")
      .contentType("application/json")
      .body(createModerationRequest)
      .response(asJson[CreateModerationResponse])

  /**
   * Expected answers:
   *   code 200 : CreateTranscriptionResponse (OK)
   * 
   * @param file The audio file object (not file name) to transcribe, in one of these formats: mp3, mp4, mpeg, mpga, m4a, wav, or webm. 
   * @param model 
   * @param prompt An optional text to guide the model's style or continue a previous audio segment. The [prompt](/docs/guides/speech-to-text/prompting) should match the audio language. 
   * @param responseFormat The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt. 
   * @param temperature The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. 
   * @param language The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency. 
   */
  def createTranscription(file: File, model: CreateTranscriptionRequestModel, prompt: Option[String] = None, responseFormat: Option[String] = None, temperature: Option[Double] = None, language: Option[String] = None
): Request[Either[ResponseException[String, Exception], CreateTranscriptionResponse], Any] =
    basicRequest
      .method(Method.POST, uri"$baseUrl/audio/transcriptions")
      .contentType("multipart/form-data")
      .multipartBody(Seq(
                multipartFile("file", file)
, 
                multipart("model", model)
, 
                prompt.map(multipart("prompt", _))
, 
                responseFormat.map(multipart("response_format", _))
, 
                temperature.map(multipart("temperature", _))
, 
                language.map(multipart("language", _))

      ).flatten)
      .response(asJson[CreateTranscriptionResponse])

  /**
   * Expected answers:
   *   code 200 : CreateTranslationResponse (OK)
   * 
   * @param file The audio file object (not file name) translate, in one of these formats: mp3, mp4, mpeg, mpga, m4a, wav, or webm. 
   * @param model 
   * @param prompt An optional text to guide the model's style or continue a previous audio segment. The [prompt](/docs/guides/speech-to-text/prompting) should be in English. 
   * @param responseFormat The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt. 
   * @param temperature The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. 
   */
  def createTranslation(file: File, model: CreateTranscriptionRequestModel, prompt: Option[String] = None, responseFormat: Option[String] = None, temperature: Option[Double] = None
): Request[Either[ResponseException[String, Exception], CreateTranslationResponse], Any] =
    basicRequest
      .method(Method.POST, uri"$baseUrl/audio/translations")
      .contentType("multipart/form-data")
      .multipartBody(Seq(
                multipartFile("file", file)
, 
                multipart("model", model)
, 
                prompt.map(multipart("prompt", _))
, 
                responseFormat.map(multipart("response_format", _))
, 
                temperature.map(multipart("temperature", _))

      ).flatten)
      .response(asJson[CreateTranslationResponse])

  /**
   * Expected answers:
   *   code 200 : DeleteFileResponse (OK)
   * 
   * @param fileId The ID of the file to use for this request
   */
  def deleteFile(fileId: String
): Request[Either[ResponseException[String, Exception], DeleteFileResponse], Any] =
    basicRequest
      .method(Method.DELETE, uri"$baseUrl/files/${fileId}")
      .contentType("application/json")
      .response(asJson[DeleteFileResponse])

  /**
   * Expected answers:
   *   code 200 : DeleteModelResponse (OK)
   * 
   * @param model The model to delete
   */
  def deleteModel(model: String
): Request[Either[ResponseException[String, Exception], DeleteModelResponse], Any] =
    basicRequest
      .method(Method.DELETE, uri"$baseUrl/models/${model}")
      .contentType("application/json")
      .response(asJson[DeleteModelResponse])

  /**
   * Expected answers:
   *   code 200 : String (OK)
   * 
   * @param fileId The ID of the file to use for this request
   */
  def downloadFile(fileId: String
): Request[Either[ResponseException[String, Exception], String], Any] =
    basicRequest
      .method(Method.GET, uri"$baseUrl/files/${fileId}/content")
      .contentType("application/json")
      .response(asJson[String])

  /**
   * Expected answers:
   *   code 200 : ListFilesResponse (OK)
   */
  def listFiles(
): Request[Either[ResponseException[String, Exception], ListFilesResponse], Any] =
    basicRequest
      .method(Method.GET, uri"$baseUrl/files")
      .contentType("application/json")
      .response(asJson[ListFilesResponse])

  /**
   * Expected answers:
   *   code 200 : ListFineTuneEventsResponse (OK)
   * 
   * @param fineTuneId The ID of the fine-tune job to get events for. 
   * @param stream Whether to stream events for the fine-tune job. If set to true, events will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available. The stream will terminate with a `data: [DONE]` message when the job is finished (succeeded, cancelled, or failed).  If set to false, only events generated so far will be returned. 
   */
  def listFineTuneEvents(fineTuneId: String, stream: Option[Boolean] = None
): Request[Either[ResponseException[String, Exception], ListFineTuneEventsResponse], Any] =
    basicRequest
      .method(Method.GET, uri"$baseUrl/fine-tunes/${fineTuneId}/events?stream=${ stream }")
      .contentType("application/json")
      .response(asJson[ListFineTuneEventsResponse])

  /**
   * Expected answers:
   *   code 200 : ListFineTunesResponse (OK)
   */
  def listFineTunes(
): Request[Either[ResponseException[String, Exception], ListFineTunesResponse], Any] =
    basicRequest
      .method(Method.GET, uri"$baseUrl/fine-tunes")
      .contentType("application/json")
      .response(asJson[ListFineTunesResponse])

  /**
   * Expected answers:
   *   code 200 : ListModelsResponse (OK)
   */
  def listModels(
): Request[Either[ResponseException[String, Exception], ListModelsResponse], Any] =
    basicRequest
      .method(Method.GET, uri"$baseUrl/models")
      .contentType("application/json")
      .response(asJson[ListModelsResponse])

  /**
   * Expected answers:
   *   code 200 : OpenAIFile (OK)
   * 
   * @param fileId The ID of the file to use for this request
   */
  def retrieveFile(fileId: String
): Request[Either[ResponseException[String, Exception], OpenAIFile], Any] =
    basicRequest
      .method(Method.GET, uri"$baseUrl/files/${fileId}")
      .contentType("application/json")
      .response(asJson[OpenAIFile])

  /**
   * Expected answers:
   *   code 200 : FineTune (OK)
   * 
   * @param fineTuneId The ID of the fine-tune job 
   */
  def retrieveFineTune(fineTuneId: String
): Request[Either[ResponseException[String, Exception], FineTune], Any] =
    basicRequest
      .method(Method.GET, uri"$baseUrl/fine-tunes/${fineTuneId}")
      .contentType("application/json")
      .response(asJson[FineTune])

  /**
   * Expected answers:
   *   code 200 : Model (OK)
   * 
   * @param model The ID of the model to use for this request
   */
  def retrieveModel(model: String
): Request[Either[ResponseException[String, Exception], Model], Any] =
    basicRequest
      .method(Method.GET, uri"$baseUrl/models/${model}")
      .contentType("application/json")
      .response(asJson[Model])

}
