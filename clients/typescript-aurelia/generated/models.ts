/**
 * OpenAI API
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.3.0
 * Contact: blah+oapicf@cliffano.com
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


export interface AdminApiKey {
  object?: string;
  id?: string;
  name?: string;
  redacted_value?: string;
  value?: string;
  created_at?: number;
  owner?: AdminApiKeyOwner;
}


export interface AdminApiKeyOwner {
  type?: string;
  id?: string;
  name?: string;
  created_at?: number;
  role?: string;
}


export interface AdminApiKeysCreateRequest {
  name: string;
}


export interface AdminApiKeysDelete200Response {
  id?: string;
  object?: string;
  deleted?: boolean;
}


export interface ApiKeyList {
  object?: string;
  data?: Array<AdminApiKey>;
  has_more?: boolean;
  first_id?: string;
  last_id?: string;
}

/**
 * @type ArrayOfContentPartsInner
 * @export
 */
export type ArrayOfContentPartsInner = MessageContentImageFileObject | MessageContentImageUrlObject | MessageRequestContentTextObject;


/**
 * Represents an `assistant` that can call the model and use tools.
 */
export interface AssistantObject {
  /**
   * The identifier, which can be referenced in API endpoints.
   */
  id: string;
  /**
   * The object type, which is always `assistant`.
   */
  object: AssistantObjectObjectEnum;
  /**
   * The Unix timestamp (in seconds) for when the assistant was created.
   */
  created_at: number;
  /**
   * The name of the assistant. The maximum length is 256 characters. 
   */
  name: string;
  /**
   * The description of the assistant. The maximum length is 512 characters. 
   */
  description: string;
  /**
   * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models) for descriptions of them. 
   */
  model: string;
  /**
   * The system instructions that the assistant uses. The maximum length is 256,000 characters. 
   */
  instructions: string;
  /**
   * A list of tool enabled on the assistant. There can be a maximum of 128 tools per assistant. Tools can be of types `code_interpreter`, `file_search`, or `function`. 
   */
  tools: Array<AssistantObjectToolsInner>;
  tool_resources?: AssistantObjectToolResources;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata: object;
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. 
   */
  temperature?: number;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or temperature but not both. 
   */
  top_p?: number;
  response_format?: AssistantsApiResponseFormatOption;
}

/**
 * Enum for the object property.
 */
export type AssistantObjectObjectEnum = 'assistant';


/**
 * A set of resources that are used by the assistant\'s tools. The resources are specific to the type of tool. For example, the `code_interpreter` tool requires a list of file IDs, while the `file_search` tool requires a list of vector store IDs. 
 */
export interface AssistantObjectToolResources {
  code_interpreter?: AssistantObjectToolResourcesCodeInterpreter;
  file_search?: AssistantObjectToolResourcesFileSearch;
}


export interface AssistantObjectToolResourcesCodeInterpreter {
  /**
   * A list of [file](/docs/api-reference/files) IDs made available to the `code_interpreter`` tool. There can be a maximum of 20 files associated with the tool. 
   */
  file_ids?: Array<string>;
}


export interface AssistantObjectToolResourcesFileSearch {
  /**
   * The ID of the [vector store](/docs/api-reference/vector-stores/object) attached to this assistant. There can be a maximum of 1 vector store attached to the assistant. 
   */
  vector_store_ids?: Array<string>;
}

/**
 * @type AssistantObjectToolsInner
 * @export
 */
export type AssistantObjectToolsInner = AssistantToolsCode | AssistantToolsFileSearch | AssistantToolsFunction;

/**
 * @type AssistantStreamEvent
 * Represents an event emitted when streaming a Run.  Each event in a server-sent events stream has an `event` and `data` property:  ``` event: thread.created data: {\"id\": \"thread_123\", \"object\": \"thread\", ...} ```  We emit events whenever a new object is created, transitions to a new state, or is being streamed in parts (deltas). For example, we emit `thread.run.created` when a new run is created, `thread.run.completed` when a run completes, and so on. When an Assistant chooses to create a message during a run, we emit a `thread.message.created event`, a `thread.message.in_progress` event, many `thread.message.delta` events, and finally a `thread.message.completed` event.  We may add additional events over time, so we recommend handling unknown events gracefully in your code. See the [Assistants API quickstart](/docs/assistants/overview) to learn how to integrate the Assistants API with streaming. 
 * @export
 */
export type AssistantStreamEvent = DoneEvent | ErrorEvent | MessageStreamEvent | RunStepStreamEvent | RunStreamEvent | ThreadStreamEvent;


export interface AssistantToolsCode {
  /**
   * The type of tool being defined: `code_interpreter`
   */
  type: AssistantToolsCodeTypeEnum;
}

/**
 * Enum for the type property.
 */
export type AssistantToolsCodeTypeEnum = 'code_interpreter';


export interface AssistantToolsFileSearch {
  /**
   * The type of tool being defined: `file_search`
   */
  type: AssistantToolsFileSearchTypeEnum;
  file_search?: AssistantToolsFileSearchFileSearch;
}

/**
 * Enum for the type property.
 */
export type AssistantToolsFileSearchTypeEnum = 'file_search';


/**
 * Overrides for the file search tool.
 */
export interface AssistantToolsFileSearchFileSearch {
  /**
   * The maximum number of results the file search tool should output. The default is 20 for `gpt-4*` models and 5 for `gpt-3.5-turbo`. This number should be between 1 and 50 inclusive.  Note that the file search tool may output fewer than `max_num_results` results. See the [file search tool documentation](/docs/assistants/tools/file-search#customizing-file-search-settings) for more information. 
   */
  max_num_results?: number;
  ranking_options?: FileSearchRankingOptions;
}


export interface AssistantToolsFileSearchTypeOnly {
  /**
   * The type of tool being defined: `file_search`
   */
  type: AssistantToolsFileSearchTypeOnlyTypeEnum;
}

/**
 * Enum for the type property.
 */
export type AssistantToolsFileSearchTypeOnlyTypeEnum = 'file_search';


export interface AssistantToolsFunction {
  /**
   * The type of tool being defined: `function`
   */
  type: AssistantToolsFunctionTypeEnum;
  _function: FunctionObject;
}

/**
 * Enum for the type property.
 */
export type AssistantToolsFunctionTypeEnum = 'function';

/**
 * @type AssistantsApiResponseFormatOption
 * Specifies the format that the model must output. Compatible with [GPT-4o](/docs/models#gpt-4o), [GPT-4 Turbo](/docs/models#gpt-4-turbo-and-gpt-4), and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.  Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema. Learn more in the [Structured Outputs guide](/docs/guides/structured-outputs).  Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.  **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if `finish_reason=\"length\"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length. 
 * @export
 */
export type AssistantsApiResponseFormatOption = ResponseFormatJsonObject | ResponseFormatJsonSchema | ResponseFormatText | string;

/**
 * @type AssistantsApiToolChoiceOption
 * Controls which (if any) tool is called by the model. `none` means the model will not call any tools and instead generates a message. `auto` is the default value and means the model can pick between generating a message or calling one or more tools. `required` means the model must call one or more tools before responding to the user. Specifying a particular tool like `{\"type\": \"file_search\"}` or `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to call that tool. 
 * @export
 */
export type AssistantsApiToolChoiceOption = AssistantsNamedToolChoice | string;


/**
 * Specifies a tool the model should use. Use to force the model to call a specific tool.
 */
export interface AssistantsNamedToolChoice {
  /**
   * The type of the tool. If type is `function`, the function name must be set
   */
  type: AssistantsNamedToolChoiceTypeEnum;
  _function?: AssistantsNamedToolChoiceFunction;
}

/**
 * Enum for the type property.
 */
export type AssistantsNamedToolChoiceTypeEnum = 'function' | 'code_interpreter' | 'file_search';


export interface AssistantsNamedToolChoiceFunction {
  /**
   * The name of the function to call.
   */
  name: string;
}


/**
 * The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`. 
 */
export type AudioResponseFormat = 'json' | 'text' | 'srt' | 'verbose_json' | 'vtt';

/**
 * A log of a user action or configuration change within this organization.
 */
export interface AuditLog {
  /**
   * The ID of this log.
   */
  id: string;
  type: AuditLogEventType;
  /**
   * The Unix timestamp (in seconds) of the event.
   */
  effective_at: number;
  project?: AuditLogProject;
  actor: AuditLogActor;
  api_key_created?: AuditLogApiKeyCreated;
  api_key_updated?: AuditLogApiKeyUpdated;
  api_key_deleted?: AuditLogApiKeyDeleted;
  invite_sent?: AuditLogInviteSent;
  invite_accepted?: AuditLogInviteAccepted;
  invite_deleted?: AuditLogInviteAccepted;
  login_failed?: AuditLogLoginFailed;
  logout_failed?: AuditLogLoginFailed;
  organization_updated?: AuditLogOrganizationUpdated;
  project_created?: AuditLogProjectCreated;
  project_updated?: AuditLogProjectUpdated;
  project_archived?: AuditLogProjectArchived;
  rate_limit_updated?: AuditLogRateLimitUpdated;
  rate_limit_deleted?: AuditLogRateLimitDeleted;
  service_account_created?: AuditLogServiceAccountCreated;
  service_account_updated?: AuditLogServiceAccountUpdated;
  service_account_deleted?: AuditLogServiceAccountDeleted;
  user_added?: AuditLogUserAdded;
  user_updated?: AuditLogUserUpdated;
  user_deleted?: AuditLogUserDeleted;
}


/**
 * The actor who performed the audit logged action.
 */
export interface AuditLogActor {
  /**
   * The type of actor. Is either `session` or `api_key`.
   */
  type?: AuditLogActorTypeEnum;
  session?: AuditLogActorSession;
  api_key?: AuditLogActorApiKey;
}

/**
 * Enum for the type property.
 */
export type AuditLogActorTypeEnum = 'session' | 'api_key';


/**
 * The API Key used to perform the audit logged action.
 */
export interface AuditLogActorApiKey {
  /**
   * The tracking id of the API key.
   */
  id?: string;
  /**
   * The type of API key. Can be either `user` or `service_account`.
   */
  type?: AuditLogActorApiKeyTypeEnum;
  user?: AuditLogActorUser;
  service_account?: AuditLogActorServiceAccount;
}

/**
 * Enum for the type property.
 */
export type AuditLogActorApiKeyTypeEnum = 'user' | 'service_account';


/**
 * The service account that performed the audit logged action.
 */
export interface AuditLogActorServiceAccount {
  /**
   * The service account id.
   */
  id?: string;
}


/**
 * The session in which the audit logged action was performed.
 */
export interface AuditLogActorSession {
  user?: AuditLogActorUser;
  /**
   * The IP address from which the action was performed.
   */
  ip_address?: string;
}


/**
 * The user who performed the audit logged action.
 */
export interface AuditLogActorUser {
  /**
   * The user id.
   */
  id?: string;
  /**
   * The user email.
   */
  email?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogApiKeyCreated {
  /**
   * The tracking ID of the API key.
   */
  id?: string;
  data?: AuditLogApiKeyCreatedData;
}


/**
 * The payload used to create the API key.
 */
export interface AuditLogApiKeyCreatedData {
  /**
   * A list of scopes allowed for the API key, e.g. `[\"api.model.request\"]`
   */
  scopes?: Array<string>;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogApiKeyDeleted {
  /**
   * The tracking ID of the API key.
   */
  id?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogApiKeyUpdated {
  /**
   * The tracking ID of the API key.
   */
  id?: string;
  changes_requested?: AuditLogApiKeyUpdatedChangesRequested;
}


/**
 * The payload used to update the API key.
 */
export interface AuditLogApiKeyUpdatedChangesRequested {
  /**
   * A list of scopes allowed for the API key, e.g. `[\"api.model.request\"]`
   */
  scopes?: Array<string>;
}


/**
 * The event type.
 */
export type AuditLogEventType = 'api_key.created' | 'api_key.updated' | 'api_key.deleted' | 'invite.sent' | 'invite.accepted' | 'invite.deleted' | 'login.succeeded' | 'login.failed' | 'logout.succeeded' | 'logout.failed' | 'organization.updated' | 'project.created' | 'project.updated' | 'project.archived' | 'service_account.created' | 'service_account.updated' | 'service_account.deleted' | 'rate_limit.updated' | 'rate_limit.deleted' | 'user.added' | 'user.updated' | 'user.deleted';

/**
 * The details for events with this `type`.
 */
export interface AuditLogInviteAccepted {
  /**
   * The ID of the invite.
   */
  id?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogInviteSent {
  /**
   * The ID of the invite.
   */
  id?: string;
  data?: AuditLogInviteSentData;
}


/**
 * The payload used to create the invite.
 */
export interface AuditLogInviteSentData {
  /**
   * The email invited to the organization.
   */
  email?: string;
  /**
   * The role the email was invited to be. Is either `owner` or `member`.
   */
  role?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogLoginFailed {
  /**
   * The error code of the failure.
   */
  error_code?: string;
  /**
   * The error message of the failure.
   */
  error_message?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogOrganizationUpdated {
  /**
   * The organization ID.
   */
  id?: string;
  changes_requested?: AuditLogOrganizationUpdatedChangesRequested;
}


/**
 * The payload used to update the organization settings.
 */
export interface AuditLogOrganizationUpdatedChangesRequested {
  /**
   * The organization title.
   */
  title?: string;
  /**
   * The organization description.
   */
  description?: string;
  /**
   * The organization name.
   */
  name?: string;
  settings?: AuditLogOrganizationUpdatedChangesRequestedSettings;
}


export interface AuditLogOrganizationUpdatedChangesRequestedSettings {
  /**
   * Visibility of the threads page which shows messages created with the Assistants API and Playground. One of `ANY_ROLE`, `OWNERS`, or `NONE`.
   */
  threads_ui_visibility?: string;
  /**
   * Visibility of the usage dashboard which shows activity and costs for your organization. One of `ANY_ROLE` or `OWNERS`.
   */
  usage_dashboard_visibility?: string;
}


/**
 * The project that the action was scoped to. Absent for actions not scoped to projects.
 */
export interface AuditLogProject {
  /**
   * The project ID.
   */
  id?: string;
  /**
   * The project title.
   */
  name?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogProjectArchived {
  /**
   * The project ID.
   */
  id?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogProjectCreated {
  /**
   * The project ID.
   */
  id?: string;
  data?: AuditLogProjectCreatedData;
}


/**
 * The payload used to create the project.
 */
export interface AuditLogProjectCreatedData {
  /**
   * The project name.
   */
  name?: string;
  /**
   * The title of the project as seen on the dashboard.
   */
  title?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogProjectUpdated {
  /**
   * The project ID.
   */
  id?: string;
  changes_requested?: AuditLogProjectUpdatedChangesRequested;
}


/**
 * The payload used to update the project.
 */
export interface AuditLogProjectUpdatedChangesRequested {
  /**
   * The title of the project as seen on the dashboard.
   */
  title?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogRateLimitDeleted {
  /**
   * The rate limit ID
   */
  id?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogRateLimitUpdated {
  /**
   * The rate limit ID
   */
  id?: string;
  changes_requested?: AuditLogRateLimitUpdatedChangesRequested;
}


/**
 * The payload used to update the rate limits.
 */
export interface AuditLogRateLimitUpdatedChangesRequested {
  /**
   * The maximum requests per minute.
   */
  max_requests_per_1_minute?: number;
  /**
   * The maximum tokens per minute.
   */
  max_tokens_per_1_minute?: number;
  /**
   * The maximum images per minute. Only relevant for certain models.
   */
  max_images_per_1_minute?: number;
  /**
   * The maximum audio megabytes per minute. Only relevant for certain models.
   */
  max_audio_megabytes_per_1_minute?: number;
  /**
   * The maximum requests per day. Only relevant for certain models.
   */
  max_requests_per_1_day?: number;
  /**
   * The maximum batch input tokens per day. Only relevant for certain models.
   */
  batch_1_day_max_input_tokens?: number;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogServiceAccountCreated {
  /**
   * The service account ID.
   */
  id?: string;
  data?: AuditLogServiceAccountCreatedData;
}


/**
 * The payload used to create the service account.
 */
export interface AuditLogServiceAccountCreatedData {
  /**
   * The role of the service account. Is either `owner` or `member`.
   */
  role?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogServiceAccountDeleted {
  /**
   * The service account ID.
   */
  id?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogServiceAccountUpdated {
  /**
   * The service account ID.
   */
  id?: string;
  changes_requested?: AuditLogServiceAccountUpdatedChangesRequested;
}


/**
 * The payload used to updated the service account.
 */
export interface AuditLogServiceAccountUpdatedChangesRequested {
  /**
   * The role of the service account. Is either `owner` or `member`.
   */
  role?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogUserAdded {
  /**
   * The user ID.
   */
  id?: string;
  data?: AuditLogUserAddedData;
}


/**
 * The payload used to add the user to the project.
 */
export interface AuditLogUserAddedData {
  /**
   * The role of the user. Is either `owner` or `member`.
   */
  role?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogUserDeleted {
  /**
   * The user ID.
   */
  id?: string;
}


/**
 * The details for events with this `type`.
 */
export interface AuditLogUserUpdated {
  /**
   * The project ID.
   */
  id?: string;
  changes_requested?: AuditLogUserUpdatedChangesRequested;
}


/**
 * The payload used to update the user.
 */
export interface AuditLogUserUpdatedChangesRequested {
  /**
   * The role of the user. Is either `owner` or `member`.
   */
  role?: string;
}


/**
 * The default strategy. This strategy currently uses a `max_chunk_size_tokens` of `800` and `chunk_overlap_tokens` of `400`.
 */
export interface AutoChunkingStrategy {
  /**
   * Always `auto`.
   */
  type: AutoChunkingStrategyTypeEnum;
}

/**
 * Enum for the type property.
 */
export type AutoChunkingStrategyTypeEnum = 'auto';


/**
 * The default strategy. This strategy currently uses a `max_chunk_size_tokens` of `800` and `chunk_overlap_tokens` of `400`.
 */
export interface AutoChunkingStrategyRequestParam {
  /**
   * Always `auto`.
   */
  type: AutoChunkingStrategyRequestParamTypeEnum;
}

/**
 * Enum for the type property.
 */
export type AutoChunkingStrategyRequestParamTypeEnum = 'auto';


export interface Batch {
  id: string;
  /**
   * The object type, which is always `batch`.
   */
  object: BatchObjectEnum;
  /**
   * The OpenAI API endpoint used by the batch.
   */
  endpoint: string;
  errors?: BatchErrors;
  /**
   * The ID of the input file for the batch.
   */
  input_file_id: string;
  /**
   * The time frame within which the batch should be processed.
   */
  completion_window: string;
  /**
   * The current status of the batch.
   */
  status: BatchStatusEnum;
  /**
   * The ID of the file containing the outputs of successfully executed requests.
   */
  output_file_id?: string;
  /**
   * The ID of the file containing the outputs of requests with errors.
   */
  error_file_id?: string;
  /**
   * The Unix timestamp (in seconds) for when the batch was created.
   */
  created_at: number;
  /**
   * The Unix timestamp (in seconds) for when the batch started processing.
   */
  in_progress_at?: number;
  /**
   * The Unix timestamp (in seconds) for when the batch will expire.
   */
  expires_at?: number;
  /**
   * The Unix timestamp (in seconds) for when the batch started finalizing.
   */
  finalizing_at?: number;
  /**
   * The Unix timestamp (in seconds) for when the batch was completed.
   */
  completed_at?: number;
  /**
   * The Unix timestamp (in seconds) for when the batch failed.
   */
  failed_at?: number;
  /**
   * The Unix timestamp (in seconds) for when the batch expired.
   */
  expired_at?: number;
  /**
   * The Unix timestamp (in seconds) for when the batch started cancelling.
   */
  cancelling_at?: number;
  /**
   * The Unix timestamp (in seconds) for when the batch was cancelled.
   */
  cancelled_at?: number;
  request_counts?: BatchRequestCounts;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
}

/**
 * Enum for the object property.
 */
export type BatchObjectEnum = 'batch';

/**
 * Enum for the status property.
 */
export type BatchStatusEnum = 'validating' | 'failed' | 'in_progress' | 'finalizing' | 'completed' | 'expired' | 'cancelling' | 'cancelled';


export interface BatchErrors {
  /**
   * The object type, which is always `list`.
   */
  object?: string;
  data?: Array<BatchErrorsDataInner>;
}


export interface BatchErrorsDataInner {
  /**
   * An error code identifying the error type.
   */
  code?: string;
  /**
   * A human-readable message providing more details about the error.
   */
  message?: string;
  /**
   * The name of the parameter that caused the error, if applicable.
   */
  param?: string;
  /**
   * The line number of the input file where the error occurred, if applicable.
   */
  line?: number;
}


/**
 * The request counts for different statuses within the batch.
 */
export interface BatchRequestCounts {
  /**
   * Total number of requests in the batch.
   */
  total: number;
  /**
   * Number of requests that have been completed successfully.
   */
  completed: number;
  /**
   * Number of requests that have failed.
   */
  failed: number;
}


/**
 * The per-line object of the batch input file
 */
export interface BatchRequestInput {
  /**
   * A developer-provided per-request id that will be used to match outputs to inputs. Must be unique for each request in a batch.
   */
  custom_id?: string;
  /**
   * The HTTP method to be used for the request. Currently only `POST` is supported.
   */
  method?: BatchRequestInputMethodEnum;
  /**
   * The OpenAI API relative URL to be used for the request. Currently `/v1/chat/completions`, `/v1/embeddings`, and `/v1/completions` are supported.
   */
  url?: string;
}

/**
 * Enum for the method property.
 */
export type BatchRequestInputMethodEnum = 'POST';


/**
 * The per-line object of the batch output and error files
 */
export interface BatchRequestOutput {
  id?: string;
  /**
   * A developer-provided per-request id that will be used to match outputs to inputs.
   */
  custom_id?: string;
  response?: BatchRequestOutputResponse;
  error?: BatchRequestOutputError;
}


/**
 * For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure.
 */
export interface BatchRequestOutputError {
  /**
   * A machine-readable error code.
   */
  code?: string;
  /**
   * A human-readable error message.
   */
  message?: string;
}


export interface BatchRequestOutputResponse {
  /**
   * The HTTP status code of the response
   */
  status_code?: number;
  /**
   * An unique identifier for the OpenAI API request. Please include this request ID when contacting support.
   */
  request_id?: string;
  /**
   * The JSON body of the response
   */
  body?: object;
}


/**
 * Specifying a particular function via `{\"name\": \"my_function\"}` forces the model to call that function. 
 */
export interface ChatCompletionFunctionCallOption {
  /**
   * The name of the function to call.
   */
  name: string;
}


export interface ChatCompletionFunctions {
  /**
   * A description of what the function does, used by the model to choose when and how to call the function.
   */
  description?: string;
  /**
   * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
   */
  name: string;
  /**
   * The parameters the functions accepts, described as a JSON Schema object. See the [guide](/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.   Omitting `parameters` defines a function with an empty parameter list.
   */
  parameters?: { [key: string]: any; };
}


export interface ChatCompletionMessageToolCall {
  /**
   * The ID of the tool call.
   */
  id: string;
  /**
   * The type of the tool. Currently, only `function` is supported.
   */
  type: ChatCompletionMessageToolCallTypeEnum;
  _function: ChatCompletionMessageToolCallFunction;
}

/**
 * Enum for the type property.
 */
export type ChatCompletionMessageToolCallTypeEnum = 'function';


export interface ChatCompletionMessageToolCallChunk {
  index: number;
  /**
   * The ID of the tool call.
   */
  id?: string;
  /**
   * The type of the tool. Currently, only `function` is supported.
   */
  type?: ChatCompletionMessageToolCallChunkTypeEnum;
  _function?: ChatCompletionMessageToolCallChunkFunction;
}

/**
 * Enum for the type property.
 */
export type ChatCompletionMessageToolCallChunkTypeEnum = 'function';


export interface ChatCompletionMessageToolCallChunkFunction {
  /**
   * The name of the function to call.
   */
  name?: string;
  /**
   * The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
   */
  arguments?: string;
}


/**
 * The function that the model called.
 */
export interface ChatCompletionMessageToolCallFunction {
  /**
   * The name of the function to call.
   */
  name: string;
  /**
   * The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
   */
  arguments: string;
}


/**
 * Specifies a tool the model should use. Use to force the model to call a specific function.
 */
export interface ChatCompletionNamedToolChoice {
  /**
   * The type of the tool. Currently, only `function` is supported.
   */
  type: ChatCompletionNamedToolChoiceTypeEnum;
  _function: AssistantsNamedToolChoiceFunction;
}

/**
 * Enum for the type property.
 */
export type ChatCompletionNamedToolChoiceTypeEnum = 'function';


/**
 * Messages sent by the model in response to user messages. 
 */
export interface ChatCompletionRequestAssistantMessage {
  content?: ChatCompletionRequestAssistantMessageContent;
  /**
   * The refusal message by the assistant.
   */
  refusal?: string;
  /**
   * The role of the messages author, in this case `assistant`.
   */
  role: ChatCompletionRequestAssistantMessageRoleEnum;
  /**
   * An optional name for the participant. Provides the model information to differentiate between participants of the same role.
   */
  name?: string;
  audio?: ChatCompletionRequestAssistantMessageAudio;
  /**
   * The tool calls generated by the model, such as function calls.
   */
  tool_calls?: Array<ChatCompletionMessageToolCall>;
  function_call?: ChatCompletionRequestAssistantMessageFunctionCall;
}

/**
 * Enum for the role property.
 */
export type ChatCompletionRequestAssistantMessageRoleEnum = 'assistant';


/**
 * Data about a previous audio response from the model.  [Learn more](/docs/guides/audio). 
 */
export interface ChatCompletionRequestAssistantMessageAudio {
  /**
   * Unique identifier for a previous audio response from the model. 
   */
  id: string;
}

/**
 * @type ChatCompletionRequestAssistantMessageContent
 * The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified. 
 * @export
 */
export type ChatCompletionRequestAssistantMessageContent = Array<ChatCompletionRequestAssistantMessageContentPart> | string;

/**
 * @type ChatCompletionRequestAssistantMessageContentPart
 * @export
 */
export type ChatCompletionRequestAssistantMessageContentPart = ChatCompletionRequestMessageContentPartRefusal | ChatCompletionRequestMessageContentPartText;


/**
 * Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model.
 */
export interface ChatCompletionRequestAssistantMessageFunctionCall {
  /**
   * The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
   */
  arguments: string;
  /**
   * The name of the function to call.
   */
  name: string;
}


/**
 * Developer-provided instructions that the model should follow, regardless of messages sent by the user. With o1 models and newer, `developer` messages replace the previous `system` messages. 
 */
export interface ChatCompletionRequestDeveloperMessage {
  content: ChatCompletionRequestDeveloperMessageContent;
  /**
   * The role of the messages author, in this case `developer`.
   */
  role: ChatCompletionRequestDeveloperMessageRoleEnum;
  /**
   * An optional name for the participant. Provides the model information to differentiate between participants of the same role.
   */
  name?: string;
}

/**
 * Enum for the role property.
 */
export type ChatCompletionRequestDeveloperMessageRoleEnum = 'developer';

/**
 * @type ChatCompletionRequestDeveloperMessageContent
 * The contents of the developer message.
 * @export
 */
export type ChatCompletionRequestDeveloperMessageContent = Array<ChatCompletionRequestMessageContentPartText> | string;


export interface ChatCompletionRequestFunctionMessage {
  /**
   * The role of the messages author, in this case `function`.
   */
  role: ChatCompletionRequestFunctionMessageRoleEnum;
  /**
   * The contents of the function message.
   */
  content: string;
  /**
   * The name of the function to call.
   */
  name: string;
}

/**
 * Enum for the role property.
 */
export type ChatCompletionRequestFunctionMessageRoleEnum = 'function';

/**
 * @type ChatCompletionRequestMessage
 * @export
 */
export type ChatCompletionRequestMessage = ChatCompletionRequestAssistantMessage | ChatCompletionRequestDeveloperMessage | ChatCompletionRequestFunctionMessage | ChatCompletionRequestSystemMessage | ChatCompletionRequestToolMessage | ChatCompletionRequestUserMessage;


/**
 * Learn about [audio inputs](/docs/guides/audio). 
 */
export interface ChatCompletionRequestMessageContentPartAudio {
  /**
   * The type of the content part. Always `input_audio`.
   */
  type: ChatCompletionRequestMessageContentPartAudioTypeEnum;
  input_audio: ChatCompletionRequestMessageContentPartAudioInputAudio;
}

/**
 * Enum for the type property.
 */
export type ChatCompletionRequestMessageContentPartAudioTypeEnum = 'input_audio';


export interface ChatCompletionRequestMessageContentPartAudioInputAudio {
  /**
   * Base64 encoded audio data.
   */
  data: string;
  /**
   * The format of the encoded audio data. Currently supports \"wav\" and \"mp3\". 
   */
  format: ChatCompletionRequestMessageContentPartAudioInputAudioFormatEnum;
}

/**
 * Enum for the format property.
 */
export type ChatCompletionRequestMessageContentPartAudioInputAudioFormatEnum = 'wav' | 'mp3';


/**
 * Learn about [image inputs](/docs/guides/vision). 
 */
export interface ChatCompletionRequestMessageContentPartImage {
  /**
   * The type of the content part.
   */
  type: ChatCompletionRequestMessageContentPartImageTypeEnum;
  image_url: ChatCompletionRequestMessageContentPartImageImageUrl;
}

/**
 * Enum for the type property.
 */
export type ChatCompletionRequestMessageContentPartImageTypeEnum = 'image_url';


export interface ChatCompletionRequestMessageContentPartImageImageUrl {
  /**
   * Either a URL of the image or the base64 encoded image data.
   */
  url: string;
  /**
   * Specifies the detail level of the image. Learn more in the [Vision guide](/docs/guides/vision#low-or-high-fidelity-image-understanding).
   */
  detail?: ChatCompletionRequestMessageContentPartImageImageUrlDetailEnum;
}

/**
 * Enum for the detail property.
 */
export type ChatCompletionRequestMessageContentPartImageImageUrlDetailEnum = 'auto' | 'low' | 'high';


export interface ChatCompletionRequestMessageContentPartRefusal {
  /**
   * The type of the content part.
   */
  type: ChatCompletionRequestMessageContentPartRefusalTypeEnum;
  /**
   * The refusal message generated by the model.
   */
  refusal: string;
}

/**
 * Enum for the type property.
 */
export type ChatCompletionRequestMessageContentPartRefusalTypeEnum = 'refusal';


/**
 * Learn about [text inputs](/docs/guides/text-generation). 
 */
export interface ChatCompletionRequestMessageContentPartText {
  /**
   * The type of the content part.
   */
  type: ChatCompletionRequestMessageContentPartTextTypeEnum;
  /**
   * The text content.
   */
  text: string;
}

/**
 * Enum for the type property.
 */
export type ChatCompletionRequestMessageContentPartTextTypeEnum = 'text';


/**
 * Developer-provided instructions that the model should follow, regardless of messages sent by the user. With o1 models and newer, use `developer` messages for this purpose instead. 
 */
export interface ChatCompletionRequestSystemMessage {
  content: ChatCompletionRequestSystemMessageContent;
  /**
   * The role of the messages author, in this case `system`.
   */
  role: ChatCompletionRequestSystemMessageRoleEnum;
  /**
   * An optional name for the participant. Provides the model information to differentiate between participants of the same role.
   */
  name?: string;
}

/**
 * Enum for the role property.
 */
export type ChatCompletionRequestSystemMessageRoleEnum = 'system';

/**
 * @type ChatCompletionRequestSystemMessageContent
 * The contents of the system message.
 * @export
 */
export type ChatCompletionRequestSystemMessageContent = Array<ChatCompletionRequestMessageContentPartText> | string;


export interface ChatCompletionRequestToolMessage {
  /**
   * The role of the messages author, in this case `tool`.
   */
  role: ChatCompletionRequestToolMessageRoleEnum;
  content: ChatCompletionRequestToolMessageContent;
  /**
   * Tool call that this message is responding to.
   */
  tool_call_id: string;
}

/**
 * Enum for the role property.
 */
export type ChatCompletionRequestToolMessageRoleEnum = 'tool';

/**
 * @type ChatCompletionRequestToolMessageContent
 * The contents of the tool message.
 * @export
 */
export type ChatCompletionRequestToolMessageContent = Array<ChatCompletionRequestMessageContentPartText> | string;


/**
 * Messages sent by an end user, containing prompts or additional context information. 
 */
export interface ChatCompletionRequestUserMessage {
  content: ChatCompletionRequestUserMessageContent;
  /**
   * The role of the messages author, in this case `user`.
   */
  role: ChatCompletionRequestUserMessageRoleEnum;
  /**
   * An optional name for the participant. Provides the model information to differentiate between participants of the same role.
   */
  name?: string;
}

/**
 * Enum for the role property.
 */
export type ChatCompletionRequestUserMessageRoleEnum = 'user';

/**
 * @type ChatCompletionRequestUserMessageContent
 * The contents of the user message. 
 * @export
 */
export type ChatCompletionRequestUserMessageContent = Array<ChatCompletionRequestUserMessageContentPart> | string;

/**
 * @type ChatCompletionRequestUserMessageContentPart
 * @export
 */
export type ChatCompletionRequestUserMessageContentPart = ChatCompletionRequestMessageContentPartAudio | ChatCompletionRequestMessageContentPartImage | ChatCompletionRequestMessageContentPartText;


/**
 * A chat completion message generated by the model.
 */
export interface ChatCompletionResponseMessage {
  /**
   * The contents of the message.
   */
  content: string;
  /**
   * The refusal message generated by the model.
   */
  refusal: string;
  /**
   * The tool calls generated by the model, such as function calls.
   */
  tool_calls?: Array<ChatCompletionMessageToolCall>;
  /**
   * The role of the author of this message.
   */
  role: ChatCompletionResponseMessageRoleEnum;
  function_call?: ChatCompletionResponseMessageFunctionCall;
  audio?: ChatCompletionResponseMessageAudio;
}

/**
 * Enum for the role property.
 */
export type ChatCompletionResponseMessageRoleEnum = 'assistant';


/**
 * If the audio output modality is requested, this object contains data about the audio response from the model. [Learn more](/docs/guides/audio). 
 */
export interface ChatCompletionResponseMessageAudio {
  /**
   * Unique identifier for this audio response.
   */
  id: string;
  /**
   * The Unix timestamp (in seconds) for when this audio response will no longer be accessible on the server for use in multi-turn conversations. 
   */
  expires_at: number;
  /**
   * Base64 encoded audio bytes generated by the model, in the format specified in the request. 
   */
  data: string;
  /**
   * Transcript of the audio generated by the model.
   */
  transcript: string;
}


/**
 * Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model.
 */
export interface ChatCompletionResponseMessageFunctionCall {
  /**
   * The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
   */
  arguments: string;
  /**
   * The name of the function to call.
   */
  name: string;
}


/**
 * The role of the author of a message
 */
export type ChatCompletionRole = 'system' | 'user' | 'assistant' | 'tool' | 'function';

/**
 * Options for streaming response. Only set this when you set `stream: true`. 
 */
export interface ChatCompletionStreamOptions {
  /**
   * If set, an additional chunk will be streamed before the `data: [DONE]` message. The `usage` field on this chunk shows the token usage statistics for the entire request, and the `choices` field will always be an empty array. All other chunks will also include a `usage` field, but with a null value. 
   */
  include_usage?: boolean;
}


/**
 * A chat completion delta generated by streamed model responses.
 */
export interface ChatCompletionStreamResponseDelta {
  /**
   * The contents of the chunk message.
   */
  content?: string;
  function_call?: ChatCompletionStreamResponseDeltaFunctionCall;
  tool_calls?: Array<ChatCompletionMessageToolCallChunk>;
  /**
   * The role of the author of this message.
   */
  role?: ChatCompletionStreamResponseDeltaRoleEnum;
  /**
   * The refusal message generated by the model.
   */
  refusal?: string;
}

/**
 * Enum for the role property.
 */
export type ChatCompletionStreamResponseDeltaRoleEnum = 'system' | 'user' | 'assistant' | 'tool';


/**
 * Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model.
 */
export interface ChatCompletionStreamResponseDeltaFunctionCall {
  /**
   * The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
   */
  arguments?: string;
  /**
   * The name of the function to call.
   */
  name?: string;
}


export interface ChatCompletionTokenLogprob {
  /**
   * The token.
   */
  token: string;
  /**
   * The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
   */
  logprob: number;
  /**
   * A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
   */
  bytes: Array<number>;
  /**
   * List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned.
   */
  top_logprobs: Array<ChatCompletionTokenLogprobTopLogprobsInner>;
}


export interface ChatCompletionTokenLogprobTopLogprobsInner {
  /**
   * The token.
   */
  token: string;
  /**
   * The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
   */
  logprob: number;
  /**
   * A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
   */
  bytes: Array<number>;
}


export interface ChatCompletionTool {
  /**
   * The type of the tool. Currently, only `function` is supported.
   */
  type: ChatCompletionToolTypeEnum;
  _function: FunctionObject;
}

/**
 * Enum for the type property.
 */
export type ChatCompletionToolTypeEnum = 'function';

/**
 * @type ChatCompletionToolChoiceOption
 * Controls which (if any) tool is called by the model. `none` means the model will not call any tool and instead generates a message. `auto` means the model can pick between generating a message or calling one or more tools. `required` means the model must call one or more tools. Specifying a particular tool via `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to call that tool.  `none` is the default when no tools are present. `auto` is the default if tools are present. 
 * @export
 */
export type ChatCompletionToolChoiceOption = ChatCompletionNamedToolChoice | string;

/**
 * @type ChunkingStrategyRequestParam
 * The chunking strategy used to chunk the file(s). If not set, will use the `auto` strategy.
 * @export
 */
export type ChunkingStrategyRequestParam = AutoChunkingStrategyRequestParam | StaticChunkingStrategyRequestParam;


export interface CompleteUploadRequest {
  /**
   * The ordered list of Part IDs. 
   */
  part_ids: Array<string>;
  /**
   * The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect. 
   */
  md5?: string;
}


/**
 * Usage statistics for the completion request.
 */
export interface CompletionUsage {
  /**
   * Number of tokens in the generated completion.
   */
  completion_tokens: number;
  /**
   * Number of tokens in the prompt.
   */
  prompt_tokens: number;
  /**
   * Total number of tokens used in the request (prompt + completion).
   */
  total_tokens: number;
  completion_tokens_details?: CompletionUsageCompletionTokensDetails;
  prompt_tokens_details?: CompletionUsagePromptTokensDetails;
}


/**
 * Breakdown of tokens used in a completion.
 */
export interface CompletionUsageCompletionTokensDetails {
  /**
   * When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion. 
   */
  accepted_prediction_tokens?: number;
  /**
   * Audio input tokens generated by the model.
   */
  audio_tokens?: number;
  /**
   * Tokens generated by the model for reasoning.
   */
  reasoning_tokens?: number;
  /**
   * When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits. 
   */
  rejected_prediction_tokens?: number;
}


/**
 * Breakdown of tokens used in the prompt.
 */
export interface CompletionUsagePromptTokensDetails {
  /**
   * Audio input tokens present in the prompt.
   */
  audio_tokens?: number;
  /**
   * Cached tokens present in the prompt.
   */
  cached_tokens?: number;
}


/**
 * The aggregated costs details of the specific time bucket.
 */
export interface CostsResult {
  object: CostsResultObjectEnum;
  amount?: CostsResultAmount;
  /**
   * When `group_by=line_item`, this field provides the line item of the grouped costs result.
   */
  line_item?: string;
  /**
   * When `group_by=project_id`, this field provides the project ID of the grouped costs result.
   */
  project_id?: string;
}

/**
 * Enum for the object property.
 */
export type CostsResultObjectEnum = 'organization.costs.result';


/**
 * The monetary value in its associated currency.
 */
export interface CostsResultAmount {
  /**
   * The numeric value of the cost.
   */
  value?: number;
  /**
   * Lowercase ISO-4217 currency e.g. \"usd\"
   */
  currency?: string;
}


export interface CreateAssistantRequest {
  model: CreateAssistantRequestModel;
  /**
   * The name of the assistant. The maximum length is 256 characters. 
   */
  name?: string;
  /**
   * The description of the assistant. The maximum length is 512 characters. 
   */
  description?: string;
  /**
   * The system instructions that the assistant uses. The maximum length is 256,000 characters. 
   */
  instructions?: string;
  /**
   * A list of tool enabled on the assistant. There can be a maximum of 128 tools per assistant. Tools can be of types `code_interpreter`, `file_search`, or `function`. 
   */
  tools?: Array<AssistantObjectToolsInner>;
  tool_resources?: CreateAssistantRequestToolResources;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. 
   */
  temperature?: number;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or temperature but not both. 
   */
  top_p?: number;
  response_format?: AssistantsApiResponseFormatOption;
}


/**
 * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models) for descriptions of them. 
 */
export interface CreateAssistantRequestModel {
}


/**
 * A set of resources that are used by the assistant\'s tools. The resources are specific to the type of tool. For example, the `code_interpreter` tool requires a list of file IDs, while the `file_search` tool requires a list of vector store IDs. 
 */
export interface CreateAssistantRequestToolResources {
  code_interpreter?: CreateAssistantRequestToolResourcesCodeInterpreter;
  file_search?: CreateAssistantRequestToolResourcesFileSearch;
}


export interface CreateAssistantRequestToolResourcesCodeInterpreter {
  /**
   * A list of [file](/docs/api-reference/files) IDs made available to the `code_interpreter` tool. There can be a maximum of 20 files associated with the tool. 
   */
  file_ids?: Array<string>;
}


export interface CreateAssistantRequestToolResourcesFileSearch {
  /**
   * The [vector store](/docs/api-reference/vector-stores/object) attached to this assistant. There can be a maximum of 1 vector store attached to the assistant. 
   */
  vector_store_ids?: Array<string>;
  /**
   * A helper to create a [vector store](/docs/api-reference/vector-stores/object) with file_ids and attach it to this assistant. There can be a maximum of 1 vector store attached to the assistant. 
   */
  vector_stores?: Array<CreateAssistantRequestToolResourcesFileSearchVectorStoresInner>;
}


export interface CreateAssistantRequestToolResourcesFileSearchVectorStoresInner {
  /**
   * A list of [file](/docs/api-reference/files) IDs to add to the vector store. There can be a maximum of 10000 files in a vector store. 
   */
  file_ids?: Array<string>;
  chunking_strategy?: CreateAssistantRequestToolResourcesFileSearchVectorStoresInnerChunkingStrategy;
  /**
   * Set of 16 key-value pairs that can be attached to a vector store. This can be useful for storing additional information about the vector store in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
}

/**
 * @type CreateAssistantRequestToolResourcesFileSearchVectorStoresInnerChunkingStrategy
 * The chunking strategy used to chunk the file(s). If not set, will use the `auto` strategy.
 * @export
 */
export type CreateAssistantRequestToolResourcesFileSearchVectorStoresInnerChunkingStrategy = AutoChunkingStrategy | StaticChunkingStrategy;


export interface CreateBatchRequest {
  /**
   * The ID of an uploaded file that contains requests for the new batch.  See [upload file](/docs/api-reference/files/create) for how to upload a file.  Your input file must be formatted as a [JSONL file](/docs/api-reference/batch/request-input), and must be uploaded with the purpose `batch`. The file can contain up to 50,000 requests, and can be up to 200 MB in size. 
   */
  input_file_id: string;
  /**
   * The endpoint to be used for all requests in the batch. Currently `/v1/chat/completions`, `/v1/embeddings`, and `/v1/completions` are supported. Note that `/v1/embeddings` batches are also restricted to a maximum of 50,000 embedding inputs across all requests in the batch.
   */
  endpoint: CreateBatchRequestEndpointEnum;
  /**
   * The time frame within which the batch should be processed. Currently only `24h` is supported.
   */
  completion_window: CreateBatchRequestCompletionWindowEnum;
  /**
   * Optional custom metadata for the batch.
   */
  metadata?: { [key: string]: string; };
}

/**
 * Enum for the endpoint property.
 */
export type CreateBatchRequestEndpointEnum = '/v1/chat/completions' | '/v1/embeddings' | '/v1/completions';

/**
 * Enum for the completion_window property.
 */
export type CreateBatchRequestCompletionWindowEnum = '24h';


/**
 * Represents a chat completion response returned by model, based on the provided input.
 */
export interface CreateChatCompletionFunctionResponse {
  /**
   * A unique identifier for the chat completion.
   */
  id: string;
  /**
   * A list of chat completion choices. Can be more than one if `n` is greater than 1.
   */
  choices: Array<CreateChatCompletionFunctionResponseChoicesInner>;
  /**
   * The Unix timestamp (in seconds) of when the chat completion was created.
   */
  created: number;
  /**
   * The model used for the chat completion.
   */
  model: string;
  /**
   * This fingerprint represents the backend configuration that the model runs with.  Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism. 
   */
  system_fingerprint?: string;
  /**
   * The object type, which is always `chat.completion`.
   */
  object: CreateChatCompletionFunctionResponseObjectEnum;
  usage?: CompletionUsage;
}

/**
 * Enum for the object property.
 */
export type CreateChatCompletionFunctionResponseObjectEnum = 'chat.completion';


export interface CreateChatCompletionFunctionResponseChoicesInner {
  /**
   * The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters, or `function_call` if the model called a function. 
   */
  finish_reason: CreateChatCompletionFunctionResponseChoicesInnerFinishReasonEnum;
  /**
   * The index of the choice in the list of choices.
   */
  index: number;
  message: ChatCompletionResponseMessage;
}

/**
 * Enum for the finish_reason property.
 */
export type CreateChatCompletionFunctionResponseChoicesInnerFinishReasonEnum = 'stop' | 'length' | 'function_call' | 'content_filter';


export interface CreateChatCompletionRequest {
  /**
   * A list of messages comprising the conversation so far. Depending on the [model](/docs/models) you use, different message types (modalities) are supported, like [text](/docs/guides/text-generation), [images](/docs/guides/vision), and [audio](/docs/guides/audio). 
   */
  messages: Array<ChatCompletionRequestMessage>;
  model: CreateChatCompletionRequestModel;
  /**
   * Whether or not to store the output of this chat completion request for  use in our [model distillation](/docs/guides/distillation) or [evals](/docs/guides/evals) products. 
   */
  store?: boolean;
  /**
   * **o1 models only**   Constrains effort on reasoning for  [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response. 
   */
  reasoning_effort?: CreateChatCompletionRequestReasoningEffortEnum;
  /**
   * Developer-defined tags and values used for filtering completions in the [dashboard](https://platform.openai.com/chat-completions). 
   */
  metadata?: { [key: string]: string; };
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\'s likelihood to repeat the same line verbatim. 
   */
  frequency_penalty?: number;
  /**
   * Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
   */
  logit_bias?: { [key: string]: number; };
  /**
   * Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. 
   */
  logprobs?: boolean;
  /**
   * An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used. 
   */
  top_logprobs?: number;
  /**
   * The maximum number of [tokens](/tokenizer) that can be generated in the chat completion. This value can be used to control [costs](https://openai.com/api/pricing/) for text generated via API.  This value is now deprecated in favor of `max_completion_tokens`, and is not compatible with [o1 series models](/docs/guides/reasoning). 
   */
  max_tokens?: number;
  /**
   * An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning). 
   */
  max_completion_tokens?: number;
  /**
   * How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
   */
  n?: number;
  /**
   * Output types that you would like the model to generate for this request. Most models are capable of generating text, which is the default:  `[\"text\"]`  The `gpt-4o-audio-preview` model can also be used to [generate audio](/docs/guides/audio). To request that this model generate both text and audio responses, you can use:  `[\"text\", \"audio\"]` 
   */
  modalities?: Array<CreateChatCompletionRequestModalitiesEnum>;
  prediction?: PredictionContent;
  audio?: CreateChatCompletionRequestAudio;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model\'s likelihood to talk about new topics. 
   */
  presence_penalty?: number;
  response_format?: CreateChatCompletionRequestResponseFormat;
  /**
   * This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. 
   */
  seed?: number;
  /**
   * Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:    - If set to \'auto\', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.   - If set to \'auto\', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - If set to \'default\', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - When not set, the default behavior is \'auto\'.    When this parameter is set, the response body will include the `service_tier` utilized. 
   */
  service_tier?: CreateChatCompletionRequestServiceTierEnum;
  stop?: CreateChatCompletionRequestStop;
  /**
   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
   */
  stream?: boolean;
  stream_options?: ChatCompletionStreamOptions;
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. 
   */
  temperature?: number;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. 
   */
  top_p?: number;
  /**
   * A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. 
   */
  tools?: Array<ChatCompletionTool>;
  tool_choice?: ChatCompletionToolChoiceOption;
  /**
   * Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.
   */
  parallel_tool_calls?: boolean;
  /**
   * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). 
   */
  user?: string;
  function_call?: CreateChatCompletionRequestFunctionCall;
  /**
   * Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for. 
   */
  functions?: Array<ChatCompletionFunctions>;
}

/**
 * Enum for the reasoning_effort property.
 */
export type CreateChatCompletionRequestReasoningEffortEnum = 'low' | 'medium' | 'high';

/**
 * Enum for the modalities property.
 */
export type CreateChatCompletionRequestModalitiesEnum = 'text' | 'audio';

/**
 * Enum for the service_tier property.
 */
export type CreateChatCompletionRequestServiceTierEnum = 'auto' | 'default';


/**
 * Parameters for audio output. Required when audio output is requested with `modalities: [\"audio\"]`. [Learn more](/docs/guides/audio). 
 */
export interface CreateChatCompletionRequestAudio {
  /**
   * The voice the model uses to respond. Supported voices are `ash`, `ballad`, `coral`, `sage`, and `verse` (also supported but not recommended are `alloy`, `echo`, and `shimmer`; these voices are less expressive). 
   */
  voice: CreateChatCompletionRequestAudioVoiceEnum;
  /**
   * Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`, `opus`, or `pcm16`. 
   */
  format: CreateChatCompletionRequestAudioFormatEnum;
}

/**
 * Enum for the voice property.
 */
export type CreateChatCompletionRequestAudioVoiceEnum = 'alloy' | 'ash' | 'ballad' | 'coral' | 'echo' | 'sage' | 'shimmer' | 'verse';

/**
 * Enum for the format property.
 */
export type CreateChatCompletionRequestAudioFormatEnum = 'wav' | 'mp3' | 'flac' | 'opus' | 'pcm16';

/**
 * @type CreateChatCompletionRequestFunctionCall
 * Deprecated in favor of `tool_choice`.  Controls which (if any) function is called by the model.  `none` means the model will not call a function and instead generates a message.  `auto` means the model can pick between generating a message or calling a function.  Specifying a particular function via `{\"name\": \"my_function\"}` forces the model to call that function.  `none` is the default when no functions are present. `auto` is the default if functions are present. 
 * @export
 */
export type CreateChatCompletionRequestFunctionCall = ChatCompletionFunctionCallOption | string;


/**
 * ID of the model to use. See the [model endpoint compatibility](/docs/models#model-endpoint-compatibility) table for details on which models work with the Chat API.
 */
export interface CreateChatCompletionRequestModel {
}

/**
 * @type CreateChatCompletionRequestResponseFormat
 * An object specifying the format that the model must output.  Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema. Learn more in the [Structured Outputs guide](/docs/guides/structured-outputs).  Setting to `{ \"type\": \"json_object\" }` enables JSON mode, which ensures the message the model generates is valid JSON.  **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if `finish_reason=\"length\"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length. 
 * @export
 */
export type CreateChatCompletionRequestResponseFormat = ResponseFormatJsonObject | ResponseFormatJsonSchema | ResponseFormatText;

/**
 * @type CreateChatCompletionRequestStop
 * Up to 4 sequences where the API will stop generating further tokens. 
 * @export
 */
export type CreateChatCompletionRequestStop = Array<string> | string;


/**
 * Represents a chat completion response returned by model, based on the provided input.
 */
export interface CreateChatCompletionResponse {
  /**
   * A unique identifier for the chat completion.
   */
  id: string;
  /**
   * A list of chat completion choices. Can be more than one if `n` is greater than 1.
   */
  choices: Array<CreateChatCompletionResponseChoicesInner>;
  /**
   * The Unix timestamp (in seconds) of when the chat completion was created.
   */
  created: number;
  /**
   * The model used for the chat completion.
   */
  model: string;
  /**
   * The service tier used for processing the request. This field is only included if the `service_tier` parameter is specified in the request.
   */
  service_tier?: CreateChatCompletionResponseServiceTierEnum;
  /**
   * This fingerprint represents the backend configuration that the model runs with.  Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism. 
   */
  system_fingerprint?: string;
  /**
   * The object type, which is always `chat.completion`.
   */
  object: CreateChatCompletionResponseObjectEnum;
  usage?: CompletionUsage;
}

/**
 * Enum for the service_tier property.
 */
export type CreateChatCompletionResponseServiceTierEnum = 'scale' | 'default';

/**
 * Enum for the object property.
 */
export type CreateChatCompletionResponseObjectEnum = 'chat.completion';


export interface CreateChatCompletionResponseChoicesInner {
  /**
   * The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters, `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function. 
   */
  finish_reason: CreateChatCompletionResponseChoicesInnerFinishReasonEnum;
  /**
   * The index of the choice in the list of choices.
   */
  index: number;
  message: ChatCompletionResponseMessage;
  logprobs: CreateChatCompletionResponseChoicesInnerLogprobs;
}

/**
 * Enum for the finish_reason property.
 */
export type CreateChatCompletionResponseChoicesInnerFinishReasonEnum = 'stop' | 'length' | 'tool_calls' | 'content_filter' | 'function_call';


/**
 * Log probability information for the choice.
 */
export interface CreateChatCompletionResponseChoicesInnerLogprobs {
  /**
   * A list of message content tokens with log probability information.
   */
  content: Array<ChatCompletionTokenLogprob>;
  /**
   * A list of message refusal tokens with log probability information.
   */
  refusal: Array<ChatCompletionTokenLogprob>;
}


/**
 * Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
 */
export interface CreateChatCompletionStreamResponse {
  /**
   * A unique identifier for the chat completion. Each chunk has the same ID.
   */
  id: string;
  /**
   * A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the last chunk if you set `stream_options: {\"include_usage\": true}`. 
   */
  choices: Array<CreateChatCompletionStreamResponseChoicesInner>;
  /**
   * The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
   */
  created: number;
  /**
   * The model to generate the completion.
   */
  model: string;
  /**
   * The service tier used for processing the request. This field is only included if the `service_tier` parameter is specified in the request.
   */
  service_tier?: CreateChatCompletionStreamResponseServiceTierEnum;
  /**
   * This fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism. 
   */
  system_fingerprint?: string;
  /**
   * The object type, which is always `chat.completion.chunk`.
   */
  object: CreateChatCompletionStreamResponseObjectEnum;
  usage?: CreateChatCompletionStreamResponseUsage;
}

/**
 * Enum for the service_tier property.
 */
export type CreateChatCompletionStreamResponseServiceTierEnum = 'scale' | 'default';

/**
 * Enum for the object property.
 */
export type CreateChatCompletionStreamResponseObjectEnum = 'chat.completion.chunk';


export interface CreateChatCompletionStreamResponseChoicesInner {
  delta: ChatCompletionStreamResponseDelta;
  logprobs?: CreateChatCompletionResponseChoicesInnerLogprobs;
  /**
   * The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters, `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function. 
   */
  finish_reason: CreateChatCompletionStreamResponseChoicesInnerFinishReasonEnum;
  /**
   * The index of the choice in the list of choices.
   */
  index: number;
}

/**
 * Enum for the finish_reason property.
 */
export type CreateChatCompletionStreamResponseChoicesInnerFinishReasonEnum = 'stop' | 'length' | 'tool_calls' | 'content_filter' | 'function_call';


/**
 * An optional field that will only be present when you set `stream_options: {\"include_usage\": true}` in your request. When present, it contains a null value except for the last chunk which contains the token usage statistics for the entire request. 
 */
export interface CreateChatCompletionStreamResponseUsage {
  /**
   * Number of tokens in the generated completion.
   */
  completion_tokens: number;
  /**
   * Number of tokens in the prompt.
   */
  prompt_tokens: number;
  /**
   * Total number of tokens used in the request (prompt + completion).
   */
  total_tokens: number;
}


export interface CreateCompletionRequest {
  model: CreateCompletionRequestModel;
  prompt: CreateCompletionRequestPrompt;
  /**
   * Generates `best_of` completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.  When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return  `best_of` must be greater than `n`.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`. 
   */
  best_of?: number;
  /**
   * Echo back the prompt in addition to the completion 
   */
  echo?: boolean;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\'s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/text-generation) 
   */
  frequency_penalty?: number;
  /**
   * Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.  As an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token from being generated. 
   */
  logit_bias?: { [key: string]: number; };
  /**
   * Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.  The maximum value for `logprobs` is 5. 
   */
  logprobs?: number;
  /**
   * The maximum number of [tokens](/tokenizer) that can be generated in the completion.  The token count of your prompt plus `max_tokens` cannot exceed the model\'s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. 
   */
  max_tokens?: number;
  /**
   * How many completions to generate for each prompt.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`. 
   */
  n?: number;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model\'s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/text-generation) 
   */
  presence_penalty?: number;
  /**
   * If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.  Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. 
   */
  seed?: number;
  stop?: CreateCompletionRequestStop;
  /**
   * Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
   */
  stream?: boolean;
  stream_options?: ChatCompletionStreamOptions;
  /**
   * The suffix that comes after a completion of inserted text.  This parameter is only supported for `gpt-3.5-turbo-instruct`. 
   */
  suffix?: string;
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both. 
   */
  temperature?: number;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. 
   */
  top_p?: number;
  /**
   * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). 
   */
  user?: string;
}


/**
 * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models) for descriptions of them. 
 */
export interface CreateCompletionRequestModel {
}

/**
 * @type CreateCompletionRequestPrompt
 * The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.  Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document. 
 * @export
 */
export type CreateCompletionRequestPrompt = Array<Array<number>> | Array<number> | Array<string> | string;

/**
 * @type CreateCompletionRequestStop
 * Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. 
 * @export
 */
export type CreateCompletionRequestStop = Array<string> | string;


/**
 * Represents a completion response from the API. Note: both the streamed and non-streamed response objects share the same shape (unlike the chat endpoint). 
 */
export interface CreateCompletionResponse {
  /**
   * A unique identifier for the completion.
   */
  id: string;
  /**
   * The list of completion choices the model generated for the input prompt.
   */
  choices: Array<CreateCompletionResponseChoicesInner>;
  /**
   * The Unix timestamp (in seconds) of when the completion was created.
   */
  created: number;
  /**
   * The model used for completion.
   */
  model: string;
  /**
   * This fingerprint represents the backend configuration that the model runs with.  Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism. 
   */
  system_fingerprint?: string;
  /**
   * The object type, which is always \"text_completion\"
   */
  object: CreateCompletionResponseObjectEnum;
  usage?: CompletionUsage;
}

/**
 * Enum for the object property.
 */
export type CreateCompletionResponseObjectEnum = 'text_completion';


export interface CreateCompletionResponseChoicesInner {
  /**
   * The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, or `content_filter` if content was omitted due to a flag from our content filters. 
   */
  finish_reason: CreateCompletionResponseChoicesInnerFinishReasonEnum;
  index: number;
  logprobs: CreateCompletionResponseChoicesInnerLogprobs;
  text: string;
}

/**
 * Enum for the finish_reason property.
 */
export type CreateCompletionResponseChoicesInnerFinishReasonEnum = 'stop' | 'length' | 'content_filter';


export interface CreateCompletionResponseChoicesInnerLogprobs {
  text_offset?: Array<number>;
  token_logprobs?: Array<number>;
  tokens?: Array<string>;
  top_logprobs?: Array<{ [key: string]: number; }>;
}


export interface CreateEmbeddingRequest {
  input: CreateEmbeddingRequestInput;
  model: CreateEmbeddingRequestModel;
  /**
   * The format to return the embeddings in. Can be either `float` or [`base64`](https://pypi.org/project/pybase64/).
   */
  encoding_format?: CreateEmbeddingRequestEncodingFormatEnum;
  /**
   * The number of dimensions the resulting output embeddings should have. Only supported in `text-embedding-3` and later models. 
   */
  dimensions?: number;
  /**
   * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). 
   */
  user?: string;
}

/**
 * Enum for the encoding_format property.
 */
export type CreateEmbeddingRequestEncodingFormatEnum = 'float' | 'base64';

/**
 * @type CreateEmbeddingRequestInput
 * Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for `text-embedding-ada-002`), cannot be an empty string, and any array must be 2048 dimensions or less. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. 
 * @export
 */
export type CreateEmbeddingRequestInput = Array<Array<number>> | Array<number> | Array<string> | string;


/**
 * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models) for descriptions of them. 
 */
export interface CreateEmbeddingRequestModel {
}


export interface CreateEmbeddingResponse {
  /**
   * The list of embeddings generated by the model.
   */
  data: Array<Embedding>;
  /**
   * The name of the model used to generate the embedding.
   */
  model: string;
  /**
   * The object type, which is always \"list\".
   */
  object: CreateEmbeddingResponseObjectEnum;
  usage: CreateEmbeddingResponseUsage;
}

/**
 * Enum for the object property.
 */
export type CreateEmbeddingResponseObjectEnum = 'list';


/**
 * The usage information for the request.
 */
export interface CreateEmbeddingResponseUsage {
  /**
   * The number of tokens used by the prompt.
   */
  prompt_tokens: number;
  /**
   * The total number of tokens used by the request.
   */
  total_tokens: number;
}


export interface CreateFineTuningJobRequest {
  model: CreateFineTuningJobRequestModel;
  /**
   * The ID of an uploaded file that contains training data.  See [upload file](/docs/api-reference/files/create) for how to upload a file.  Your dataset must be formatted as a JSONL file. Additionally, you must upload your file with the purpose `fine-tune`.  The contents of the file should differ depending on if the model uses the [chat](/docs/api-reference/fine-tuning/chat-input), [completions](/docs/api-reference/fine-tuning/completions-input) format, or if the fine-tuning method uses the [preference](/docs/api-reference/fine-tuning/preference-input) format.  See the [fine-tuning guide](/docs/guides/fine-tuning) for more details. 
   */
  training_file: string;
  hyperparameters?: CreateFineTuningJobRequestHyperparameters;
  /**
   * A string of up to 64 characters that will be added to your fine-tuned model name.  For example, a `suffix` of \"custom-model-name\" would produce a model name like `ft:gpt-4o-mini:openai:custom-model-name:7p4lURel`. 
   */
  suffix?: string;
  /**
   * The ID of an uploaded file that contains validation data.  If you provide this file, the data is used to generate validation metrics periodically during fine-tuning. These metrics can be viewed in the fine-tuning results file. The same data should not be present in both train and validation files.  Your dataset must be formatted as a JSONL file. You must upload your file with the purpose `fine-tune`.  See the [fine-tuning guide](/docs/guides/fine-tuning) for more details. 
   */
  validation_file?: string;
  /**
   * A list of integrations to enable for your fine-tuning job.
   */
  integrations?: Array<CreateFineTuningJobRequestIntegrationsInner>;
  /**
   * The seed controls the reproducibility of the job. Passing in the same seed and job parameters should produce the same results, but may differ in rare cases. If a seed is not specified, one will be generated for you. 
   */
  seed?: number;
  method?: FineTuneMethod;
}


/**
 * The hyperparameters used for the fine-tuning job. This value is now deprecated in favor of `method`, and should be passed in under the `method` parameter. 
 */
export interface CreateFineTuningJobRequestHyperparameters {
  batch_size?: CreateFineTuningJobRequestHyperparametersBatchSize;
  learning_rate_multiplier?: CreateFineTuningJobRequestHyperparametersLearningRateMultiplier;
  n_epochs?: CreateFineTuningJobRequestHyperparametersNEpochs;
}

/**
 * @type CreateFineTuningJobRequestHyperparametersBatchSize
 * Number of examples in each batch. A larger batch size means that model parameters are updated less frequently, but with lower variance. 
 * @export
 */
export type CreateFineTuningJobRequestHyperparametersBatchSize = number | string;

/**
 * @type CreateFineTuningJobRequestHyperparametersLearningRateMultiplier
 * Scaling factor for the learning rate. A smaller learning rate may be useful to avoid overfitting. 
 * @export
 */
export type CreateFineTuningJobRequestHyperparametersLearningRateMultiplier = number | string;

/**
 * @type CreateFineTuningJobRequestHyperparametersNEpochs
 * The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset. 
 * @export
 */
export type CreateFineTuningJobRequestHyperparametersNEpochs = number | string;


export interface CreateFineTuningJobRequestIntegrationsInner {
  type: CreateFineTuningJobRequestIntegrationsInnerTypeEnum;
  wandb: CreateFineTuningJobRequestIntegrationsInnerWandb;
}

/**
 * Enum for the type property.
 */
export type CreateFineTuningJobRequestIntegrationsInnerTypeEnum = 'wandb';


/**
 * The settings for your integration with Weights and Biases. This payload specifies the project that metrics will be sent to. Optionally, you can set an explicit display name for your run, add tags to your run, and set a default entity (team, username, etc) to be associated with your run. 
 */
export interface CreateFineTuningJobRequestIntegrationsInnerWandb {
  /**
   * The name of the project that the new run will be created under. 
   */
  project: string;
  /**
   * A display name to set for the run. If not set, we will use the Job ID as the name. 
   */
  name?: string;
  /**
   * The entity to use for the run. This allows you to set the team or username of the WandB user that you would like associated with the run. If not set, the default entity for the registered WandB API key is used. 
   */
  entity?: string;
  /**
   * A list of tags to be attached to the newly created run. These tags are passed through directly to WandB. Some default tags are generated by OpenAI: \"openai/finetune\", \"openai/{base-model}\", \"openai/{ftjob-abcdef}\". 
   */
  tags?: Array<string>;
}


/**
 * The name of the model to fine-tune. You can select one of the [supported models](/docs/guides/fine-tuning#which-models-can-be-fine-tuned). 
 */
export interface CreateFineTuningJobRequestModel {
}


/**
 * The model to use for image generation. Only `dall-e-2` is supported at this time.
 */
export interface CreateImageEditRequestModel {
}


export interface CreateImageRequest {
  /**
   * A text description of the desired image(s). The maximum length is 1000 characters for `dall-e-2` and 4000 characters for `dall-e-3`.
   */
  prompt: string;
  model?: CreateImageRequestModel;
  /**
   * The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only `n=1` is supported.
   */
  n?: number;
  /**
   * The quality of the image that will be generated. `hd` creates images with finer details and greater consistency across the image. This param is only supported for `dall-e-3`.
   */
  quality?: CreateImageRequestQualityEnum;
  /**
   * The format in which the generated images are returned. Must be one of `url` or `b64_json`. URLs are only valid for 60 minutes after the image has been generated.
   */
  response_format?: CreateImageRequestResponseFormatEnum;
  /**
   * The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`. Must be one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3` models.
   */
  size?: CreateImageRequestSizeEnum;
  /**
   * The style of the generated images. Must be one of `vivid` or `natural`. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This param is only supported for `dall-e-3`.
   */
  style?: CreateImageRequestStyleEnum;
  /**
   * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids). 
   */
  user?: string;
}

/**
 * Enum for the quality property.
 */
export type CreateImageRequestQualityEnum = 'standard' | 'hd';

/**
 * Enum for the response_format property.
 */
export type CreateImageRequestResponseFormatEnum = 'url' | 'b64_json';

/**
 * Enum for the size property.
 */
export type CreateImageRequestSizeEnum = '256x256' | '512x512' | '1024x1024' | '1792x1024' | '1024x1792';

/**
 * Enum for the style property.
 */
export type CreateImageRequestStyleEnum = 'vivid' | 'natural';


/**
 * The model to use for image generation.
 */
export interface CreateImageRequestModel {
}


export interface CreateMessageRequest {
  /**
   * The role of the entity that is creating the message. Allowed values include: - `user`: Indicates the message is sent by an actual user and should be used in most cases to represent user-generated messages. - `assistant`: Indicates the message is generated by the assistant. Use this value to insert messages from the assistant into the conversation. 
   */
  role: CreateMessageRequestRoleEnum;
  content: CreateMessageRequestContent;
  /**
   * A list of files attached to the message, and the tools they should be added to.
   */
  attachments?: Array<CreateMessageRequestAttachmentsInner>;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
}

/**
 * Enum for the role property.
 */
export type CreateMessageRequestRoleEnum = 'user' | 'assistant';


export interface CreateMessageRequestAttachmentsInner {
  /**
   * The ID of the file to attach to the message.
   */
  file_id?: string;
  /**
   * The tools to add this file to.
   */
  tools?: Array<CreateMessageRequestAttachmentsInnerToolsInner>;
}

/**
 * @type CreateMessageRequestAttachmentsInnerToolsInner
 * @export
 */
export type CreateMessageRequestAttachmentsInnerToolsInner = AssistantToolsCode | AssistantToolsFileSearchTypeOnly;

/**
 * @type CreateMessageRequestContent
 * @export
 */
export type CreateMessageRequestContent = Array<ArrayOfContentPartsInner> | string;


export interface CreateModerationRequest {
  input: CreateModerationRequestInput;
  model?: CreateModerationRequestModel;
}

/**
 * @type CreateModerationRequestInput
 * Input (or inputs) to classify. Can be a single string, an array of strings, or an array of multi-modal input objects similar to other models. 
 * @export
 */
export type CreateModerationRequestInput = Array<CreateModerationRequestInputOneOfInner> | Array<string> | string;

/**
 * @type CreateModerationRequestInputOneOfInner
 * @export
 */
export type CreateModerationRequestInputOneOfInner = CreateModerationRequestInputOneOfInnerOneOf | CreateModerationRequestInputOneOfInnerOneOf1;


/**
 * An object describing an image to classify.
 */
export interface CreateModerationRequestInputOneOfInnerOneOf {
  /**
   * Always `image_url`.
   */
  type: CreateModerationRequestInputOneOfInnerOneOfTypeEnum;
  image_url: CreateModerationRequestInputOneOfInnerOneOfImageUrl;
}

/**
 * Enum for the type property.
 */
export type CreateModerationRequestInputOneOfInnerOneOfTypeEnum = 'image_url';


/**
 * An object describing text to classify.
 */
export interface CreateModerationRequestInputOneOfInnerOneOf1 {
  /**
   * Always `text`.
   */
  type: CreateModerationRequestInputOneOfInnerOneOf1TypeEnum;
  /**
   * A string of text to classify.
   */
  text: string;
}

/**
 * Enum for the type property.
 */
export type CreateModerationRequestInputOneOfInnerOneOf1TypeEnum = 'text';


/**
 * Contains either an image URL or a data URL for a base64 encoded image.
 */
export interface CreateModerationRequestInputOneOfInnerOneOfImageUrl {
  /**
   * Either a URL of the image or the base64 encoded image data.
   */
  url: string;
}


/**
 * The content moderation model you would like to use. Learn more in [the moderation guide](/docs/guides/moderation), and learn about available models [here](/docs/models#moderation). 
 */
export interface CreateModerationRequestModel {
}


/**
 * Represents if a given text input is potentially harmful.
 */
export interface CreateModerationResponse {
  /**
   * The unique identifier for the moderation request.
   */
  id: string;
  /**
   * The model used to generate the moderation results.
   */
  model: string;
  /**
   * A list of moderation objects.
   */
  results: Array<CreateModerationResponseResultsInner>;
}


export interface CreateModerationResponseResultsInner {
  /**
   * Whether any of the below categories are flagged.
   */
  flagged: boolean;
  categories: CreateModerationResponseResultsInnerCategories;
  category_scores: CreateModerationResponseResultsInnerCategoryScores;
  category_applied_input_types: CreateModerationResponseResultsInnerCategoryAppliedInputTypes;
}


/**
 * A list of the categories, and whether they are flagged or not.
 */
export interface CreateModerationResponseResultsInnerCategories {
  /**
   * Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups (e.g., chess players) is harassment.
   */
  hate: boolean;
  /**
   * Hateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.
   */
  hate_threatening: boolean;
  /**
   * Content that expresses, incites, or promotes harassing language towards any target.
   */
  harassment: boolean;
  /**
   * Harassment content that also includes violence or serious harm towards any target.
   */
  harassment_threatening: boolean;
  /**
   * Content that includes instructions or advice that facilitate the planning or execution of wrongdoing, or that gives advice or instruction on how to commit illicit acts. For example, \"how to shoplift\" would fit this category.
   */
  illicit: boolean;
  /**
   * Content that includes instructions or advice that facilitate the planning or execution of wrongdoing that also includes violence, or that gives advice or instruction on the procurement of any weapon.
   */
  illicit_violent: boolean;
  /**
   * Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.
   */
  self_harm: boolean;
  /**
   * Content where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.
   */
  self_harm_intent: boolean;
  /**
   * Content that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.
   */
  self_harm_instructions: boolean;
  /**
   * Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).
   */
  sexual: boolean;
  /**
   * Sexual content that includes an individual who is under 18 years old.
   */
  sexual_minors: boolean;
  /**
   * Content that depicts death, violence, or physical injury.
   */
  violence: boolean;
  /**
   * Content that depicts death, violence, or physical injury in graphic detail.
   */
  violence_graphic: boolean;
}


/**
 * A list of the categories along with the input type(s) that the score applies to.
 */
export interface CreateModerationResponseResultsInnerCategoryAppliedInputTypes {
  /**
   * The applied input type(s) for the category \'hate\'.
   */
  hate: Array<CreateModerationResponseResultsInnerCategoryAppliedInputTypesHateEnum>;
  /**
   * The applied input type(s) for the category \'hate/threatening\'.
   */
  hate_threatening: Array<CreateModerationResponseResultsInnerCategoryAppliedInputTypesHateThreateningEnum>;
  /**
   * The applied input type(s) for the category \'harassment\'.
   */
  harassment: Array<CreateModerationResponseResultsInnerCategoryAppliedInputTypesHarassmentEnum>;
  /**
   * The applied input type(s) for the category \'harassment/threatening\'.
   */
  harassment_threatening: Array<CreateModerationResponseResultsInnerCategoryAppliedInputTypesHarassmentThreateningEnum>;
  /**
   * The applied input type(s) for the category \'illicit\'.
   */
  illicit: Array<CreateModerationResponseResultsInnerCategoryAppliedInputTypesIllicitEnum>;
  /**
   * The applied input type(s) for the category \'illicit/violent\'.
   */
  illicit_violent: Array<CreateModerationResponseResultsInnerCategoryAppliedInputTypesIllicitViolentEnum>;
  /**
   * The applied input type(s) for the category \'self-harm\'.
   */
  self_harm: Array<CreateModerationResponseResultsInnerCategoryAppliedInputTypesSelfHarmEnum>;
  /**
   * The applied input type(s) for the category \'self-harm/intent\'.
   */
  self_harm_intent: Array<CreateModerationResponseResultsInnerCategoryAppliedInputTypesSelfHarmIntentEnum>;
  /**
   * The applied input type(s) for the category \'self-harm/instructions\'.
   */
  self_harm_instructions: Array<CreateModerationResponseResultsInnerCategoryAppliedInputTypesSelfHarmInstructionsEnum>;
  /**
   * The applied input type(s) for the category \'sexual\'.
   */
  sexual: Array<CreateModerationResponseResultsInnerCategoryAppliedInputTypesSexualEnum>;
  /**
   * The applied input type(s) for the category \'sexual/minors\'.
   */
  sexual_minors: Array<CreateModerationResponseResultsInnerCategoryAppliedInputTypesSexualMinorsEnum>;
  /**
   * The applied input type(s) for the category \'violence\'.
   */
  violence: Array<CreateModerationResponseResultsInnerCategoryAppliedInputTypesViolenceEnum>;
  /**
   * The applied input type(s) for the category \'violence/graphic\'.
   */
  violence_graphic: Array<CreateModerationResponseResultsInnerCategoryAppliedInputTypesViolenceGraphicEnum>;
}

/**
 * Enum for the hate property.
 */
export type CreateModerationResponseResultsInnerCategoryAppliedInputTypesHateEnum = 'text';

/**
 * Enum for the hate_threatening property.
 */
export type CreateModerationResponseResultsInnerCategoryAppliedInputTypesHateThreateningEnum = 'text';

/**
 * Enum for the harassment property.
 */
export type CreateModerationResponseResultsInnerCategoryAppliedInputTypesHarassmentEnum = 'text';

/**
 * Enum for the harassment_threatening property.
 */
export type CreateModerationResponseResultsInnerCategoryAppliedInputTypesHarassmentThreateningEnum = 'text';

/**
 * Enum for the illicit property.
 */
export type CreateModerationResponseResultsInnerCategoryAppliedInputTypesIllicitEnum = 'text';

/**
 * Enum for the illicit_violent property.
 */
export type CreateModerationResponseResultsInnerCategoryAppliedInputTypesIllicitViolentEnum = 'text';

/**
 * Enum for the self_harm property.
 */
export type CreateModerationResponseResultsInnerCategoryAppliedInputTypesSelfHarmEnum = 'text' | 'image';

/**
 * Enum for the self_harm_intent property.
 */
export type CreateModerationResponseResultsInnerCategoryAppliedInputTypesSelfHarmIntentEnum = 'text' | 'image';

/**
 * Enum for the self_harm_instructions property.
 */
export type CreateModerationResponseResultsInnerCategoryAppliedInputTypesSelfHarmInstructionsEnum = 'text' | 'image';

/**
 * Enum for the sexual property.
 */
export type CreateModerationResponseResultsInnerCategoryAppliedInputTypesSexualEnum = 'text' | 'image';

/**
 * Enum for the sexual_minors property.
 */
export type CreateModerationResponseResultsInnerCategoryAppliedInputTypesSexualMinorsEnum = 'text';

/**
 * Enum for the violence property.
 */
export type CreateModerationResponseResultsInnerCategoryAppliedInputTypesViolenceEnum = 'text' | 'image';

/**
 * Enum for the violence_graphic property.
 */
export type CreateModerationResponseResultsInnerCategoryAppliedInputTypesViolenceGraphicEnum = 'text' | 'image';


/**
 * A list of the categories along with their scores as predicted by model.
 */
export interface CreateModerationResponseResultsInnerCategoryScores {
  /**
   * The score for the category \'hate\'.
   */
  hate: number;
  /**
   * The score for the category \'hate/threatening\'.
   */
  hate_threatening: number;
  /**
   * The score for the category \'harassment\'.
   */
  harassment: number;
  /**
   * The score for the category \'harassment/threatening\'.
   */
  harassment_threatening: number;
  /**
   * The score for the category \'illicit\'.
   */
  illicit: number;
  /**
   * The score for the category \'illicit/violent\'.
   */
  illicit_violent: number;
  /**
   * The score for the category \'self-harm\'.
   */
  self_harm: number;
  /**
   * The score for the category \'self-harm/intent\'.
   */
  self_harm_intent: number;
  /**
   * The score for the category \'self-harm/instructions\'.
   */
  self_harm_instructions: number;
  /**
   * The score for the category \'sexual\'.
   */
  sexual: number;
  /**
   * The score for the category \'sexual/minors\'.
   */
  sexual_minors: number;
  /**
   * The score for the category \'violence\'.
   */
  violence: number;
  /**
   * The score for the category \'violence/graphic\'.
   */
  violence_graphic: number;
}


export interface CreateRunRequest {
  /**
   * The ID of the [assistant](/docs/api-reference/assistants) to use to execute this run.
   */
  assistant_id: string;
  model?: CreateRunRequestModel;
  /**
   * Overrides the [instructions](/docs/api-reference/assistants/createAssistant) of the assistant. This is useful for modifying the behavior on a per-run basis.
   */
  instructions?: string;
  /**
   * Appends additional instructions at the end of the instructions for the run. This is useful for modifying the behavior on a per-run basis without overriding other instructions.
   */
  additional_instructions?: string;
  /**
   * Adds additional messages to the thread before creating the run.
   */
  additional_messages?: Array<CreateMessageRequest>;
  /**
   * Override the tools the assistant can use for this run. This is useful for modifying the behavior on a per-run basis.
   */
  tools?: Array<AssistantObjectToolsInner>;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. 
   */
  temperature?: number;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or temperature but not both. 
   */
  top_p?: number;
  /**
   * If `true`, returns a stream of events that happen during the Run as server-sent events, terminating when the Run enters a terminal state with a `data: [DONE]` message. 
   */
  stream?: boolean;
  /**
   * The maximum number of prompt tokens that may be used over the course of the run. The run will make a best effort to use only the number of prompt tokens specified, across multiple turns of the run. If the run exceeds the number of prompt tokens specified, the run will end with status `incomplete`. See `incomplete_details` for more info. 
   */
  max_prompt_tokens?: number;
  /**
   * The maximum number of completion tokens that may be used over the course of the run. The run will make a best effort to use only the number of completion tokens specified, across multiple turns of the run. If the run exceeds the number of completion tokens specified, the run will end with status `incomplete`. See `incomplete_details` for more info. 
   */
  max_completion_tokens?: number;
  truncation_strategy?: TruncationObject;
  tool_choice?: AssistantsApiToolChoiceOption;
  /**
   * Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.
   */
  parallel_tool_calls?: boolean;
  response_format?: AssistantsApiResponseFormatOption;
}


/**
 * The ID of the [Model](/docs/api-reference/models) to be used to execute this run. If a value is provided here, it will override the model associated with the assistant. If not, the model associated with the assistant will be used.
 */
export interface CreateRunRequestModel {
}


export interface CreateSpeechRequest {
  model: CreateSpeechRequestModel;
  /**
   * The text to generate audio for. The maximum length is 4096 characters.
   */
  input: string;
  /**
   * The voice to use when generating the audio. Supported voices are `alloy`, `echo`, `fable`, `onyx`, `nova`, and `shimmer`. Previews of the voices are available in the [Text to speech guide](/docs/guides/text-to-speech#voice-options).
   */
  voice: CreateSpeechRequestVoiceEnum;
  /**
   * The format to audio in. Supported formats are `mp3`, `opus`, `aac`, `flac`, `wav`, and `pcm`.
   */
  response_format?: CreateSpeechRequestResponseFormatEnum;
  /**
   * The speed of the generated audio. Select a value from `0.25` to `4.0`. `1.0` is the default.
   */
  speed?: number;
}

/**
 * Enum for the voice property.
 */
export type CreateSpeechRequestVoiceEnum = 'alloy' | 'echo' | 'fable' | 'onyx' | 'nova' | 'shimmer';

/**
 * Enum for the response_format property.
 */
export type CreateSpeechRequestResponseFormatEnum = 'mp3' | 'opus' | 'aac' | 'flac' | 'wav' | 'pcm';


/**
 * One of the available [TTS models](/docs/models#tts): `tts-1` or `tts-1-hd` 
 */
export interface CreateSpeechRequestModel {
}


export interface CreateThreadAndRunRequest {
  /**
   * The ID of the [assistant](/docs/api-reference/assistants) to use to execute this run.
   */
  assistant_id: string;
  thread?: CreateThreadRequest;
  model?: CreateRunRequestModel;
  /**
   * Override the default system message of the assistant. This is useful for modifying the behavior on a per-run basis.
   */
  instructions?: string;
  /**
   * Override the tools the assistant can use for this run. This is useful for modifying the behavior on a per-run basis.
   */
  tools?: Array<CreateThreadAndRunRequestToolsInner>;
  tool_resources?: CreateThreadAndRunRequestToolResources;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. 
   */
  temperature?: number;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or temperature but not both. 
   */
  top_p?: number;
  /**
   * If `true`, returns a stream of events that happen during the Run as server-sent events, terminating when the Run enters a terminal state with a `data: [DONE]` message. 
   */
  stream?: boolean;
  /**
   * The maximum number of prompt tokens that may be used over the course of the run. The run will make a best effort to use only the number of prompt tokens specified, across multiple turns of the run. If the run exceeds the number of prompt tokens specified, the run will end with status `incomplete`. See `incomplete_details` for more info. 
   */
  max_prompt_tokens?: number;
  /**
   * The maximum number of completion tokens that may be used over the course of the run. The run will make a best effort to use only the number of completion tokens specified, across multiple turns of the run. If the run exceeds the number of completion tokens specified, the run will end with status `incomplete`. See `incomplete_details` for more info. 
   */
  max_completion_tokens?: number;
  truncation_strategy?: TruncationObject;
  tool_choice?: AssistantsApiToolChoiceOption;
  /**
   * Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.
   */
  parallel_tool_calls?: boolean;
  response_format?: AssistantsApiResponseFormatOption;
}


/**
 * A set of resources that are used by the assistant\'s tools. The resources are specific to the type of tool. For example, the `code_interpreter` tool requires a list of file IDs, while the `file_search` tool requires a list of vector store IDs. 
 */
export interface CreateThreadAndRunRequestToolResources {
  code_interpreter?: CreateAssistantRequestToolResourcesCodeInterpreter;
  file_search?: AssistantObjectToolResourcesFileSearch;
}

/**
 * @type CreateThreadAndRunRequestToolsInner
 * @export
 */
export type CreateThreadAndRunRequestToolsInner = AssistantToolsCode | AssistantToolsFileSearch | AssistantToolsFunction;


export interface CreateThreadRequest {
  /**
   * A list of [messages](/docs/api-reference/messages) to start the thread with.
   */
  messages?: Array<CreateMessageRequest>;
  tool_resources?: CreateThreadRequestToolResources;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
}


/**
 * A set of resources that are made available to the assistant\'s tools in this thread. The resources are specific to the type of tool. For example, the `code_interpreter` tool requires a list of file IDs, while the `file_search` tool requires a list of vector store IDs. 
 */
export interface CreateThreadRequestToolResources {
  code_interpreter?: CreateAssistantRequestToolResourcesCodeInterpreter;
  file_search?: CreateThreadRequestToolResourcesFileSearch;
}


export interface CreateThreadRequestToolResourcesFileSearch {
  /**
   * The [vector store](/docs/api-reference/vector-stores/object) attached to this thread. There can be a maximum of 1 vector store attached to the thread. 
   */
  vector_store_ids?: Array<string>;
  /**
   * A helper to create a [vector store](/docs/api-reference/vector-stores/object) with file_ids and attach it to this thread. There can be a maximum of 1 vector store attached to the thread. 
   */
  vector_stores?: Array<CreateThreadRequestToolResourcesFileSearchVectorStoresInner>;
}


export interface CreateThreadRequestToolResourcesFileSearchVectorStoresInner {
  /**
   * A list of [file](/docs/api-reference/files) IDs to add to the vector store. There can be a maximum of 10000 files in a vector store. 
   */
  file_ids?: Array<string>;
  chunking_strategy?: CreateAssistantRequestToolResourcesFileSearchVectorStoresInnerChunkingStrategy;
  /**
   * Set of 16 key-value pairs that can be attached to a vector store. This can be useful for storing additional information about the vector store in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
}

/**
 * @type CreateTranscription200Response
 * @export
 */
export type CreateTranscription200Response = CreateTranscriptionResponseJson | CreateTranscriptionResponseVerboseJson;


/**
 * ID of the model to use. Only `whisper-1` (which is powered by our open source Whisper V2 model) is currently available. 
 */
export interface CreateTranscriptionRequestModel {
}


/**
 * Represents a transcription response returned by model, based on the provided input.
 */
export interface CreateTranscriptionResponseJson {
  /**
   * The transcribed text.
   */
  text: string;
}


/**
 * Represents a verbose json transcription response returned by model, based on the provided input.
 */
export interface CreateTranscriptionResponseVerboseJson {
  /**
   * The language of the input audio.
   */
  language: string;
  /**
   * The duration of the input audio.
   */
  duration: string;
  /**
   * The transcribed text.
   */
  text: string;
  /**
   * Extracted words and their corresponding timestamps.
   */
  words?: Array<TranscriptionWord>;
  /**
   * Segments of the transcribed text and their corresponding details.
   */
  segments?: Array<TranscriptionSegment>;
}

/**
 * @type CreateTranslation200Response
 * @export
 */
export type CreateTranslation200Response = CreateTranslationResponseJson | CreateTranslationResponseVerboseJson;


export interface CreateTranslationResponseJson {
  text: string;
}


export interface CreateTranslationResponseVerboseJson {
  /**
   * The language of the output translation (always `english`).
   */
  language: string;
  /**
   * The duration of the input audio.
   */
  duration: string;
  /**
   * The translated text.
   */
  text: string;
  /**
   * Segments of the translated text and their corresponding details.
   */
  segments?: Array<TranscriptionSegment>;
}


export interface CreateUploadRequest {
  /**
   * The name of the file to upload. 
   */
  filename: string;
  /**
   * The intended purpose of the uploaded file.  See the [documentation on File purposes](/docs/api-reference/files/create#files-create-purpose). 
   */
  purpose: CreateUploadRequestPurposeEnum;
  /**
   * The number of bytes in the file you are uploading. 
   */
  bytes: number;
  /**
   * The MIME type of the file.  This must fall within the supported MIME types for your file purpose. See the supported MIME types for assistants and vision. 
   */
  mime_type: string;
}

/**
 * Enum for the purpose property.
 */
export type CreateUploadRequestPurposeEnum = 'assistants' | 'batch' | 'fine-tune' | 'vision';


export interface CreateVectorStoreFileBatchRequest {
  /**
   * A list of [File](/docs/api-reference/files) IDs that the vector store should use. Useful for tools like `file_search` that can access files.
   */
  file_ids: Array<string>;
  chunking_strategy?: ChunkingStrategyRequestParam;
}


export interface CreateVectorStoreFileRequest {
  /**
   * A [File](/docs/api-reference/files) ID that the vector store should use. Useful for tools like `file_search` that can access files.
   */
  file_id: string;
  chunking_strategy?: ChunkingStrategyRequestParam;
}


export interface CreateVectorStoreRequest {
  /**
   * A list of [File](/docs/api-reference/files) IDs that the vector store should use. Useful for tools like `file_search` that can access files.
   */
  file_ids?: Array<string>;
  /**
   * The name of the vector store.
   */
  name?: string;
  expires_after?: VectorStoreExpirationAfter;
  chunking_strategy?: CreateVectorStoreRequestChunkingStrategy;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
}

/**
 * @type CreateVectorStoreRequestChunkingStrategy
 * The chunking strategy used to chunk the file(s). If not set, will use the `auto` strategy. Only applicable if `file_ids` is non-empty.
 * @export
 */
export type CreateVectorStoreRequestChunkingStrategy = AutoChunkingStrategyRequestParam | StaticChunkingStrategyRequestParam;


export interface DefaultProjectErrorResponse {
  code: number;
  message: string;
}


export interface DeleteAssistantResponse {
  id: string;
  deleted: boolean;
  object: DeleteAssistantResponseObjectEnum;
}

/**
 * Enum for the object property.
 */
export type DeleteAssistantResponseObjectEnum = 'assistant.deleted';


export interface DeleteFileResponse {
  id: string;
  object: DeleteFileResponseObjectEnum;
  deleted: boolean;
}

/**
 * Enum for the object property.
 */
export type DeleteFileResponseObjectEnum = 'file';


export interface DeleteMessageResponse {
  id: string;
  deleted: boolean;
  object: DeleteMessageResponseObjectEnum;
}

/**
 * Enum for the object property.
 */
export type DeleteMessageResponseObjectEnum = 'thread.message.deleted';


export interface DeleteModelResponse {
  id: string;
  deleted: boolean;
  object: string;
}


export interface DeleteThreadResponse {
  id: string;
  deleted: boolean;
  object: DeleteThreadResponseObjectEnum;
}

/**
 * Enum for the object property.
 */
export type DeleteThreadResponseObjectEnum = 'thread.deleted';


export interface DeleteVectorStoreFileResponse {
  id: string;
  deleted: boolean;
  object: DeleteVectorStoreFileResponseObjectEnum;
}

/**
 * Enum for the object property.
 */
export type DeleteVectorStoreFileResponseObjectEnum = 'vector_store.file.deleted';


export interface DeleteVectorStoreResponse {
  id: string;
  deleted: boolean;
  object: DeleteVectorStoreResponseObjectEnum;
}

/**
 * Enum for the object property.
 */
export type DeleteVectorStoreResponseObjectEnum = 'vector_store.deleted';


/**
 * Occurs when a stream ends.
 */
export interface DoneEvent {
  event: DoneEventEventEnum;
  data: DoneEventDataEnum;
}

/**
 * Enum for the event property.
 */
export type DoneEventEventEnum = 'done';

/**
 * Enum for the data property.
 */
export type DoneEventDataEnum = '[DONE]';


/**
 * Represents an embedding vector returned by embedding endpoint. 
 */
export interface Embedding {
  /**
   * The index of the embedding in the list of embeddings.
   */
  index: number;
  /**
   * The embedding vector, which is a list of floats. The length of vector depends on the model as listed in the [embedding guide](/docs/guides/embeddings). 
   */
  embedding: Array<number>;
  /**
   * The object type, which is always \"embedding\".
   */
  object: EmbeddingObjectEnum;
}

/**
 * Enum for the object property.
 */
export type EmbeddingObjectEnum = 'embedding';


/**
 * Occurs when an [error](/docs/guides/error-codes#api-errors) occurs. This can happen due to an internal server error or a timeout.
 */
export interface ErrorEvent {
  event: ErrorEventEventEnum;
  data: Error;
}

/**
 * Enum for the event property.
 */
export type ErrorEventEventEnum = 'error';


export interface ErrorResponse {
  error: Error;
}


/**
 * The ranking options for the file search. If not specified, the file search tool will use the `auto` ranker and a score_threshold of 0.  See the [file search tool documentation](/docs/assistants/tools/file-search#customizing-file-search-settings) for more information. 
 */
export interface FileSearchRankingOptions {
  /**
   * The ranker to use for the file search. If not specified will use the `auto` ranker.
   */
  ranker?: FileSearchRankingOptionsRankerEnum;
  /**
   * The score threshold for the file search. All values must be a floating point number between 0 and 1.
   */
  score_threshold: number;
}

/**
 * Enum for the ranker property.
 */
export type FileSearchRankingOptionsRankerEnum = 'auto' | 'default_2024_08_21';


export interface FineTuneChatCompletionRequestAssistantMessage {
  content?: ChatCompletionRequestAssistantMessageContent;
  /**
   * The refusal message by the assistant.
   */
  refusal?: string;
  /**
   * The role of the messages author, in this case `assistant`.
   */
  role: FineTuneChatCompletionRequestAssistantMessageRoleEnum;
  /**
   * An optional name for the participant. Provides the model information to differentiate between participants of the same role.
   */
  name?: string;
  audio?: ChatCompletionRequestAssistantMessageAudio;
  /**
   * The tool calls generated by the model, such as function calls.
   */
  tool_calls?: Array<ChatCompletionMessageToolCall>;
  function_call?: ChatCompletionRequestAssistantMessageFunctionCall;
  /**
   * Controls whether the assistant message is trained against (0 or 1)
   */
  weight?: FineTuneChatCompletionRequestAssistantMessageWeightEnum;
}

/**
 * Enum for the role property.
 */
export type FineTuneChatCompletionRequestAssistantMessageRoleEnum = 'assistant';

/**
 * Enum for the weight property.
 */
export type FineTuneChatCompletionRequestAssistantMessageWeightEnum = '0' | '1';


/**
 * The per-line training example of a fine-tuning input file for chat models using the supervised method.
 */
export interface FineTuneChatRequestInput {
  messages?: Array<FineTuneChatRequestInputMessagesInner>;
  /**
   * A list of tools the model may generate JSON inputs for.
   */
  tools?: Array<ChatCompletionTool>;
  /**
   * Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.
   */
  parallel_tool_calls?: boolean;
  /**
   * A list of functions the model may generate JSON inputs for.
   */
  functions?: Array<ChatCompletionFunctions>;
}

/**
 * @type FineTuneChatRequestInputMessagesInner
 * @export
 */
export type FineTuneChatRequestInputMessagesInner = ChatCompletionRequestFunctionMessage | ChatCompletionRequestSystemMessage | ChatCompletionRequestToolMessage | ChatCompletionRequestUserMessage | FineTuneChatCompletionRequestAssistantMessage;


/**
 * The per-line training example of a fine-tuning input file for completions models
 */
export interface FineTuneCompletionRequestInput {
  /**
   * The input prompt for this training example.
   */
  prompt?: string;
  /**
   * The desired completion for this training example.
   */
  completion?: string;
}


/**
 * Configuration for the DPO fine-tuning method.
 */
export interface FineTuneDPOMethod {
  hyperparameters?: FineTuneDPOMethodHyperparameters;
}


/**
 * The hyperparameters used for the fine-tuning job.
 */
export interface FineTuneDPOMethodHyperparameters {
  beta?: FineTuneDPOMethodHyperparametersBeta;
  batch_size?: FineTuneDPOMethodHyperparametersBatchSize;
  learning_rate_multiplier?: FineTuneDPOMethodHyperparametersLearningRateMultiplier;
  n_epochs?: FineTuneDPOMethodHyperparametersNEpochs;
}

/**
 * @type FineTuneDPOMethodHyperparametersBatchSize
 * Number of examples in each batch. A larger batch size means that model parameters are updated less frequently, but with lower variance. 
 * @export
 */
export type FineTuneDPOMethodHyperparametersBatchSize = number | string;

/**
 * @type FineTuneDPOMethodHyperparametersBeta
 * The beta value for the DPO method. A higher beta value will increase the weight of the penalty between the policy and reference model. 
 * @export
 */
export type FineTuneDPOMethodHyperparametersBeta = number | string;

/**
 * @type FineTuneDPOMethodHyperparametersLearningRateMultiplier
 * Scaling factor for the learning rate. A smaller learning rate may be useful to avoid overfitting. 
 * @export
 */
export type FineTuneDPOMethodHyperparametersLearningRateMultiplier = number | string;

/**
 * @type FineTuneDPOMethodHyperparametersNEpochs
 * The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset. 
 * @export
 */
export type FineTuneDPOMethodHyperparametersNEpochs = number | string;


/**
 * The method used for fine-tuning.
 */
export interface FineTuneMethod {
  /**
   * The type of method. Is either `supervised` or `dpo`.
   */
  type?: FineTuneMethodTypeEnum;
  supervised?: FineTuneSupervisedMethod;
  dpo?: FineTuneDPOMethod;
}

/**
 * Enum for the type property.
 */
export type FineTuneMethodTypeEnum = 'supervised' | 'dpo';


/**
 * The per-line training example of a fine-tuning input file for chat models using the dpo method.
 */
export interface FineTunePreferenceRequestInput {
  input?: FineTunePreferenceRequestInputInput;
  /**
   * The preferred completion message for the output.
   */
  preferred_completion?: Array<FineTunePreferenceRequestInputPreferredCompletionInner>;
  /**
   * The non-preferred completion message for the output.
   */
  non_preferred_completion?: Array<FineTunePreferenceRequestInputPreferredCompletionInner>;
}


export interface FineTunePreferenceRequestInputInput {
  messages?: Array<FineTuneChatRequestInputMessagesInner>;
  /**
   * A list of tools the model may generate JSON inputs for.
   */
  tools?: Array<ChatCompletionTool>;
  /**
   * Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.
   */
  parallel_tool_calls?: boolean;
}

/**
 * @type FineTunePreferenceRequestInputPreferredCompletionInner
 * @export
 */
export type FineTunePreferenceRequestInputPreferredCompletionInner = ChatCompletionRequestAssistantMessage;


/**
 * Configuration for the supervised fine-tuning method.
 */
export interface FineTuneSupervisedMethod {
  hyperparameters?: FineTuneSupervisedMethodHyperparameters;
}


/**
 * The hyperparameters used for the fine-tuning job.
 */
export interface FineTuneSupervisedMethodHyperparameters {
  batch_size?: FineTuneDPOMethodHyperparametersBatchSize;
  learning_rate_multiplier?: FineTuneDPOMethodHyperparametersLearningRateMultiplier;
  n_epochs?: FineTuneDPOMethodHyperparametersNEpochs;
}


export interface FineTuningIntegration {
  /**
   * The type of the integration being enabled for the fine-tuning job
   */
  type: FineTuningIntegrationTypeEnum;
  wandb: CreateFineTuningJobRequestIntegrationsInnerWandb;
}

/**
 * Enum for the type property.
 */
export type FineTuningIntegrationTypeEnum = 'wandb';


/**
 * The `fine_tuning.job` object represents a fine-tuning job that has been created through the API. 
 */
export interface FineTuningJob {
  /**
   * The object identifier, which can be referenced in the API endpoints.
   */
  id: string;
  /**
   * The Unix timestamp (in seconds) for when the fine-tuning job was created.
   */
  created_at: number;
  error: FineTuningJobError;
  /**
   * The name of the fine-tuned model that is being created. The value will be null if the fine-tuning job is still running.
   */
  fine_tuned_model: string;
  /**
   * The Unix timestamp (in seconds) for when the fine-tuning job was finished. The value will be null if the fine-tuning job is still running.
   */
  finished_at: number;
  hyperparameters: FineTuningJobHyperparameters;
  /**
   * The base model that is being fine-tuned.
   */
  model: string;
  /**
   * The object type, which is always \"fine_tuning.job\".
   */
  object: FineTuningJobObjectEnum;
  /**
   * The organization that owns the fine-tuning job.
   */
  organization_id: string;
  /**
   * The compiled results file ID(s) for the fine-tuning job. You can retrieve the results with the [Files API](/docs/api-reference/files/retrieve-contents).
   */
  result_files: Array<string>;
  /**
   * The current status of the fine-tuning job, which can be either `validating_files`, `queued`, `running`, `succeeded`, `failed`, or `cancelled`.
   */
  status: FineTuningJobStatusEnum;
  /**
   * The total number of billable tokens processed by this fine-tuning job. The value will be null if the fine-tuning job is still running.
   */
  trained_tokens: number;
  /**
   * The file ID used for training. You can retrieve the training data with the [Files API](/docs/api-reference/files/retrieve-contents).
   */
  training_file: string;
  /**
   * The file ID used for validation. You can retrieve the validation results with the [Files API](/docs/api-reference/files/retrieve-contents).
   */
  validation_file: string;
  /**
   * A list of integrations to enable for this fine-tuning job.
   */
  integrations?: Array<FineTuningJobIntegrationsInner>;
  /**
   * The seed used for the fine-tuning job.
   */
  seed: number;
  /**
   * The Unix timestamp (in seconds) for when the fine-tuning job is estimated to finish. The value will be null if the fine-tuning job is not running.
   */
  estimated_finish?: number;
  method?: FineTuneMethod;
}

/**
 * Enum for the object property.
 */
export type FineTuningJobObjectEnum = 'fine_tuning.job';

/**
 * Enum for the status property.
 */
export type FineTuningJobStatusEnum = 'validating_files' | 'queued' | 'running' | 'succeeded' | 'failed' | 'cancelled';


/**
 * The `fine_tuning.job.checkpoint` object represents a model checkpoint for a fine-tuning job that is ready to use. 
 */
export interface FineTuningJobCheckpoint {
  /**
   * The checkpoint identifier, which can be referenced in the API endpoints.
   */
  id: string;
  /**
   * The Unix timestamp (in seconds) for when the checkpoint was created.
   */
  created_at: number;
  /**
   * The name of the fine-tuned checkpoint model that is created.
   */
  fine_tuned_model_checkpoint: string;
  /**
   * The step number that the checkpoint was created at.
   */
  step_number: number;
  metrics: FineTuningJobCheckpointMetrics;
  /**
   * The name of the fine-tuning job that this checkpoint was created from.
   */
  fine_tuning_job_id: string;
  /**
   * The object type, which is always \"fine_tuning.job.checkpoint\".
   */
  object: FineTuningJobCheckpointObjectEnum;
}

/**
 * Enum for the object property.
 */
export type FineTuningJobCheckpointObjectEnum = 'fine_tuning.job.checkpoint';


/**
 * Metrics at the step number during the fine-tuning job.
 */
export interface FineTuningJobCheckpointMetrics {
  step?: number;
  train_loss?: number;
  train_mean_token_accuracy?: number;
  valid_loss?: number;
  valid_mean_token_accuracy?: number;
  full_valid_loss?: number;
  full_valid_mean_token_accuracy?: number;
}


/**
 * For fine-tuning jobs that have `failed`, this will contain more information on the cause of the failure.
 */
export interface FineTuningJobError {
  /**
   * A machine-readable error code.
   */
  code: string;
  /**
   * A human-readable error message.
   */
  message: string;
  /**
   * The parameter that was invalid, usually `training_file` or `validation_file`. This field will be null if the failure was not parameter-specific.
   */
  param: string;
}


/**
 * Fine-tuning job event object
 */
export interface FineTuningJobEvent {
  /**
   * The object type, which is always \"fine_tuning.job.event\".
   */
  object: FineTuningJobEventObjectEnum;
  /**
   * The object identifier.
   */
  id: string;
  /**
   * The Unix timestamp (in seconds) for when the fine-tuning job was created.
   */
  created_at: number;
  /**
   * The log level of the event.
   */
  level: FineTuningJobEventLevelEnum;
  /**
   * The message of the event.
   */
  message: string;
  /**
   * The type of event.
   */
  type?: FineTuningJobEventTypeEnum;
  /**
   * The data associated with the event.
   */
  data?: object;
}

/**
 * Enum for the object property.
 */
export type FineTuningJobEventObjectEnum = 'fine_tuning.job.event';

/**
 * Enum for the level property.
 */
export type FineTuningJobEventLevelEnum = 'info' | 'warn' | 'error';

/**
 * Enum for the type property.
 */
export type FineTuningJobEventTypeEnum = 'message' | 'metrics';


/**
 * The hyperparameters used for the fine-tuning job. This value will only be returned when running `supervised` jobs.
 */
export interface FineTuningJobHyperparameters {
  batch_size?: CreateFineTuningJobRequestHyperparametersBatchSize;
  learning_rate_multiplier?: CreateFineTuningJobRequestHyperparametersLearningRateMultiplier;
  n_epochs?: CreateFineTuningJobRequestHyperparametersNEpochs;
}

/**
 * @type FineTuningJobIntegrationsInner
 * @export
 */
export type FineTuningJobIntegrationsInner = FineTuningIntegration;


export interface FunctionObject {
  /**
   * A description of what the function does, used by the model to choose when and how to call the function.
   */
  description?: string;
  /**
   * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
   */
  name: string;
  /**
   * The parameters the functions accepts, described as a JSON Schema object. See the [guide](/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.   Omitting `parameters` defines a function with an empty parameter list.
   */
  parameters?: { [key: string]: any; };
  /**
   * Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the `parameters` field. Only a subset of JSON Schema is supported when `strict` is `true`. Learn more about Structured Outputs in the [function calling guide](docs/guides/function-calling).
   */
  strict?: boolean;
}


/**
 * Represents the url or the content of an image generated by the OpenAI API.
 */
export interface Image {
  /**
   * The base64-encoded JSON of the generated image, if `response_format` is `b64_json`.
   */
  b64_json?: string;
  /**
   * The URL of the generated image, if `response_format` is `url` (default).
   */
  url?: string;
  /**
   * The prompt that was used to generate the image, if there was any revision to the prompt.
   */
  revised_prompt?: string;
}


export interface ImagesResponse {
  created: number;
  data: Array<Image>;
}


/**
 * Represents an individual `invite` to the organization.
 */
export interface Invite {
  /**
   * The object type, which is always `organization.invite`
   */
  object: InviteObjectEnum;
  /**
   * The identifier, which can be referenced in API endpoints
   */
  id: string;
  /**
   * The email address of the individual to whom the invite was sent
   */
  email: string;
  /**
   * `owner` or `reader`
   */
  role: InviteRoleEnum;
  /**
   * `accepted`,`expired`, or `pending`
   */
  status: InviteStatusEnum;
  /**
   * The Unix timestamp (in seconds) of when the invite was sent.
   */
  invited_at: number;
  /**
   * The Unix timestamp (in seconds) of when the invite expires.
   */
  expires_at: number;
  /**
   * The Unix timestamp (in seconds) of when the invite was accepted.
   */
  accepted_at?: number;
  /**
   * The projects that were granted membership upon acceptance of the invite.
   */
  projects?: Array<InviteProjectsInner>;
}

/**
 * Enum for the object property.
 */
export type InviteObjectEnum = 'organization.invite';

/**
 * Enum for the role property.
 */
export type InviteRoleEnum = 'owner' | 'reader';

/**
 * Enum for the status property.
 */
export type InviteStatusEnum = 'accepted' | 'expired' | 'pending';


export interface InviteDeleteResponse {
  /**
   * The object type, which is always `organization.invite.deleted`
   */
  object: InviteDeleteResponseObjectEnum;
  id: string;
  deleted: boolean;
}

/**
 * Enum for the object property.
 */
export type InviteDeleteResponseObjectEnum = 'organization.invite.deleted';


export interface InviteListResponse {
  /**
   * The object type, which is always `list`
   */
  object: InviteListResponseObjectEnum;
  data: Array<Invite>;
  /**
   * The first `invite_id` in the retrieved `list`
   */
  first_id?: string;
  /**
   * The last `invite_id` in the retrieved `list`
   */
  last_id?: string;
  /**
   * The `has_more` property is used for pagination to indicate there are additional results.
   */
  has_more?: boolean;
}

/**
 * Enum for the object property.
 */
export type InviteListResponseObjectEnum = 'list';


export interface InviteProjectsInner {
  /**
   * Project\'s public ID
   */
  id?: string;
  /**
   * Project membership role
   */
  role?: InviteProjectsInnerRoleEnum;
}

/**
 * Enum for the role property.
 */
export type InviteProjectsInnerRoleEnum = 'member' | 'owner';


export interface InviteRequest {
  /**
   * Send an email to this address
   */
  email: string;
  /**
   * `owner` or `reader`
   */
  role: InviteRequestRoleEnum;
  /**
   * An array of projects to which membership is granted at the same time the org invite is accepted. If omitted, the user will be invited to the default project for compatibility with legacy behavior.
   */
  projects?: Array<InviteRequestProjectsInner>;
}

/**
 * Enum for the role property.
 */
export type InviteRequestRoleEnum = 'reader' | 'owner';


export interface InviteRequestProjectsInner {
  /**
   * Project\'s public ID
   */
  id: string;
  /**
   * Project membership role
   */
  role: InviteRequestProjectsInnerRoleEnum;
}

/**
 * Enum for the role property.
 */
export type InviteRequestProjectsInnerRoleEnum = 'member' | 'owner';


export interface ListAssistantsResponse {
  object: string;
  data: Array<AssistantObject>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}


export interface ListAuditLogsEffectiveAtParameter {
  /**
   * Return only events whose `effective_at` (Unix seconds) is greater than this value.
   */
  gt?: number;
  /**
   * Return only events whose `effective_at` (Unix seconds) is greater than or equal to this value.
   */
  gte?: number;
  /**
   * Return only events whose `effective_at` (Unix seconds) is less than this value.
   */
  lt?: number;
  /**
   * Return only events whose `effective_at` (Unix seconds) is less than or equal to this value.
   */
  lte?: number;
}


export interface ListAuditLogsResponse {
  object: ListAuditLogsResponseObjectEnum;
  data: Array<AuditLog>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}

/**
 * Enum for the object property.
 */
export type ListAuditLogsResponseObjectEnum = 'list';


export interface ListBatchesResponse {
  data: Array<Batch>;
  first_id?: string;
  last_id?: string;
  has_more: boolean;
  object: ListBatchesResponseObjectEnum;
}

/**
 * Enum for the object property.
 */
export type ListBatchesResponseObjectEnum = 'list';


export interface ListFilesResponse {
  object: string;
  data: Array<OpenAIFile>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}


export interface ListFineTuningJobCheckpointsResponse {
  data: Array<FineTuningJobCheckpoint>;
  object: ListFineTuningJobCheckpointsResponseObjectEnum;
  first_id?: string;
  last_id?: string;
  has_more: boolean;
}

/**
 * Enum for the object property.
 */
export type ListFineTuningJobCheckpointsResponseObjectEnum = 'list';


export interface ListFineTuningJobEventsResponse {
  data: Array<FineTuningJobEvent>;
  object: ListFineTuningJobEventsResponseObjectEnum;
}

/**
 * Enum for the object property.
 */
export type ListFineTuningJobEventsResponseObjectEnum = 'list';


export interface ListMessagesResponse {
  object: string;
  data: Array<MessageObject>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}


export interface ListModelsResponse {
  object: ListModelsResponseObjectEnum;
  data: Array<Model>;
}

/**
 * Enum for the object property.
 */
export type ListModelsResponseObjectEnum = 'list';


export interface ListPaginatedFineTuningJobsResponse {
  data: Array<FineTuningJob>;
  has_more: boolean;
  object: ListPaginatedFineTuningJobsResponseObjectEnum;
}

/**
 * Enum for the object property.
 */
export type ListPaginatedFineTuningJobsResponseObjectEnum = 'list';


export interface ListRunStepsResponse {
  object: string;
  data: Array<RunStepObject>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}


export interface ListRunsResponse {
  object: string;
  data: Array<RunObject>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}


export interface ListThreadsResponse {
  object: string;
  data: Array<ThreadObject>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}


export interface ListVectorStoreFilesResponse {
  object: string;
  data: Array<VectorStoreFileObject>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}


export interface ListVectorStoresResponse {
  object: string;
  data: Array<VectorStoreObject>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}


/**
 * References an image [File](/docs/api-reference/files) in the content of a message.
 */
export interface MessageContentImageFileObject {
  /**
   * Always `image_file`.
   */
  type: MessageContentImageFileObjectTypeEnum;
  image_file: MessageContentImageFileObjectImageFile;
}

/**
 * Enum for the type property.
 */
export type MessageContentImageFileObjectTypeEnum = 'image_file';


export interface MessageContentImageFileObjectImageFile {
  /**
   * The [File](/docs/api-reference/files) ID of the image in the message content. Set `purpose=\"vision\"` when uploading the File if you need to later display the file content.
   */
  file_id: string;
  /**
   * Specifies the detail level of the image if specified by the user. `low` uses fewer tokens, you can opt in to high resolution using `high`.
   */
  detail?: MessageContentImageFileObjectImageFileDetailEnum;
}

/**
 * Enum for the detail property.
 */
export type MessageContentImageFileObjectImageFileDetailEnum = 'auto' | 'low' | 'high';


/**
 * References an image URL in the content of a message.
 */
export interface MessageContentImageUrlObject {
  /**
   * The type of the content part.
   */
  type: MessageContentImageUrlObjectTypeEnum;
  image_url: MessageContentImageUrlObjectImageUrl;
}

/**
 * Enum for the type property.
 */
export type MessageContentImageUrlObjectTypeEnum = 'image_url';


export interface MessageContentImageUrlObjectImageUrl {
  /**
   * The external URL of the image, must be a supported image types: jpeg, jpg, png, gif, webp.
   */
  url: string;
  /**
   * Specifies the detail level of the image. `low` uses fewer tokens, you can opt in to high resolution using `high`. Default value is `auto`
   */
  detail?: MessageContentImageUrlObjectImageUrlDetailEnum;
}

/**
 * Enum for the detail property.
 */
export type MessageContentImageUrlObjectImageUrlDetailEnum = 'auto' | 'low' | 'high';


/**
 * The refusal content generated by the assistant.
 */
export interface MessageContentRefusalObject {
  /**
   * Always `refusal`.
   */
  type: MessageContentRefusalObjectTypeEnum;
  refusal: string;
}

/**
 * Enum for the type property.
 */
export type MessageContentRefusalObjectTypeEnum = 'refusal';


/**
 * A citation within the message that points to a specific quote from a specific File associated with the assistant or the message. Generated when the assistant uses the \"file_search\" tool to search files.
 */
export interface MessageContentTextAnnotationsFileCitationObject {
  /**
   * Always `file_citation`.
   */
  type: MessageContentTextAnnotationsFileCitationObjectTypeEnum;
  /**
   * The text in the message content that needs to be replaced.
   */
  text: string;
  file_citation: MessageContentTextAnnotationsFileCitationObjectFileCitation;
  start_index: number;
  end_index: number;
}

/**
 * Enum for the type property.
 */
export type MessageContentTextAnnotationsFileCitationObjectTypeEnum = 'file_citation';


export interface MessageContentTextAnnotationsFileCitationObjectFileCitation {
  /**
   * The ID of the specific File the citation is from.
   */
  file_id: string;
}


/**
 * A URL for the file that\'s generated when the assistant used the `code_interpreter` tool to generate a file.
 */
export interface MessageContentTextAnnotationsFilePathObject {
  /**
   * Always `file_path`.
   */
  type: MessageContentTextAnnotationsFilePathObjectTypeEnum;
  /**
   * The text in the message content that needs to be replaced.
   */
  text: string;
  file_path: MessageContentTextAnnotationsFilePathObjectFilePath;
  start_index: number;
  end_index: number;
}

/**
 * Enum for the type property.
 */
export type MessageContentTextAnnotationsFilePathObjectTypeEnum = 'file_path';


export interface MessageContentTextAnnotationsFilePathObjectFilePath {
  /**
   * The ID of the file that was generated.
   */
  file_id: string;
}


/**
 * The text content that is part of a message.
 */
export interface MessageContentTextObject {
  /**
   * Always `text`.
   */
  type: MessageContentTextObjectTypeEnum;
  text: MessageContentTextObjectText;
}

/**
 * Enum for the type property.
 */
export type MessageContentTextObjectTypeEnum = 'text';


export interface MessageContentTextObjectText {
  /**
   * The data that makes up the text.
   */
  value: string;
  annotations: Array<MessageContentTextObjectTextAnnotationsInner>;
}

/**
 * @type MessageContentTextObjectTextAnnotationsInner
 * @export
 */
export type MessageContentTextObjectTextAnnotationsInner = MessageContentTextAnnotationsFileCitationObject | MessageContentTextAnnotationsFilePathObject;


/**
 * References an image [File](/docs/api-reference/files) in the content of a message.
 */
export interface MessageDeltaContentImageFileObject {
  /**
   * The index of the content part in the message.
   */
  index: number;
  /**
   * Always `image_file`.
   */
  type: MessageDeltaContentImageFileObjectTypeEnum;
  image_file?: MessageDeltaContentImageFileObjectImageFile;
}

/**
 * Enum for the type property.
 */
export type MessageDeltaContentImageFileObjectTypeEnum = 'image_file';


export interface MessageDeltaContentImageFileObjectImageFile {
  /**
   * The [File](/docs/api-reference/files) ID of the image in the message content. Set `purpose=\"vision\"` when uploading the File if you need to later display the file content.
   */
  file_id?: string;
  /**
   * Specifies the detail level of the image if specified by the user. `low` uses fewer tokens, you can opt in to high resolution using `high`.
   */
  detail?: MessageDeltaContentImageFileObjectImageFileDetailEnum;
}

/**
 * Enum for the detail property.
 */
export type MessageDeltaContentImageFileObjectImageFileDetailEnum = 'auto' | 'low' | 'high';


/**
 * References an image URL in the content of a message.
 */
export interface MessageDeltaContentImageUrlObject {
  /**
   * The index of the content part in the message.
   */
  index: number;
  /**
   * Always `image_url`.
   */
  type: MessageDeltaContentImageUrlObjectTypeEnum;
  image_url?: MessageDeltaContentImageUrlObjectImageUrl;
}

/**
 * Enum for the type property.
 */
export type MessageDeltaContentImageUrlObjectTypeEnum = 'image_url';


export interface MessageDeltaContentImageUrlObjectImageUrl {
  /**
   * The URL of the image, must be a supported image types: jpeg, jpg, png, gif, webp.
   */
  url?: string;
  /**
   * Specifies the detail level of the image. `low` uses fewer tokens, you can opt in to high resolution using `high`.
   */
  detail?: MessageDeltaContentImageUrlObjectImageUrlDetailEnum;
}

/**
 * Enum for the detail property.
 */
export type MessageDeltaContentImageUrlObjectImageUrlDetailEnum = 'auto' | 'low' | 'high';


/**
 * The refusal content that is part of a message.
 */
export interface MessageDeltaContentRefusalObject {
  /**
   * The index of the refusal part in the message.
   */
  index: number;
  /**
   * Always `refusal`.
   */
  type: MessageDeltaContentRefusalObjectTypeEnum;
  refusal?: string;
}

/**
 * Enum for the type property.
 */
export type MessageDeltaContentRefusalObjectTypeEnum = 'refusal';


/**
 * A citation within the message that points to a specific quote from a specific File associated with the assistant or the message. Generated when the assistant uses the \"file_search\" tool to search files.
 */
export interface MessageDeltaContentTextAnnotationsFileCitationObject {
  /**
   * The index of the annotation in the text content part.
   */
  index: number;
  /**
   * Always `file_citation`.
   */
  type: MessageDeltaContentTextAnnotationsFileCitationObjectTypeEnum;
  /**
   * The text in the message content that needs to be replaced.
   */
  text?: string;
  file_citation?: MessageDeltaContentTextAnnotationsFileCitationObjectFileCitation;
  start_index?: number;
  end_index?: number;
}

/**
 * Enum for the type property.
 */
export type MessageDeltaContentTextAnnotationsFileCitationObjectTypeEnum = 'file_citation';


export interface MessageDeltaContentTextAnnotationsFileCitationObjectFileCitation {
  /**
   * The ID of the specific File the citation is from.
   */
  file_id?: string;
  /**
   * The specific quote in the file.
   */
  quote?: string;
}


/**
 * A URL for the file that\'s generated when the assistant used the `code_interpreter` tool to generate a file.
 */
export interface MessageDeltaContentTextAnnotationsFilePathObject {
  /**
   * The index of the annotation in the text content part.
   */
  index: number;
  /**
   * Always `file_path`.
   */
  type: MessageDeltaContentTextAnnotationsFilePathObjectTypeEnum;
  /**
   * The text in the message content that needs to be replaced.
   */
  text?: string;
  file_path?: MessageDeltaContentTextAnnotationsFilePathObjectFilePath;
  start_index?: number;
  end_index?: number;
}

/**
 * Enum for the type property.
 */
export type MessageDeltaContentTextAnnotationsFilePathObjectTypeEnum = 'file_path';


export interface MessageDeltaContentTextAnnotationsFilePathObjectFilePath {
  /**
   * The ID of the file that was generated.
   */
  file_id?: string;
}


/**
 * The text content that is part of a message.
 */
export interface MessageDeltaContentTextObject {
  /**
   * The index of the content part in the message.
   */
  index: number;
  /**
   * Always `text`.
   */
  type: MessageDeltaContentTextObjectTypeEnum;
  text?: MessageDeltaContentTextObjectText;
}

/**
 * Enum for the type property.
 */
export type MessageDeltaContentTextObjectTypeEnum = 'text';


export interface MessageDeltaContentTextObjectText {
  /**
   * The data that makes up the text.
   */
  value?: string;
  annotations?: Array<MessageDeltaContentTextObjectTextAnnotationsInner>;
}

/**
 * @type MessageDeltaContentTextObjectTextAnnotationsInner
 * @export
 */
export type MessageDeltaContentTextObjectTextAnnotationsInner = MessageDeltaContentTextAnnotationsFileCitationObject | MessageDeltaContentTextAnnotationsFilePathObject;


/**
 * Represents a message delta i.e. any changed fields on a message during streaming. 
 */
export interface MessageDeltaObject {
  /**
   * The identifier of the message, which can be referenced in API endpoints.
   */
  id: string;
  /**
   * The object type, which is always `thread.message.delta`.
   */
  object: MessageDeltaObjectObjectEnum;
  delta: MessageDeltaObjectDelta;
}

/**
 * Enum for the object property.
 */
export type MessageDeltaObjectObjectEnum = 'thread.message.delta';


/**
 * The delta containing the fields that have changed on the Message.
 */
export interface MessageDeltaObjectDelta {
  /**
   * The entity that produced the message. One of `user` or `assistant`.
   */
  role?: MessageDeltaObjectDeltaRoleEnum;
  /**
   * The content of the message in array of text and/or images.
   */
  content?: Array<MessageDeltaObjectDeltaContentInner>;
}

/**
 * Enum for the role property.
 */
export type MessageDeltaObjectDeltaRoleEnum = 'user' | 'assistant';

/**
 * @type MessageDeltaObjectDeltaContentInner
 * @export
 */
export type MessageDeltaObjectDeltaContentInner = MessageDeltaContentImageFileObject | MessageDeltaContentImageUrlObject | MessageDeltaContentRefusalObject | MessageDeltaContentTextObject;


/**
 * Represents a message within a [thread](/docs/api-reference/threads).
 */
export interface MessageObject {
  /**
   * The identifier, which can be referenced in API endpoints.
   */
  id: string;
  /**
   * The object type, which is always `thread.message`.
   */
  object: MessageObjectObjectEnum;
  /**
   * The Unix timestamp (in seconds) for when the message was created.
   */
  created_at: number;
  /**
   * The [thread](/docs/api-reference/threads) ID that this message belongs to.
   */
  thread_id: string;
  /**
   * The status of the message, which can be either `in_progress`, `incomplete`, or `completed`.
   */
  status: MessageObjectStatusEnum;
  incomplete_details: MessageObjectIncompleteDetails;
  /**
   * The Unix timestamp (in seconds) for when the message was completed.
   */
  completed_at: number;
  /**
   * The Unix timestamp (in seconds) for when the message was marked as incomplete.
   */
  incomplete_at: number;
  /**
   * The entity that produced the message. One of `user` or `assistant`.
   */
  role: MessageObjectRoleEnum;
  /**
   * The content of the message in array of text and/or images.
   */
  content: Array<MessageObjectContentInner>;
  /**
   * If applicable, the ID of the [assistant](/docs/api-reference/assistants) that authored this message.
   */
  assistant_id: string;
  /**
   * The ID of the [run](/docs/api-reference/runs) associated with the creation of this message. Value is `null` when messages are created manually using the create message or create thread endpoints.
   */
  run_id: string;
  /**
   * A list of files attached to the message, and the tools they were added to.
   */
  attachments: Array<CreateMessageRequestAttachmentsInner>;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata: object;
}

/**
 * Enum for the object property.
 */
export type MessageObjectObjectEnum = 'thread.message';

/**
 * Enum for the status property.
 */
export type MessageObjectStatusEnum = 'in_progress' | 'incomplete' | 'completed';

/**
 * Enum for the role property.
 */
export type MessageObjectRoleEnum = 'user' | 'assistant';

/**
 * @type MessageObjectContentInner
 * @export
 */
export type MessageObjectContentInner = MessageContentImageFileObject | MessageContentImageUrlObject | MessageContentRefusalObject | MessageContentTextObject;


/**
 * On an incomplete message, details about why the message is incomplete.
 */
export interface MessageObjectIncompleteDetails {
  /**
   * The reason the message is incomplete.
   */
  reason: MessageObjectIncompleteDetailsReasonEnum;
}

/**
 * Enum for the reason property.
 */
export type MessageObjectIncompleteDetailsReasonEnum = 'content_filter' | 'max_tokens' | 'run_cancelled' | 'run_expired' | 'run_failed';


/**
 * The text content that is part of a message.
 */
export interface MessageRequestContentTextObject {
  /**
   * Always `text`.
   */
  type: MessageRequestContentTextObjectTypeEnum;
  /**
   * Text content to be sent to the model
   */
  text: string;
}

/**
 * Enum for the type property.
 */
export type MessageRequestContentTextObjectTypeEnum = 'text';

/**
 * @type MessageStreamEvent
 * @export
 */
export type MessageStreamEvent = MessageStreamEventOneOf | MessageStreamEventOneOf1 | MessageStreamEventOneOf2 | MessageStreamEventOneOf3 | MessageStreamEventOneOf4;


/**
 * Occurs when a [message](/docs/api-reference/messages/object) is created.
 */
export interface MessageStreamEventOneOf {
  event: MessageStreamEventOneOfEventEnum;
  data: MessageObject;
}

/**
 * Enum for the event property.
 */
export type MessageStreamEventOneOfEventEnum = 'thread.message.created';


/**
 * Occurs when a [message](/docs/api-reference/messages/object) moves to an `in_progress` state.
 */
export interface MessageStreamEventOneOf1 {
  event: MessageStreamEventOneOf1EventEnum;
  data: MessageObject;
}

/**
 * Enum for the event property.
 */
export type MessageStreamEventOneOf1EventEnum = 'thread.message.in_progress';


/**
 * Occurs when parts of a [Message](/docs/api-reference/messages/object) are being streamed.
 */
export interface MessageStreamEventOneOf2 {
  event: MessageStreamEventOneOf2EventEnum;
  data: MessageDeltaObject;
}

/**
 * Enum for the event property.
 */
export type MessageStreamEventOneOf2EventEnum = 'thread.message.delta';


/**
 * Occurs when a [message](/docs/api-reference/messages/object) is completed.
 */
export interface MessageStreamEventOneOf3 {
  event: MessageStreamEventOneOf3EventEnum;
  data: MessageObject;
}

/**
 * Enum for the event property.
 */
export type MessageStreamEventOneOf3EventEnum = 'thread.message.completed';


/**
 * Occurs when a [message](/docs/api-reference/messages/object) ends before it is completed.
 */
export interface MessageStreamEventOneOf4 {
  event: MessageStreamEventOneOf4EventEnum;
  data: MessageObject;
}

/**
 * Enum for the event property.
 */
export type MessageStreamEventOneOf4EventEnum = 'thread.message.incomplete';


/**
 * Describes an OpenAI model offering that can be used with the API.
 */
export interface Model {
  /**
   * The model identifier, which can be referenced in the API endpoints.
   */
  id: string;
  /**
   * The Unix timestamp (in seconds) when the model was created.
   */
  created: number;
  /**
   * The object type, which is always \"model\".
   */
  object: ModelObjectEnum;
  /**
   * The organization that owns the model.
   */
  owned_by: string;
}

/**
 * Enum for the object property.
 */
export type ModelObjectEnum = 'model';


export interface ModelError {
  code: string;
  message: string;
  param: string;
  type: string;
}


export interface ModifyAssistantRequest {
  model?: string;
  /**
   * The name of the assistant. The maximum length is 256 characters. 
   */
  name?: string;
  /**
   * The description of the assistant. The maximum length is 512 characters. 
   */
  description?: string;
  /**
   * The system instructions that the assistant uses. The maximum length is 256,000 characters. 
   */
  instructions?: string;
  /**
   * A list of tool enabled on the assistant. There can be a maximum of 128 tools per assistant. Tools can be of types `code_interpreter`, `file_search`, or `function`. 
   */
  tools?: Array<AssistantObjectToolsInner>;
  tool_resources?: ModifyAssistantRequestToolResources;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. 
   */
  temperature?: number;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or temperature but not both. 
   */
  top_p?: number;
  response_format?: AssistantsApiResponseFormatOption;
}


/**
 * A set of resources that are used by the assistant\'s tools. The resources are specific to the type of tool. For example, the `code_interpreter` tool requires a list of file IDs, while the `file_search` tool requires a list of vector store IDs. 
 */
export interface ModifyAssistantRequestToolResources {
  code_interpreter?: ModifyAssistantRequestToolResourcesCodeInterpreter;
  file_search?: ModifyAssistantRequestToolResourcesFileSearch;
}


export interface ModifyAssistantRequestToolResourcesCodeInterpreter {
  /**
   * Overrides the list of [file](/docs/api-reference/files) IDs made available to the `code_interpreter` tool. There can be a maximum of 20 files associated with the tool. 
   */
  file_ids?: Array<string>;
}


export interface ModifyAssistantRequestToolResourcesFileSearch {
  /**
   * Overrides the [vector store](/docs/api-reference/vector-stores/object) attached to this assistant. There can be a maximum of 1 vector store attached to the assistant. 
   */
  vector_store_ids?: Array<string>;
}


export interface ModifyMessageRequest {
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
}


export interface ModifyRunRequest {
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
}


export interface ModifyThreadRequest {
  tool_resources?: ModifyThreadRequestToolResources;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
}


/**
 * A set of resources that are made available to the assistant\'s tools in this thread. The resources are specific to the type of tool. For example, the `code_interpreter` tool requires a list of file IDs, while the `file_search` tool requires a list of vector store IDs. 
 */
export interface ModifyThreadRequestToolResources {
  code_interpreter?: CreateAssistantRequestToolResourcesCodeInterpreter;
  file_search?: ModifyThreadRequestToolResourcesFileSearch;
}


export interface ModifyThreadRequestToolResourcesFileSearch {
  /**
   * The [vector store](/docs/api-reference/vector-stores/object) attached to this thread. There can be a maximum of 1 vector store attached to the thread. 
   */
  vector_store_ids?: Array<string>;
}


/**
 * The `File` object represents a document that has been uploaded to OpenAI.
 */
export interface OpenAIFile {
  /**
   * The file identifier, which can be referenced in the API endpoints.
   */
  id: string;
  /**
   * The size of the file, in bytes.
   */
  bytes: number;
  /**
   * The Unix timestamp (in seconds) for when the file was created.
   */
  created_at: number;
  /**
   * The name of the file.
   */
  filename: string;
  /**
   * The object type, which is always `file`.
   */
  object: OpenAIFileObjectEnum;
  /**
   * The intended purpose of the file. Supported values are `assistants`, `assistants_output`, `batch`, `batch_output`, `fine-tune`, `fine-tune-results` and `vision`.
   */
  purpose: OpenAIFilePurposeEnum;
  /**
   * Deprecated. The current status of the file, which can be either `uploaded`, `processed`, or `error`.
   */
  status: OpenAIFileStatusEnum;
  /**
   * Deprecated. For details on why a fine-tuning training file failed validation, see the `error` field on `fine_tuning.job`.
   */
  status_details?: string;
}

/**
 * Enum for the object property.
 */
export type OpenAIFileObjectEnum = 'file';

/**
 * Enum for the purpose property.
 */
export type OpenAIFilePurposeEnum = 'assistants' | 'assistants_output' | 'batch' | 'batch_output' | 'fine-tune' | 'fine-tune-results' | 'vision';

/**
 * Enum for the status property.
 */
export type OpenAIFileStatusEnum = 'uploaded' | 'processed' | 'error';


/**
 * This is returned when the chunking strategy is unknown. Typically, this is because the file was indexed before the `chunking_strategy` concept was introduced in the API.
 */
export interface OtherChunkingStrategyResponseParam {
  /**
   * Always `other`.
   */
  type: OtherChunkingStrategyResponseParamTypeEnum;
}

/**
 * Enum for the type property.
 */
export type OtherChunkingStrategyResponseParamTypeEnum = 'other';


/**
 * Static predicted output content, such as the content of a text file that is being regenerated. 
 */
export interface PredictionContent {
  /**
   * The type of the predicted content you want to provide. This type is currently always `content`. 
   */
  type: PredictionContentTypeEnum;
  content: PredictionContentContent;
}

/**
 * Enum for the type property.
 */
export type PredictionContentTypeEnum = 'content';

/**
 * @type PredictionContentContent
 * The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly. 
 * @export
 */
export type PredictionContentContent = Array<ChatCompletionRequestMessageContentPartText> | string;


/**
 * Represents an individual project.
 */
export interface Project {
  /**
   * The identifier, which can be referenced in API endpoints
   */
  id: string;
  /**
   * The object type, which is always `organization.project`
   */
  object: ProjectObjectEnum;
  /**
   * The name of the project. This appears in reporting.
   */
  name: string;
  /**
   * The Unix timestamp (in seconds) of when the project was created.
   */
  created_at: number;
  /**
   * The Unix timestamp (in seconds) of when the project was archived or `null`.
   */
  archived_at?: number;
  /**
   * `active` or `archived`
   */
  status: ProjectStatusEnum;
}

/**
 * Enum for the object property.
 */
export type ProjectObjectEnum = 'organization.project';

/**
 * Enum for the status property.
 */
export type ProjectStatusEnum = 'active' | 'archived';


/**
 * Represents an individual API key in a project.
 */
export interface ProjectApiKey {
  /**
   * The object type, which is always `organization.project.api_key`
   */
  object: ProjectApiKeyObjectEnum;
  /**
   * The redacted value of the API key
   */
  redacted_value: string;
  /**
   * The name of the API key
   */
  name: string;
  /**
   * The Unix timestamp (in seconds) of when the API key was created
   */
  created_at: number;
  /**
   * The identifier, which can be referenced in API endpoints
   */
  id: string;
  owner: ProjectApiKeyOwner;
}

/**
 * Enum for the object property.
 */
export type ProjectApiKeyObjectEnum = 'organization.project.api_key';


export interface ProjectApiKeyDeleteResponse {
  object: ProjectApiKeyDeleteResponseObjectEnum;
  id: string;
  deleted: boolean;
}

/**
 * Enum for the object property.
 */
export type ProjectApiKeyDeleteResponseObjectEnum = 'organization.project.api_key.deleted';


export interface ProjectApiKeyListResponse {
  object: ProjectApiKeyListResponseObjectEnum;
  data: Array<ProjectApiKey>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}

/**
 * Enum for the object property.
 */
export type ProjectApiKeyListResponseObjectEnum = 'list';


export interface ProjectApiKeyOwner {
  /**
   * `user` or `service_account`
   */
  type?: ProjectApiKeyOwnerTypeEnum;
  user?: ProjectUser;
  service_account?: ProjectServiceAccount;
}

/**
 * Enum for the type property.
 */
export type ProjectApiKeyOwnerTypeEnum = 'user' | 'service_account';


export interface ProjectCreateRequest {
  /**
   * The friendly name of the project, this name appears in reports.
   */
  name: string;
}


export interface ProjectListResponse {
  object: ProjectListResponseObjectEnum;
  data: Array<Project>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}

/**
 * Enum for the object property.
 */
export type ProjectListResponseObjectEnum = 'list';


/**
 * Represents a project rate limit config.
 */
export interface ProjectRateLimit {
  /**
   * The object type, which is always `project.rate_limit`
   */
  object: ProjectRateLimitObjectEnum;
  /**
   * The identifier, which can be referenced in API endpoints.
   */
  id: string;
  /**
   * The model this rate limit applies to.
   */
  model: string;
  /**
   * The maximum requests per minute.
   */
  max_requests_per_1_minute: number;
  /**
   * The maximum tokens per minute.
   */
  max_tokens_per_1_minute: number;
  /**
   * The maximum images per minute. Only present for relevant models.
   */
  max_images_per_1_minute?: number;
  /**
   * The maximum audio megabytes per minute. Only present for relevant models.
   */
  max_audio_megabytes_per_1_minute?: number;
  /**
   * The maximum requests per day. Only present for relevant models.
   */
  max_requests_per_1_day?: number;
  /**
   * The maximum batch input tokens per day. Only present for relevant models.
   */
  batch_1_day_max_input_tokens?: number;
}

/**
 * Enum for the object property.
 */
export type ProjectRateLimitObjectEnum = 'project.rate_limit';


export interface ProjectRateLimitListResponse {
  object: ProjectRateLimitListResponseObjectEnum;
  data: Array<ProjectRateLimit>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}

/**
 * Enum for the object property.
 */
export type ProjectRateLimitListResponseObjectEnum = 'list';


export interface ProjectRateLimitUpdateRequest {
  /**
   * The maximum requests per minute.
   */
  max_requests_per_1_minute?: number;
  /**
   * The maximum tokens per minute.
   */
  max_tokens_per_1_minute?: number;
  /**
   * The maximum images per minute. Only relevant for certain models.
   */
  max_images_per_1_minute?: number;
  /**
   * The maximum audio megabytes per minute. Only relevant for certain models.
   */
  max_audio_megabytes_per_1_minute?: number;
  /**
   * The maximum requests per day. Only relevant for certain models.
   */
  max_requests_per_1_day?: number;
  /**
   * The maximum batch input tokens per day. Only relevant for certain models.
   */
  batch_1_day_max_input_tokens?: number;
}


/**
 * Represents an individual service account in a project.
 */
export interface ProjectServiceAccount {
  /**
   * The object type, which is always `organization.project.service_account`
   */
  object: ProjectServiceAccountObjectEnum;
  /**
   * The identifier, which can be referenced in API endpoints
   */
  id: string;
  /**
   * The name of the service account
   */
  name: string;
  /**
   * `owner` or `member`
   */
  role: ProjectServiceAccountRoleEnum;
  /**
   * The Unix timestamp (in seconds) of when the service account was created
   */
  created_at: number;
}

/**
 * Enum for the object property.
 */
export type ProjectServiceAccountObjectEnum = 'organization.project.service_account';

/**
 * Enum for the role property.
 */
export type ProjectServiceAccountRoleEnum = 'owner' | 'member';


export interface ProjectServiceAccountApiKey {
  /**
   * The object type, which is always `organization.project.service_account.api_key`
   */
  object: ProjectServiceAccountApiKeyObjectEnum;
  value: string;
  name: string;
  created_at: number;
  id: string;
}

/**
 * Enum for the object property.
 */
export type ProjectServiceAccountApiKeyObjectEnum = 'organization.project.service_account.api_key';


export interface ProjectServiceAccountCreateRequest {
  /**
   * The name of the service account being created.
   */
  name: string;
}


export interface ProjectServiceAccountCreateResponse {
  object: ProjectServiceAccountCreateResponseObjectEnum;
  id: string;
  name: string;
  /**
   * Service accounts can only have one role of type `member`
   */
  role: ProjectServiceAccountCreateResponseRoleEnum;
  created_at: number;
  api_key: ProjectServiceAccountApiKey;
}

/**
 * Enum for the object property.
 */
export type ProjectServiceAccountCreateResponseObjectEnum = 'organization.project.service_account';

/**
 * Enum for the role property.
 */
export type ProjectServiceAccountCreateResponseRoleEnum = 'member';


export interface ProjectServiceAccountDeleteResponse {
  object: ProjectServiceAccountDeleteResponseObjectEnum;
  id: string;
  deleted: boolean;
}

/**
 * Enum for the object property.
 */
export type ProjectServiceAccountDeleteResponseObjectEnum = 'organization.project.service_account.deleted';


export interface ProjectServiceAccountListResponse {
  object: ProjectServiceAccountListResponseObjectEnum;
  data: Array<ProjectServiceAccount>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}

/**
 * Enum for the object property.
 */
export type ProjectServiceAccountListResponseObjectEnum = 'list';


export interface ProjectUpdateRequest {
  /**
   * The updated name of the project, this name appears in reports.
   */
  name: string;
}


/**
 * Represents an individual user in a project.
 */
export interface ProjectUser {
  /**
   * The object type, which is always `organization.project.user`
   */
  object: ProjectUserObjectEnum;
  /**
   * The identifier, which can be referenced in API endpoints
   */
  id: string;
  /**
   * The name of the user
   */
  name: string;
  /**
   * The email address of the user
   */
  email: string;
  /**
   * `owner` or `member`
   */
  role: ProjectUserRoleEnum;
  /**
   * The Unix timestamp (in seconds) of when the project was added.
   */
  added_at: number;
}

/**
 * Enum for the object property.
 */
export type ProjectUserObjectEnum = 'organization.project.user';

/**
 * Enum for the role property.
 */
export type ProjectUserRoleEnum = 'owner' | 'member';


export interface ProjectUserCreateRequest {
  /**
   * The ID of the user.
   */
  user_id: string;
  /**
   * `owner` or `member`
   */
  role: ProjectUserCreateRequestRoleEnum;
}

/**
 * Enum for the role property.
 */
export type ProjectUserCreateRequestRoleEnum = 'owner' | 'member';


export interface ProjectUserDeleteResponse {
  object: ProjectUserDeleteResponseObjectEnum;
  id: string;
  deleted: boolean;
}

/**
 * Enum for the object property.
 */
export type ProjectUserDeleteResponseObjectEnum = 'organization.project.user.deleted';


export interface ProjectUserListResponse {
  object: string;
  data: Array<ProjectUser>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}


export interface ProjectUserUpdateRequest {
  /**
   * `owner` or `member`
   */
  role: ProjectUserUpdateRequestRoleEnum;
}

/**
 * Enum for the role property.
 */
export type ProjectUserUpdateRequestRoleEnum = 'owner' | 'member';


/**
 * Add a new Item to the Conversation\'s context, including messages, function  calls, and function call responses. This event can be used both to populate a  \"history\" of the conversation and to add new items mid-stream, but has the  current limitation that it cannot populate assistant audio messages.  If successful, the server will respond with a `conversation.item.created`  event, otherwise an `error` event will be sent. 
 */
export interface RealtimeClientEventConversationItemCreate {
  /**
   * Optional client-generated ID used to identify this event.
   */
  event_id?: string;
  /**
   * The event type, must be `conversation.item.create`.
   */
  type: RealtimeClientEventConversationItemCreateTypeEnum;
  /**
   * The ID of the preceding item after which the new item will be inserted.  If not set, the new item will be appended to the end of the conversation.  If set, it allows an item to be inserted mid-conversation. If the ID  cannot be found, an error will be returned and the item will not be added. 
   */
  previous_item_id?: string;
  item: RealtimeConversationItem;
}

/**
 * Enum for the type property.
 */
export type RealtimeClientEventConversationItemCreateTypeEnum = 'conversation.item.create';


/**
 * Send this event when you want to remove any item from the conversation  history. The server will respond with a `conversation.item.deleted` event,  unless the item does not exist in the conversation history, in which case the  server will respond with an error. 
 */
export interface RealtimeClientEventConversationItemDelete {
  /**
   * Optional client-generated ID used to identify this event.
   */
  event_id?: string;
  /**
   * The event type, must be `conversation.item.delete`.
   */
  type: RealtimeClientEventConversationItemDeleteTypeEnum;
  /**
   * The ID of the item to delete.
   */
  item_id: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeClientEventConversationItemDeleteTypeEnum = 'conversation.item.delete';


/**
 * Send this event to truncate a previous assistant messages audio. The server  will produce audio faster than realtime, so this event is useful when the user  interrupts to truncate audio that has already been sent to the client but not  yet played. This will synchronize the server\'s understanding of the audio with  the client\'s playback.  Truncating audio will delete the server-side text transcript to ensure there  is not text in the context that hasn\'t been heard by the user.  If successful, the server will respond with a `conversation.item.truncated`  event.  
 */
export interface RealtimeClientEventConversationItemTruncate {
  /**
   * Optional client-generated ID used to identify this event.
   */
  event_id?: string;
  /**
   * The event type, must be `conversation.item.truncate`.
   */
  type: RealtimeClientEventConversationItemTruncateTypeEnum;
  /**
   * The ID of the assistant message item to truncate. Only assistant message  items can be truncated. 
   */
  item_id: string;
  /**
   * The index of the content part to truncate. Set this to 0.
   */
  content_index: number;
  /**
   * Inclusive duration up to which audio is truncated, in milliseconds. If  the audio_end_ms is greater than the actual audio duration, the server  will respond with an error. 
   */
  audio_end_ms: number;
}

/**
 * Enum for the type property.
 */
export type RealtimeClientEventConversationItemTruncateTypeEnum = 'conversation.item.truncate';


/**
 * Send this event to append audio bytes to the input audio buffer. The audio  buffer is temporary storage you can write to and later commit. In Server VAD  mode, the audio buffer is used to detect speech and the server will decide  when to commit. When Server VAD is disabled, you must commit the audio buffer manually.  The client may choose how much audio to place in each event up to a maximum  of 15 MiB, for example streaming smaller chunks from the client may allow the  VAD to be more responsive. Unlike made other client events, the server will  not send a confirmation response to this event. 
 */
export interface RealtimeClientEventInputAudioBufferAppend {
  /**
   * Optional client-generated ID used to identify this event.
   */
  event_id?: string;
  /**
   * The event type, must be `input_audio_buffer.append`.
   */
  type: RealtimeClientEventInputAudioBufferAppendTypeEnum;
  /**
   * Base64-encoded audio bytes. This must be in the format specified by the  `input_audio_format` field in the session configuration. 
   */
  audio: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeClientEventInputAudioBufferAppendTypeEnum = 'input_audio_buffer.append';


/**
 * Send this event to clear the audio bytes in the buffer. The server will  respond with an `input_audio_buffer.cleared` event. 
 */
export interface RealtimeClientEventInputAudioBufferClear {
  /**
   * Optional client-generated ID used to identify this event.
   */
  event_id?: string;
  /**
   * The event type, must be `input_audio_buffer.clear`.
   */
  type: RealtimeClientEventInputAudioBufferClearTypeEnum;
}

/**
 * Enum for the type property.
 */
export type RealtimeClientEventInputAudioBufferClearTypeEnum = 'input_audio_buffer.clear';


/**
 * Send this event to commit the user input audio buffer, which will create a  new user message item in the conversation. This event will produce an error  if the input audio buffer is empty. When in Server VAD mode, the client does  not need to send this event, the server will commit the audio buffer  automatically.  Committing the input audio buffer will trigger input audio transcription  (if enabled in session configuration), but it will not create a response  from the model. The server will respond with an `input_audio_buffer.committed`  event. 
 */
export interface RealtimeClientEventInputAudioBufferCommit {
  /**
   * Optional client-generated ID used to identify this event.
   */
  event_id?: string;
  /**
   * The event type, must be `input_audio_buffer.commit`.
   */
  type: RealtimeClientEventInputAudioBufferCommitTypeEnum;
}

/**
 * Enum for the type property.
 */
export type RealtimeClientEventInputAudioBufferCommitTypeEnum = 'input_audio_buffer.commit';


/**
 * Send this event to cancel an in-progress response. The server will respond  with a `response.cancelled` event or an error if there is no response to  cancel. 
 */
export interface RealtimeClientEventResponseCancel {
  /**
   * Optional client-generated ID used to identify this event.
   */
  event_id?: string;
  /**
   * The event type, must be `response.cancel`.
   */
  type: RealtimeClientEventResponseCancelTypeEnum;
  /**
   * A specific response ID to cancel - if not provided, will cancel an  in-progress response in the default conversation. 
   */
  response_id?: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeClientEventResponseCancelTypeEnum = 'response.cancel';


/**
 * This event instructs the server to create a Response, which means triggering  model inference. When in Server VAD mode, the server will create Responses  automatically.  A Response will include at least one Item, and may have two, in which case  the second will be a function call. These Items will be appended to the  conversation history.  The server will respond with a `response.created` event, events for Items  and content created, and finally a `response.done` event to indicate the  Response is complete.  The `response.create` event includes inference configuration like  `instructions`, and `temperature`. These fields will override the Session\'s  configuration for this Response only. 
 */
export interface RealtimeClientEventResponseCreate {
  /**
   * Optional client-generated ID used to identify this event.
   */
  event_id?: string;
  /**
   * The event type, must be `response.create`.
   */
  type: RealtimeClientEventResponseCreateTypeEnum;
  response?: RealtimeResponseCreateParams;
}

/**
 * Enum for the type property.
 */
export type RealtimeClientEventResponseCreateTypeEnum = 'response.create';


/**
 * Send this event to update the sessions default configuration. The client may  send this event at any time to update the session configuration, and any  field may be updated at any time, except for \"voice\". The server will respond  with a `session.updated` event that shows the full effective configuration.  Only fields that are present are updated, thus the correct way to clear a  field like \"instructions\" is to pass an empty string. 
 */
export interface RealtimeClientEventSessionUpdate {
  /**
   * Optional client-generated ID used to identify this event.
   */
  event_id?: string;
  /**
   * The event type, must be `session.update`.
   */
  type: RealtimeClientEventSessionUpdateTypeEnum;
  session: RealtimeSessionCreateRequest;
}

/**
 * Enum for the type property.
 */
export type RealtimeClientEventSessionUpdateTypeEnum = 'session.update';


/**
 * The item to add to the conversation.
 */
export interface RealtimeConversationItem {
  /**
   * The unique ID of the item, this can be generated by the client to help  manage server-side context, but is not required because the server will  generate one if not provided. 
   */
  id?: string;
  /**
   * The type of the item (`message`, `function_call`, `function_call_output`). 
   */
  type?: RealtimeConversationItemTypeEnum;
  /**
   * Identifier for the API object being returned - always `realtime.item`. 
   */
  object?: RealtimeConversationItemObjectEnum;
  /**
   * The status of the item (`completed`, `incomplete`). These have no effect  on the conversation, but are accepted for consistency with the  `conversation.item.created` event. 
   */
  status?: RealtimeConversationItemStatusEnum;
  /**
   * The role of the message sender (`user`, `assistant`, `system`), only  applicable for `message` items. 
   */
  role?: RealtimeConversationItemRoleEnum;
  /**
   * The content of the message, applicable for `message` items.  - Message items of role `system` support only `input_text` content - Message items of role `user` support `input_text` and `input_audio`    content - Message items of role `assistant` support `text` content. 
   */
  content?: Array<RealtimeConversationItemContentInner>;
  /**
   * The ID of the function call (for `function_call` and  `function_call_output` items). If passed on a `function_call_output`  item, the server will check that a `function_call` item with the same  ID exists in the conversation history. 
   */
  call_id?: string;
  /**
   * The name of the function being called (for `function_call` items). 
   */
  name?: string;
  /**
   * The arguments of the function call (for `function_call` items). 
   */
  arguments?: string;
  /**
   * The output of the function call (for `function_call_output` items). 
   */
  output?: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeConversationItemTypeEnum = 'message' | 'function_call' | 'function_call_output';

/**
 * Enum for the object property.
 */
export type RealtimeConversationItemObjectEnum = 'realtime.item';

/**
 * Enum for the status property.
 */
export type RealtimeConversationItemStatusEnum = 'completed' | 'incomplete';

/**
 * Enum for the role property.
 */
export type RealtimeConversationItemRoleEnum = 'user' | 'assistant' | 'system';


export interface RealtimeConversationItemContentInner {
  /**
   * The content type (`input_text`, `input_audio`, `item_reference`, `text`). 
   */
  type?: RealtimeConversationItemContentInnerTypeEnum;
  /**
   * The text content, used for `input_text` and `text` content types. 
   */
  text?: string;
  /**
   * ID of a previous conversation item to reference (for `item_reference` content types in `response.create` events). These can reference both client and server created items. 
   */
  id?: string;
  /**
   * Base64-encoded audio bytes, used for `input_audio` content type. 
   */
  audio?: string;
  /**
   * The transcript of the audio, used for `input_audio` content type. 
   */
  transcript?: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeConversationItemContentInnerTypeEnum = 'input_audio' | 'input_text' | 'item_reference' | 'text';


/**
 * The response resource.
 */
export interface RealtimeResponse {
  /**
   * The unique ID of the response.
   */
  id?: string;
  /**
   * The object type, must be `realtime.response`.
   */
  object?: RealtimeResponseObjectEnum;
  /**
   * The final status of the response (`completed`, `cancelled`, `failed`, or  `incomplete`). 
   */
  status?: RealtimeResponseStatusEnum;
  status_details?: RealtimeResponseStatusDetails;
  /**
   * The list of output items generated by the response.
   */
  output?: Array<RealtimeConversationItem>;
  /**
   * Developer-provided string key-value pairs associated with this response. 
   */
  metadata?: object;
  usage?: RealtimeResponseUsage;
}

/**
 * Enum for the object property.
 */
export type RealtimeResponseObjectEnum = 'realtime.response';

/**
 * Enum for the status property.
 */
export type RealtimeResponseStatusEnum = 'completed' | 'cancelled' | 'failed' | 'incomplete';


/**
 * Create a new Realtime response with these parameters
 */
export interface RealtimeResponseCreateParams {
  /**
   * The set of modalities the model can respond with. To disable audio, set this to [\"text\"]. 
   */
  modalities?: Array<RealtimeResponseCreateParamsModalitiesEnum>;
  /**
   * The default system instructions (i.e. system message) prepended to model  calls. This field allows the client to guide the model on desired  responses. The model can be instructed on response content and format,  (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of good  responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion  into your voice\", \"laugh frequently\"). The instructions are not guaranteed  to be followed by the model, but they provide guidance to the model on the  desired behavior.  Note that the server sets default instructions which will be used if this  field is not set and are visible in the `session.created` event at the  start of the session. 
   */
  instructions?: string;
  /**
   * The voice the model uses to respond. Voice cannot be changed during the  session once the model has responded with audio at least once. Current  voice options are `alloy`, `ash`, `ballad`, `coral`, `echo` `sage`,  `shimmer` and `verse`. 
   */
  voice?: RealtimeResponseCreateParamsVoiceEnum;
  /**
   * The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. 
   */
  output_audio_format?: RealtimeResponseCreateParamsOutputAudioFormatEnum;
  /**
   * Tools (functions) available to the model.
   */
  tools?: Array<RealtimeResponseCreateParamsToolsInner>;
  /**
   * How the model chooses tools. Options are `auto`, `none`, `required`, or  specify a function, like `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`. 
   */
  tool_choice?: string;
  /**
   * Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8. 
   */
  temperature?: number;
  max_response_output_tokens?: RealtimeResponseCreateParamsMaxResponseOutputTokens;
  conversation?: RealtimeResponseCreateParamsConversation;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
  /**
   * Input items to include in the prompt for the model. Creates a new context for this response, without including the default conversation. Can include references to items from the default conversation. 
   */
  input?: Array<RealtimeConversationItem>;
}

/**
 * Enum for the modalities property.
 */
export type RealtimeResponseCreateParamsModalitiesEnum = 'text' | 'audio';

/**
 * Enum for the voice property.
 */
export type RealtimeResponseCreateParamsVoiceEnum = 'alloy' | 'ash' | 'ballad' | 'coral' | 'echo' | 'sage' | 'shimmer' | 'verse';

/**
 * Enum for the output_audio_format property.
 */
export type RealtimeResponseCreateParamsOutputAudioFormatEnum = 'pcm16' | 'g711_ulaw' | 'g711_alaw';

/**
 * @type RealtimeResponseCreateParamsConversation
 * Controls which conversation the response is added to. Currently supports `auto` and `none`, with `auto` as the default value. The `auto` value means that the contents of the response will be added to the default conversation. Set this to `none` to create an out-of-band response which  will not add items to default conversation. 
 * @export
 */
export type RealtimeResponseCreateParamsConversation = string;

/**
 * @type RealtimeResponseCreateParamsMaxResponseOutputTokens
 * Maximum number of output tokens for a single assistant response, inclusive of tool calls. Provide an integer between 1 and 4096 to limit output tokens, or `inf` for the maximum available tokens for a given model. Defaults to `inf`. 
 * @export
 */
export type RealtimeResponseCreateParamsMaxResponseOutputTokens = number | string;


export interface RealtimeResponseCreateParamsToolsInner {
  /**
   * The type of the tool, i.e. `function`.
   */
  type?: RealtimeResponseCreateParamsToolsInnerTypeEnum;
  /**
   * The name of the function.
   */
  name?: string;
  /**
   * The description of the function, including guidance on when and how  to call it, and guidance about what to tell the user when calling  (if anything). 
   */
  description?: string;
  /**
   * Parameters of the function in JSON Schema.
   */
  parameters?: object;
}

/**
 * Enum for the type property.
 */
export type RealtimeResponseCreateParamsToolsInnerTypeEnum = 'function';


/**
 * Additional details about the status.
 */
export interface RealtimeResponseStatusDetails {
  /**
   * The type of error that caused the response to fail, corresponding  with the `status` field (`completed`, `cancelled`, `incomplete`,  `failed`). 
   */
  type?: RealtimeResponseStatusDetailsTypeEnum;
  /**
   * The reason the Response did not complete. For a `cancelled` Response,  one of `turn_detected` (the server VAD detected a new start of speech)  or `client_cancelled` (the client sent a cancel event). For an  `incomplete` Response, one of `max_output_tokens` or `content_filter`  (the server-side safety filter activated and cut off the response). 
   */
  reason?: RealtimeResponseStatusDetailsReasonEnum;
  error?: RealtimeResponseStatusDetailsError;
}

/**
 * Enum for the type property.
 */
export type RealtimeResponseStatusDetailsTypeEnum = 'completed' | 'cancelled' | 'failed' | 'incomplete';

/**
 * Enum for the reason property.
 */
export type RealtimeResponseStatusDetailsReasonEnum = 'turn_detected' | 'client_cancelled' | 'max_output_tokens' | 'content_filter';


/**
 * A description of the error that caused the response to fail,  populated when the `status` is `failed`. 
 */
export interface RealtimeResponseStatusDetailsError {
  /**
   * The type of error.
   */
  type?: string;
  /**
   * Error code, if any.
   */
  code?: string;
}


/**
 * Usage statistics for the Response, this will correspond to billing. A  Realtime API session will maintain a conversation context and append new  Items to the Conversation, thus output from previous turns (text and  audio tokens) will become the input for later turns. 
 */
export interface RealtimeResponseUsage {
  /**
   * The total number of tokens in the Response including input and output  text and audio tokens. 
   */
  total_tokens?: number;
  /**
   * The number of input tokens used in the Response, including text and  audio tokens. 
   */
  input_tokens?: number;
  /**
   * The number of output tokens sent in the Response, including text and  audio tokens. 
   */
  output_tokens?: number;
  input_token_details?: RealtimeResponseUsageInputTokenDetails;
  output_token_details?: RealtimeResponseUsageOutputTokenDetails;
}


/**
 * Details about the input tokens used in the Response.
 */
export interface RealtimeResponseUsageInputTokenDetails {
  /**
   * The number of cached tokens used in the Response.
   */
  cached_tokens?: number;
  /**
   * The number of text tokens used in the Response.
   */
  text_tokens?: number;
  /**
   * The number of audio tokens used in the Response.
   */
  audio_tokens?: number;
}


/**
 * Details about the output tokens used in the Response.
 */
export interface RealtimeResponseUsageOutputTokenDetails {
  /**
   * The number of text tokens used in the Response.
   */
  text_tokens?: number;
  /**
   * The number of audio tokens used in the Response.
   */
  audio_tokens?: number;
}


/**
 * Returned when a conversation is created. Emitted right after session creation. 
 */
export interface RealtimeServerEventConversationCreated {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `conversation.created`.
   */
  type: RealtimeServerEventConversationCreatedTypeEnum;
  conversation: RealtimeServerEventConversationCreatedConversation;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventConversationCreatedTypeEnum = 'conversation.created';


/**
 * The conversation resource.
 */
export interface RealtimeServerEventConversationCreatedConversation {
  /**
   * The unique ID of the conversation.
   */
  id?: string;
  /**
   * The object type, must be `realtime.conversation`.
   */
  object?: string;
}


/**
 * Returned when a conversation item is created. There are several scenarios that  produce this event:   - The server is generating a Response, which if successful will produce      either one or two Items, which will be of type `message`      (role `assistant`) or type `function_call`.   - The input audio buffer has been committed, either by the client or the      server (in `server_vad` mode). The server will take the content of the      input audio buffer and add it to a new user message Item.   - The client has sent a `conversation.item.create` event to add a new Item      to the Conversation. 
 */
export interface RealtimeServerEventConversationItemCreated {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `conversation.item.created`.
   */
  type: RealtimeServerEventConversationItemCreatedTypeEnum;
  /**
   * The ID of the preceding item in the Conversation context, allows the  client to understand the order of the conversation. 
   */
  previous_item_id: string;
  item: RealtimeConversationItem;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventConversationItemCreatedTypeEnum = 'conversation.item.created';


/**
 * Returned when an item in the conversation is deleted by the client with a  `conversation.item.delete` event. This event is used to synchronize the  server\'s understanding of the conversation history with the client\'s view. 
 */
export interface RealtimeServerEventConversationItemDeleted {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `conversation.item.deleted`.
   */
  type: RealtimeServerEventConversationItemDeletedTypeEnum;
  /**
   * The ID of the item that was deleted.
   */
  item_id: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventConversationItemDeletedTypeEnum = 'conversation.item.deleted';


/**
 * This event is the output of audio transcription for user audio written to the  user audio buffer. Transcription begins when the input audio buffer is  committed by the client or server (in `server_vad` mode). Transcription runs  asynchronously with Response creation, so this event may come before or after  the Response events.  Realtime API models accept audio natively, and thus input transcription is a  separate process run on a separate ASR (Automatic Speech Recognition) model,  currently always `whisper-1`. Thus the transcript may diverge somewhat from  the model\'s interpretation, and should be treated as a rough guide. 
 */
export interface RealtimeServerEventConversationItemInputAudioTranscriptionCompleted {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `conversation.item.input_audio_transcription.completed`. 
   */
  type: RealtimeServerEventConversationItemInputAudioTranscriptionCompletedTypeEnum;
  /**
   * The ID of the user message item containing the audio.
   */
  item_id: string;
  /**
   * The index of the content part containing the audio.
   */
  content_index: number;
  /**
   * The transcribed text.
   */
  transcript: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventConversationItemInputAudioTranscriptionCompletedTypeEnum = 'conversation.item.input_audio_transcription.completed';


/**
 * Returned when input audio transcription is configured, and a transcription  request for a user message failed. These events are separate from other  `error` events so that the client can identify the related Item. 
 */
export interface RealtimeServerEventConversationItemInputAudioTranscriptionFailed {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `conversation.item.input_audio_transcription.failed`. 
   */
  type: RealtimeServerEventConversationItemInputAudioTranscriptionFailedTypeEnum;
  /**
   * The ID of the user message item.
   */
  item_id: string;
  /**
   * The index of the content part containing the audio.
   */
  content_index: number;
  error: RealtimeServerEventConversationItemInputAudioTranscriptionFailedError;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventConversationItemInputAudioTranscriptionFailedTypeEnum = 'conversation.item.input_audio_transcription.failed';


/**
 * Details of the transcription error.
 */
export interface RealtimeServerEventConversationItemInputAudioTranscriptionFailedError {
  /**
   * The type of error.
   */
  type?: string;
  /**
   * Error code, if any.
   */
  code?: string;
  /**
   * A human-readable error message.
   */
  message?: string;
  /**
   * Parameter related to the error, if any.
   */
  param?: string;
}


/**
 * Returned when an earlier assistant audio message item is truncated by the  client with a `conversation.item.truncate` event. This event is used to  synchronize the server\'s understanding of the audio with the client\'s playback.  This action will truncate the audio and remove the server-side text transcript  to ensure there is no text in the context that hasn\'t been heard by the user. 
 */
export interface RealtimeServerEventConversationItemTruncated {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `conversation.item.truncated`.
   */
  type: RealtimeServerEventConversationItemTruncatedTypeEnum;
  /**
   * The ID of the assistant message item that was truncated.
   */
  item_id: string;
  /**
   * The index of the content part that was truncated.
   */
  content_index: number;
  /**
   * The duration up to which the audio was truncated, in milliseconds. 
   */
  audio_end_ms: number;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventConversationItemTruncatedTypeEnum = 'conversation.item.truncated';


/**
 * Returned when an error occurs, which could be a client problem or a server  problem. Most errors are recoverable and the session will stay open, we  recommend to implementors to monitor and log error messages by default. 
 */
export interface RealtimeServerEventError {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `error`.
   */
  type: RealtimeServerEventErrorTypeEnum;
  error: RealtimeServerEventErrorError;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventErrorTypeEnum = 'error';


/**
 * Details of the error.
 */
export interface RealtimeServerEventErrorError {
  /**
   * The type of error (e.g., \"invalid_request_error\", \"server_error\"). 
   */
  type: string;
  /**
   * Error code, if any.
   */
  code?: string;
  /**
   * A human-readable error message.
   */
  message: string;
  /**
   * Parameter related to the error, if any.
   */
  param?: string;
  /**
   * The event_id of the client event that caused the error, if applicable. 
   */
  event_id?: string;
}


/**
 * Returned when the input audio buffer is cleared by the client with a  `input_audio_buffer.clear` event. 
 */
export interface RealtimeServerEventInputAudioBufferCleared {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `input_audio_buffer.cleared`.
   */
  type: RealtimeServerEventInputAudioBufferClearedTypeEnum;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventInputAudioBufferClearedTypeEnum = 'input_audio_buffer.cleared';


/**
 * Returned when an input audio buffer is committed, either by the client or  automatically in server VAD mode. The `item_id` property is the ID of the user message item that will be created, thus a `conversation.item.created` event  will also be sent to the client. 
 */
export interface RealtimeServerEventInputAudioBufferCommitted {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `input_audio_buffer.committed`.
   */
  type: RealtimeServerEventInputAudioBufferCommittedTypeEnum;
  /**
   * The ID of the preceding item after which the new item will be inserted. 
   */
  previous_item_id: string;
  /**
   * The ID of the user message item that will be created.
   */
  item_id: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventInputAudioBufferCommittedTypeEnum = 'input_audio_buffer.committed';


/**
 * Sent by the server when in `server_vad` mode to indicate that speech has been  detected in the audio buffer. This can happen any time audio is added to the  buffer (unless speech is already detected). The client may want to use this  event to interrupt audio playback or provide visual feedback to the user.   The client should expect to receive a `input_audio_buffer.speech_stopped` event  when speech stops. The `item_id` property is the ID of the user message item  that will be created when speech stops and will also be included in the  `input_audio_buffer.speech_stopped` event (unless the client manually commits  the audio buffer during VAD activation). 
 */
export interface RealtimeServerEventInputAudioBufferSpeechStarted {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `input_audio_buffer.speech_started`.
   */
  type: RealtimeServerEventInputAudioBufferSpeechStartedTypeEnum;
  /**
   * Milliseconds from the start of all audio written to the buffer during the  session when speech was first detected. This will correspond to the  beginning of audio sent to the model, and thus includes the  `prefix_padding_ms` configured in the Session. 
   */
  audio_start_ms: number;
  /**
   * The ID of the user message item that will be created when speech stops. 
   */
  item_id: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventInputAudioBufferSpeechStartedTypeEnum = 'input_audio_buffer.speech_started';


/**
 * Returned in `server_vad` mode when the server detects the end of speech in  the audio buffer. The server will also send an `conversation.item.created`  event with the user message item that is created from the audio buffer. 
 */
export interface RealtimeServerEventInputAudioBufferSpeechStopped {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `input_audio_buffer.speech_stopped`.
   */
  type: RealtimeServerEventInputAudioBufferSpeechStoppedTypeEnum;
  /**
   * Milliseconds since the session started when speech stopped. This will  correspond to the end of audio sent to the model, and thus includes the  `min_silence_duration_ms` configured in the Session. 
   */
  audio_end_ms: number;
  /**
   * The ID of the user message item that will be created.
   */
  item_id: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventInputAudioBufferSpeechStoppedTypeEnum = 'input_audio_buffer.speech_stopped';


/**
 * Emitted at the beginning of a Response to indicate the updated rate limits.  When a Response is created some tokens will be \"reserved\" for the output  tokens, the rate limits shown here reflect that reservation, which is then  adjusted accordingly once the Response is completed. 
 */
export interface RealtimeServerEventRateLimitsUpdated {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `rate_limits.updated`.
   */
  type: RealtimeServerEventRateLimitsUpdatedTypeEnum;
  /**
   * List of rate limit information.
   */
  rate_limits: Array<RealtimeServerEventRateLimitsUpdatedRateLimitsInner>;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventRateLimitsUpdatedTypeEnum = 'rate_limits.updated';


export interface RealtimeServerEventRateLimitsUpdatedRateLimitsInner {
  /**
   * The name of the rate limit (`requests`, `tokens`). 
   */
  name?: RealtimeServerEventRateLimitsUpdatedRateLimitsInnerNameEnum;
  /**
   * The maximum allowed value for the rate limit.
   */
  limit?: number;
  /**
   * The remaining value before the limit is reached.
   */
  remaining?: number;
  /**
   * Seconds until the rate limit resets.
   */
  reset_seconds?: number;
}

/**
 * Enum for the name property.
 */
export type RealtimeServerEventRateLimitsUpdatedRateLimitsInnerNameEnum = 'requests' | 'tokens';


/**
 * Returned when the model-generated audio is updated.
 */
export interface RealtimeServerEventResponseAudioDelta {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `response.audio.delta`.
   */
  type: RealtimeServerEventResponseAudioDeltaTypeEnum;
  /**
   * The ID of the response.
   */
  response_id: string;
  /**
   * The ID of the item.
   */
  item_id: string;
  /**
   * The index of the output item in the response.
   */
  output_index: number;
  /**
   * The index of the content part in the item\'s content array.
   */
  content_index: number;
  /**
   * Base64-encoded audio data delta.
   */
  delta: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseAudioDeltaTypeEnum = 'response.audio.delta';


/**
 * Returned when the model-generated audio is done. Also emitted when a Response is interrupted, incomplete, or cancelled. 
 */
export interface RealtimeServerEventResponseAudioDone {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `response.audio.done`.
   */
  type: RealtimeServerEventResponseAudioDoneTypeEnum;
  /**
   * The ID of the response.
   */
  response_id: string;
  /**
   * The ID of the item.
   */
  item_id: string;
  /**
   * The index of the output item in the response.
   */
  output_index: number;
  /**
   * The index of the content part in the item\'s content array.
   */
  content_index: number;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseAudioDoneTypeEnum = 'response.audio.done';


/**
 * Returned when the model-generated transcription of audio output is updated. 
 */
export interface RealtimeServerEventResponseAudioTranscriptDelta {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `response.audio_transcript.delta`.
   */
  type: RealtimeServerEventResponseAudioTranscriptDeltaTypeEnum;
  /**
   * The ID of the response.
   */
  response_id: string;
  /**
   * The ID of the item.
   */
  item_id: string;
  /**
   * The index of the output item in the response.
   */
  output_index: number;
  /**
   * The index of the content part in the item\'s content array.
   */
  content_index: number;
  /**
   * The transcript delta.
   */
  delta: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseAudioTranscriptDeltaTypeEnum = 'response.audio_transcript.delta';


/**
 * Returned when the model-generated transcription of audio output is done streaming. Also emitted when a Response is interrupted, incomplete, or cancelled. 
 */
export interface RealtimeServerEventResponseAudioTranscriptDone {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `response.audio_transcript.done`.
   */
  type: RealtimeServerEventResponseAudioTranscriptDoneTypeEnum;
  /**
   * The ID of the response.
   */
  response_id: string;
  /**
   * The ID of the item.
   */
  item_id: string;
  /**
   * The index of the output item in the response.
   */
  output_index: number;
  /**
   * The index of the content part in the item\'s content array.
   */
  content_index: number;
  /**
   * The final transcript of the audio.
   */
  transcript: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseAudioTranscriptDoneTypeEnum = 'response.audio_transcript.done';


/**
 * Returned when a new content part is added to an assistant message item during response generation. 
 */
export interface RealtimeServerEventResponseContentPartAdded {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `response.content_part.added`.
   */
  type: RealtimeServerEventResponseContentPartAddedTypeEnum;
  /**
   * The ID of the response.
   */
  response_id: string;
  /**
   * The ID of the item to which the content part was added.
   */
  item_id: string;
  /**
   * The index of the output item in the response.
   */
  output_index: number;
  /**
   * The index of the content part in the item\'s content array.
   */
  content_index: number;
  part: RealtimeServerEventResponseContentPartAddedPart;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseContentPartAddedTypeEnum = 'response.content_part.added';


/**
 * The content part that was added.
 */
export interface RealtimeServerEventResponseContentPartAddedPart {
  /**
   * The content type (\"text\", \"audio\").
   */
  type?: RealtimeServerEventResponseContentPartAddedPartTypeEnum;
  /**
   * The text content (if type is \"text\").
   */
  text?: string;
  /**
   * Base64-encoded audio data (if type is \"audio\").
   */
  audio?: string;
  /**
   * The transcript of the audio (if type is \"audio\").
   */
  transcript?: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseContentPartAddedPartTypeEnum = 'audio' | 'text';


/**
 * Returned when a content part is done streaming in an assistant message item. Also emitted when a Response is interrupted, incomplete, or cancelled. 
 */
export interface RealtimeServerEventResponseContentPartDone {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `response.content_part.done`.
   */
  type: RealtimeServerEventResponseContentPartDoneTypeEnum;
  /**
   * The ID of the response.
   */
  response_id: string;
  /**
   * The ID of the item.
   */
  item_id: string;
  /**
   * The index of the output item in the response.
   */
  output_index: number;
  /**
   * The index of the content part in the item\'s content array.
   */
  content_index: number;
  part: RealtimeServerEventResponseContentPartDonePart;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseContentPartDoneTypeEnum = 'response.content_part.done';


/**
 * The content part that is done.
 */
export interface RealtimeServerEventResponseContentPartDonePart {
  /**
   * The content type (\"text\", \"audio\").
   */
  type?: RealtimeServerEventResponseContentPartDonePartTypeEnum;
  /**
   * The text content (if type is \"text\").
   */
  text?: string;
  /**
   * Base64-encoded audio data (if type is \"audio\").
   */
  audio?: string;
  /**
   * The transcript of the audio (if type is \"audio\").
   */
  transcript?: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseContentPartDonePartTypeEnum = 'audio' | 'text';


/**
 * Returned when a new Response is created. The first event of response creation, where the response is in an initial state of `in_progress`. 
 */
export interface RealtimeServerEventResponseCreated {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `response.created`.
   */
  type: RealtimeServerEventResponseCreatedTypeEnum;
  response: RealtimeResponse;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseCreatedTypeEnum = 'response.created';


/**
 * Returned when a Response is done streaming. Always emitted, no matter the  final state. The Response object included in the `response.done` event will  include all output Items in the Response but will omit the raw audio data. 
 */
export interface RealtimeServerEventResponseDone {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `response.done`.
   */
  type: RealtimeServerEventResponseDoneTypeEnum;
  response: RealtimeResponse;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseDoneTypeEnum = 'response.done';


/**
 * Returned when the model-generated function call arguments are updated. 
 */
export interface RealtimeServerEventResponseFunctionCallArgumentsDelta {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `response.function_call_arguments.delta`. 
   */
  type: RealtimeServerEventResponseFunctionCallArgumentsDeltaTypeEnum;
  /**
   * The ID of the response.
   */
  response_id: string;
  /**
   * The ID of the function call item.
   */
  item_id: string;
  /**
   * The index of the output item in the response.
   */
  output_index: number;
  /**
   * The ID of the function call.
   */
  call_id: string;
  /**
   * The arguments delta as a JSON string.
   */
  delta: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseFunctionCallArgumentsDeltaTypeEnum = 'response.function_call_arguments.delta';


/**
 * Returned when the model-generated function call arguments are done streaming. Also emitted when a Response is interrupted, incomplete, or cancelled. 
 */
export interface RealtimeServerEventResponseFunctionCallArgumentsDone {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `response.function_call_arguments.done`. 
   */
  type: RealtimeServerEventResponseFunctionCallArgumentsDoneTypeEnum;
  /**
   * The ID of the response.
   */
  response_id: string;
  /**
   * The ID of the function call item.
   */
  item_id: string;
  /**
   * The index of the output item in the response.
   */
  output_index: number;
  /**
   * The ID of the function call.
   */
  call_id: string;
  /**
   * The final arguments as a JSON string.
   */
  arguments: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseFunctionCallArgumentsDoneTypeEnum = 'response.function_call_arguments.done';


/**
 * Returned when a new Item is created during Response generation.
 */
export interface RealtimeServerEventResponseOutputItemAdded {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `response.output_item.added`.
   */
  type: RealtimeServerEventResponseOutputItemAddedTypeEnum;
  /**
   * The ID of the Response to which the item belongs.
   */
  response_id: string;
  /**
   * The index of the output item in the Response.
   */
  output_index: number;
  item: RealtimeConversationItem;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseOutputItemAddedTypeEnum = 'response.output_item.added';


/**
 * Returned when an Item is done streaming. Also emitted when a Response is  interrupted, incomplete, or cancelled. 
 */
export interface RealtimeServerEventResponseOutputItemDone {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `response.output_item.done`.
   */
  type: RealtimeServerEventResponseOutputItemDoneTypeEnum;
  /**
   * The ID of the Response to which the item belongs.
   */
  response_id: string;
  /**
   * The index of the output item in the Response.
   */
  output_index: number;
  item: RealtimeConversationItem;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseOutputItemDoneTypeEnum = 'response.output_item.done';


/**
 * Returned when the text value of a \"text\" content part is updated.
 */
export interface RealtimeServerEventResponseTextDelta {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `response.text.delta`.
   */
  type: RealtimeServerEventResponseTextDeltaTypeEnum;
  /**
   * The ID of the response.
   */
  response_id: string;
  /**
   * The ID of the item.
   */
  item_id: string;
  /**
   * The index of the output item in the response.
   */
  output_index: number;
  /**
   * The index of the content part in the item\'s content array.
   */
  content_index: number;
  /**
   * The text delta.
   */
  delta: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseTextDeltaTypeEnum = 'response.text.delta';


/**
 * Returned when the text value of a \"text\" content part is done streaming. Also emitted when a Response is interrupted, incomplete, or cancelled. 
 */
export interface RealtimeServerEventResponseTextDone {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `response.text.done`.
   */
  type: RealtimeServerEventResponseTextDoneTypeEnum;
  /**
   * The ID of the response.
   */
  response_id: string;
  /**
   * The ID of the item.
   */
  item_id: string;
  /**
   * The index of the output item in the response.
   */
  output_index: number;
  /**
   * The index of the content part in the item\'s content array.
   */
  content_index: number;
  /**
   * The final text content.
   */
  text: string;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventResponseTextDoneTypeEnum = 'response.text.done';


/**
 * Returned when a Session is created. Emitted automatically when a new  connection is established as the first server event. This event will contain  the default Session configuration. 
 */
export interface RealtimeServerEventSessionCreated {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `session.created`.
   */
  type: RealtimeServerEventSessionCreatedTypeEnum;
  session: RealtimeSession;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventSessionCreatedTypeEnum = 'session.created';


/**
 * Returned when a session is updated with a `session.update` event, unless  there is an error. 
 */
export interface RealtimeServerEventSessionUpdated {
  /**
   * The unique ID of the server event.
   */
  event_id: string;
  /**
   * The event type, must be `session.updated`.
   */
  type: RealtimeServerEventSessionUpdatedTypeEnum;
  session: RealtimeSession;
}

/**
 * Enum for the type property.
 */
export type RealtimeServerEventSessionUpdatedTypeEnum = 'session.updated';


/**
 * Realtime session object configuration.
 */
export interface RealtimeSession {
  /**
   * Unique identifier for the session object. 
   */
  id?: string;
  /**
   * The set of modalities the model can respond with. To disable audio, set this to [\"text\"]. 
   */
  modalities?: Array<RealtimeSessionModalitiesEnum>;
  model?: RealtimeSessionModel;
  /**
   * The default system instructions (i.e. system message) prepended to model  calls. This field allows the client to guide the model on desired  responses. The model can be instructed on response content and format,  (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of good  responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion  into your voice\", \"laugh frequently\"). The instructions are not guaranteed  to be followed by the model, but they provide guidance to the model on the  desired behavior.  Note that the server sets default instructions which will be used if this  field is not set and are visible in the `session.created` event at the  start of the session. 
   */
  instructions?: string;
  /**
   * The voice the model uses to respond. Voice cannot be changed during the  session once the model has responded with audio at least once. Current  voice options are `alloy`, `ash`, `ballad`, `coral`, `echo` `sage`,  `shimmer` and `verse`. 
   */
  voice?: RealtimeSessionVoiceEnum;
  /**
   * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. 
   */
  input_audio_format?: RealtimeSessionInputAudioFormatEnum;
  /**
   * The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. 
   */
  output_audio_format?: RealtimeSessionOutputAudioFormatEnum;
  input_audio_transcription?: RealtimeSessionInputAudioTranscription;
  turn_detection?: RealtimeSessionTurnDetection;
  /**
   * Tools (functions) available to the model.
   */
  tools?: Array<RealtimeResponseCreateParamsToolsInner>;
  /**
   * How the model chooses tools. Options are `auto`, `none`, `required`, or  specify a function. 
   */
  tool_choice?: string;
  /**
   * Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8. 
   */
  temperature?: number;
  max_response_output_tokens?: RealtimeResponseCreateParamsMaxResponseOutputTokens;
}

/**
 * Enum for the modalities property.
 */
export type RealtimeSessionModalitiesEnum = 'text' | 'audio';

/**
 * Enum for the voice property.
 */
export type RealtimeSessionVoiceEnum = 'alloy' | 'ash' | 'ballad' | 'coral' | 'echo' | 'sage' | 'shimmer' | 'verse';

/**
 * Enum for the input_audio_format property.
 */
export type RealtimeSessionInputAudioFormatEnum = 'pcm16' | 'g711_ulaw' | 'g711_alaw';

/**
 * Enum for the output_audio_format property.
 */
export type RealtimeSessionOutputAudioFormatEnum = 'pcm16' | 'g711_ulaw' | 'g711_alaw';


/**
 * Realtime session object configuration.
 */
export interface RealtimeSessionCreateRequest {
  /**
   * The set of modalities the model can respond with. To disable audio, set this to [\"text\"]. 
   */
  modalities?: Array<RealtimeSessionCreateRequestModalitiesEnum>;
  /**
   * The Realtime model used for this session. 
   */
  model: RealtimeSessionCreateRequestModelEnum;
  /**
   * The default system instructions (i.e. system message) prepended to model  calls. This field allows the client to guide the model on desired  responses. The model can be instructed on response content and format,  (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of good  responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion  into your voice\", \"laugh frequently\"). The instructions are not guaranteed  to be followed by the model, but they provide guidance to the model on the  desired behavior.  Note that the server sets default instructions which will be used if this  field is not set and are visible in the `session.created` event at the  start of the session. 
   */
  instructions?: string;
  /**
   * The voice the model uses to respond. Voice cannot be changed during the  session once the model has responded with audio at least once. Current  voice options are `alloy`, `ash`, `ballad`, `coral`, `echo` `sage`,  `shimmer` and `verse`. 
   */
  voice?: RealtimeSessionCreateRequestVoiceEnum;
  /**
   * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. 
   */
  input_audio_format?: RealtimeSessionCreateRequestInputAudioFormatEnum;
  /**
   * The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. 
   */
  output_audio_format?: RealtimeSessionCreateRequestOutputAudioFormatEnum;
  input_audio_transcription?: RealtimeSessionInputAudioTranscription;
  turn_detection?: RealtimeSessionCreateRequestTurnDetection;
  /**
   * Tools (functions) available to the model.
   */
  tools?: Array<RealtimeResponseCreateParamsToolsInner>;
  /**
   * How the model chooses tools. Options are `auto`, `none`, `required`, or  specify a function. 
   */
  tool_choice?: string;
  /**
   * Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8. 
   */
  temperature?: number;
  max_response_output_tokens?: RealtimeResponseCreateParamsMaxResponseOutputTokens;
}

/**
 * Enum for the modalities property.
 */
export type RealtimeSessionCreateRequestModalitiesEnum = 'text' | 'audio';

/**
 * Enum for the model property.
 */
export type RealtimeSessionCreateRequestModelEnum = 'gpt-4o-realtime-preview' | 'gpt-4o-realtime-preview-2024-10-01' | 'gpt-4o-realtime-preview-2024-12-17' | 'gpt-4o-mini-realtime-preview' | 'gpt-4o-mini-realtime-preview-2024-12-17';

/**
 * Enum for the voice property.
 */
export type RealtimeSessionCreateRequestVoiceEnum = 'alloy' | 'ash' | 'ballad' | 'coral' | 'echo' | 'sage' | 'shimmer' | 'verse';

/**
 * Enum for the input_audio_format property.
 */
export type RealtimeSessionCreateRequestInputAudioFormatEnum = 'pcm16' | 'g711_ulaw' | 'g711_alaw';

/**
 * Enum for the output_audio_format property.
 */
export type RealtimeSessionCreateRequestOutputAudioFormatEnum = 'pcm16' | 'g711_ulaw' | 'g711_alaw';


/**
 * Configuration for turn detection. Can be set to `null` to turn off. Server  VAD means that the model will detect the start and end of speech based on  audio volume and respond at the end of user speech. 
 */
export interface RealtimeSessionCreateRequestTurnDetection {
  /**
   * Type of turn detection, only `server_vad` is currently supported. 
   */
  type?: string;
  /**
   * Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A  higher threshold will require louder audio to activate the model, and  thus might perform better in noisy environments. 
   */
  threshold?: number;
  /**
   * Amount of audio to include before the VAD detected speech (in  milliseconds). Defaults to 300ms. 
   */
  prefix_padding_ms?: number;
  /**
   * Duration of silence to detect speech stop (in milliseconds). Defaults  to 500ms. With shorter values the model will respond more quickly,  but may jump in on short pauses from the user. 
   */
  silence_duration_ms?: number;
  /**
   * Whether or not to automatically generate a response when VAD is enabled. `true` by default. 
   */
  create_response?: boolean;
}


/**
 * A new Realtime session configuration, with an ephermeral key. Default TTL for keys is one minute. 
 */
export interface RealtimeSessionCreateResponse {
  client_secret?: RealtimeSessionCreateResponseClientSecret;
  /**
   * The set of modalities the model can respond with. To disable audio, set this to [\"text\"]. 
   */
  modalities?: Array<RealtimeSessionCreateResponseModalitiesEnum>;
  /**
   * The default system instructions (i.e. system message) prepended to model  calls. This field allows the client to guide the model on desired  responses. The model can be instructed on response content and format,  (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of good  responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion  into your voice\", \"laugh frequently\"). The instructions are not guaranteed  to be followed by the model, but they provide guidance to the model on the  desired behavior.  Note that the server sets default instructions which will be used if this  field is not set and are visible in the `session.created` event at the  start of the session. 
   */
  instructions?: string;
  /**
   * The voice the model uses to respond. Voice cannot be changed during the  session once the model has responded with audio at least once. Current  voice options are `alloy`, `ash`, `ballad`, `coral`, `echo` `sage`,  `shimmer` and `verse`. 
   */
  voice?: RealtimeSessionCreateResponseVoiceEnum;
  /**
   * The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. 
   */
  input_audio_format?: string;
  /**
   * The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. 
   */
  output_audio_format?: string;
  input_audio_transcription?: RealtimeSessionInputAudioTranscription;
  turn_detection?: RealtimeSessionCreateResponseTurnDetection;
  /**
   * Tools (functions) available to the model.
   */
  tools?: Array<RealtimeResponseCreateParamsToolsInner>;
  /**
   * How the model chooses tools. Options are `auto`, `none`, `required`, or  specify a function. 
   */
  tool_choice?: string;
  /**
   * Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8. 
   */
  temperature?: number;
  max_response_output_tokens?: RealtimeResponseCreateParamsMaxResponseOutputTokens;
}

/**
 * Enum for the modalities property.
 */
export type RealtimeSessionCreateResponseModalitiesEnum = 'text' | 'audio';

/**
 * Enum for the voice property.
 */
export type RealtimeSessionCreateResponseVoiceEnum = 'alloy' | 'ash' | 'ballad' | 'coral' | 'echo' | 'sage' | 'shimmer' | 'verse';


/**
 * Ephemeral key returned by the API.
 */
export interface RealtimeSessionCreateResponseClientSecret {
  /**
   * Ephemeral key usable in client environments to authenticate connections to the Realtime API. Use this in client-side environments rather than a standard API token, which should only be used server-side. 
   */
  value?: string;
  /**
   * Timestamp for when the token expires. Currently, all tokens expire after one minute. 
   */
  expires_at?: number;
}


/**
 * Configuration for turn detection. Can be set to `null` to turn off. Server  VAD means that the model will detect the start and end of speech based on  audio volume and respond at the end of user speech. 
 */
export interface RealtimeSessionCreateResponseTurnDetection {
  /**
   * Type of turn detection, only `server_vad` is currently supported. 
   */
  type?: string;
  /**
   * Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A  higher threshold will require louder audio to activate the model, and  thus might perform better in noisy environments. 
   */
  threshold?: number;
  /**
   * Amount of audio to include before the VAD detected speech (in  milliseconds). Defaults to 300ms. 
   */
  prefix_padding_ms?: number;
  /**
   * Duration of silence to detect speech stop (in milliseconds). Defaults  to 500ms. With shorter values the model will respond more quickly,  but may jump in on short pauses from the user. 
   */
  silence_duration_ms?: number;
}


/**
 * Configuration for input audio transcription, defaults to off and can be  set to `null` to turn off once on. Input audio transcription is not native  to the model, since the model consumes audio directly. Transcription runs  asynchronously through Whisper and should be treated as rough guidance  rather than the representation understood by the model. 
 */
export interface RealtimeSessionInputAudioTranscription {
  /**
   * The model to use for transcription, `whisper-1` is the only currently  supported model. 
   */
  model?: string;
}


/**
 * The Realtime model used for this session. 
 */
export interface RealtimeSessionModel {
}


/**
 * Configuration for turn detection. Can be set to `null` to turn off. Server  VAD means that the model will detect the start and end of speech based on  audio volume and respond at the end of user speech. 
 */
export interface RealtimeSessionTurnDetection {
  /**
   * Type of turn detection, only `server_vad` is currently supported. 
   */
  type?: RealtimeSessionTurnDetectionTypeEnum;
  /**
   * Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A  higher threshold will require louder audio to activate the model, and  thus might perform better in noisy environments. 
   */
  threshold?: number;
  /**
   * Amount of audio to include before the VAD detected speech (in  milliseconds). Defaults to 300ms. 
   */
  prefix_padding_ms?: number;
  /**
   * Duration of silence to detect speech stop (in milliseconds). Defaults  to 500ms. With shorter values the model will respond more quickly,  but may jump in on short pauses from the user. 
   */
  silence_duration_ms?: number;
}

/**
 * Enum for the type property.
 */
export type RealtimeSessionTurnDetectionTypeEnum = 'server_vad';


export interface ResponseFormatJsonObject {
  /**
   * The type of response format being defined: `json_object`
   */
  type: ResponseFormatJsonObjectTypeEnum;
}

/**
 * Enum for the type property.
 */
export type ResponseFormatJsonObjectTypeEnum = 'json_object';


export interface ResponseFormatJsonSchema {
  /**
   * The type of response format being defined: `json_schema`
   */
  type: ResponseFormatJsonSchemaTypeEnum;
  json_schema: ResponseFormatJsonSchemaJsonSchema;
}

/**
 * Enum for the type property.
 */
export type ResponseFormatJsonSchemaTypeEnum = 'json_schema';


export interface ResponseFormatJsonSchemaJsonSchema {
  /**
   * A description of what the response format is for, used by the model to determine how to respond in the format.
   */
  description?: string;
  /**
   * The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
   */
  name: string;
  /**
   * The schema for the response format, described as a JSON Schema object.
   */
  schema?: { [key: string]: any; };
  /**
   * Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the `schema` field. Only a subset of JSON Schema is supported when `strict` is `true`. To learn more, read the [Structured Outputs guide](/docs/guides/structured-outputs).
   */
  strict?: boolean;
}


export interface ResponseFormatText {
  /**
   * The type of response format being defined: `text`
   */
  type: ResponseFormatTextTypeEnum;
}

/**
 * Enum for the type property.
 */
export type ResponseFormatTextTypeEnum = 'text';


/**
 * Usage statistics related to the run. This value will be `null` if the run is not in a terminal state (i.e. `in_progress`, `queued`, etc.).
 */
export interface RunCompletionUsage {
  /**
   * Number of completion tokens used over the course of the run.
   */
  completion_tokens: number;
  /**
   * Number of prompt tokens used over the course of the run.
   */
  prompt_tokens: number;
  /**
   * Total number of tokens used (prompt + completion).
   */
  total_tokens: number;
}


/**
 * Represents an execution run on a [thread](/docs/api-reference/threads).
 */
export interface RunObject {
  /**
   * The identifier, which can be referenced in API endpoints.
   */
  id: string;
  /**
   * The object type, which is always `thread.run`.
   */
  object: RunObjectObjectEnum;
  /**
   * The Unix timestamp (in seconds) for when the run was created.
   */
  created_at: number;
  /**
   * The ID of the [thread](/docs/api-reference/threads) that was executed on as a part of this run.
   */
  thread_id: string;
  /**
   * The ID of the [assistant](/docs/api-reference/assistants) used for execution of this run.
   */
  assistant_id: string;
  /**
   * The status of the run, which can be either `queued`, `in_progress`, `requires_action`, `cancelling`, `cancelled`, `failed`, `completed`, `incomplete`, or `expired`.
   */
  status: RunObjectStatusEnum;
  required_action: RunObjectRequiredAction;
  last_error: RunObjectLastError;
  /**
   * The Unix timestamp (in seconds) for when the run will expire.
   */
  expires_at: number;
  /**
   * The Unix timestamp (in seconds) for when the run was started.
   */
  started_at: number;
  /**
   * The Unix timestamp (in seconds) for when the run was cancelled.
   */
  cancelled_at: number;
  /**
   * The Unix timestamp (in seconds) for when the run failed.
   */
  failed_at: number;
  /**
   * The Unix timestamp (in seconds) for when the run was completed.
   */
  completed_at: number;
  incomplete_details: RunObjectIncompleteDetails;
  /**
   * The model that the [assistant](/docs/api-reference/assistants) used for this run.
   */
  model: string;
  /**
   * The instructions that the [assistant](/docs/api-reference/assistants) used for this run.
   */
  instructions: string;
  /**
   * The list of tools that the [assistant](/docs/api-reference/assistants) used for this run.
   */
  tools: Array<AssistantObjectToolsInner>;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata: object;
  usage: RunCompletionUsage;
  /**
   * The sampling temperature used for this run. If not set, defaults to 1.
   */
  temperature?: number;
  /**
   * The nucleus sampling value used for this run. If not set, defaults to 1.
   */
  top_p?: number;
  /**
   * The maximum number of prompt tokens specified to have been used over the course of the run. 
   */
  max_prompt_tokens: number;
  /**
   * The maximum number of completion tokens specified to have been used over the course of the run. 
   */
  max_completion_tokens: number;
  truncation_strategy: TruncationObject;
  tool_choice: AssistantsApiToolChoiceOption;
  /**
   * Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.
   */
  parallel_tool_calls: boolean;
  response_format: AssistantsApiResponseFormatOption;
}

/**
 * Enum for the object property.
 */
export type RunObjectObjectEnum = 'thread.run';

/**
 * Enum for the status property.
 */
export type RunObjectStatusEnum = 'queued' | 'in_progress' | 'requires_action' | 'cancelling' | 'cancelled' | 'failed' | 'completed' | 'incomplete' | 'expired';


/**
 * Details on why the run is incomplete. Will be `null` if the run is not incomplete.
 */
export interface RunObjectIncompleteDetails {
  /**
   * The reason why the run is incomplete. This will point to which specific token limit was reached over the course of the run.
   */
  reason?: RunObjectIncompleteDetailsReasonEnum;
}

/**
 * Enum for the reason property.
 */
export type RunObjectIncompleteDetailsReasonEnum = 'max_completion_tokens' | 'max_prompt_tokens';


/**
 * The last error associated with this run. Will be `null` if there are no errors.
 */
export interface RunObjectLastError {
  /**
   * One of `server_error`, `rate_limit_exceeded`, or `invalid_prompt`.
   */
  code: RunObjectLastErrorCodeEnum;
  /**
   * A human-readable description of the error.
   */
  message: string;
}

/**
 * Enum for the code property.
 */
export type RunObjectLastErrorCodeEnum = 'server_error' | 'rate_limit_exceeded' | 'invalid_prompt';


/**
 * Details on the action required to continue the run. Will be `null` if no action is required.
 */
export interface RunObjectRequiredAction {
  /**
   * For now, this is always `submit_tool_outputs`.
   */
  type: RunObjectRequiredActionTypeEnum;
  submit_tool_outputs: RunObjectRequiredActionSubmitToolOutputs;
}

/**
 * Enum for the type property.
 */
export type RunObjectRequiredActionTypeEnum = 'submit_tool_outputs';


/**
 * Details on the tool outputs needed for this run to continue.
 */
export interface RunObjectRequiredActionSubmitToolOutputs {
  /**
   * A list of the relevant tool calls.
   */
  tool_calls: Array<RunToolCallObject>;
}


/**
 * Usage statistics related to the run step. This value will be `null` while the run step\'s status is `in_progress`.
 */
export interface RunStepCompletionUsage {
  /**
   * Number of completion tokens used over the course of the run step.
   */
  completion_tokens: number;
  /**
   * Number of prompt tokens used over the course of the run step.
   */
  prompt_tokens: number;
  /**
   * Total number of tokens used (prompt + completion).
   */
  total_tokens: number;
}


/**
 * Represents a run step delta i.e. any changed fields on a run step during streaming. 
 */
export interface RunStepDeltaObject {
  /**
   * The identifier of the run step, which can be referenced in API endpoints.
   */
  id: string;
  /**
   * The object type, which is always `thread.run.step.delta`.
   */
  object: RunStepDeltaObjectObjectEnum;
  delta: RunStepDeltaObjectDelta;
}

/**
 * Enum for the object property.
 */
export type RunStepDeltaObjectObjectEnum = 'thread.run.step.delta';


/**
 * The delta containing the fields that have changed on the run step.
 */
export interface RunStepDeltaObjectDelta {
  step_details?: RunStepDeltaObjectDeltaStepDetails;
}

/**
 * @type RunStepDeltaObjectDeltaStepDetails
 * The details of the run step.
 * @export
 */
export type RunStepDeltaObjectDeltaStepDetails = RunStepDeltaStepDetailsMessageCreationObject | RunStepDeltaStepDetailsToolCallsObject;


/**
 * Details of the message creation by the run step.
 */
export interface RunStepDeltaStepDetailsMessageCreationObject {
  /**
   * Always `message_creation`.
   */
  type: RunStepDeltaStepDetailsMessageCreationObjectTypeEnum;
  message_creation?: RunStepDeltaStepDetailsMessageCreationObjectMessageCreation;
}

/**
 * Enum for the type property.
 */
export type RunStepDeltaStepDetailsMessageCreationObjectTypeEnum = 'message_creation';


export interface RunStepDeltaStepDetailsMessageCreationObjectMessageCreation {
  /**
   * The ID of the message that was created by this run step.
   */
  message_id?: string;
}


/**
 * Details of the Code Interpreter tool call the run step was involved in.
 */
export interface RunStepDeltaStepDetailsToolCallsCodeObject {
  /**
   * The index of the tool call in the tool calls array.
   */
  index: number;
  /**
   * The ID of the tool call.
   */
  id?: string;
  /**
   * The type of tool call. This is always going to be `code_interpreter` for this type of tool call.
   */
  type: RunStepDeltaStepDetailsToolCallsCodeObjectTypeEnum;
  code_interpreter?: RunStepDeltaStepDetailsToolCallsCodeObjectCodeInterpreter;
}

/**
 * Enum for the type property.
 */
export type RunStepDeltaStepDetailsToolCallsCodeObjectTypeEnum = 'code_interpreter';


/**
 * The Code Interpreter tool call definition.
 */
export interface RunStepDeltaStepDetailsToolCallsCodeObjectCodeInterpreter {
  /**
   * The input to the Code Interpreter tool call.
   */
  input?: string;
  /**
   * The outputs from the Code Interpreter tool call. Code Interpreter can output one or more items, including text (`logs`) or images (`image`). Each of these are represented by a different object type.
   */
  outputs?: Array<RunStepDeltaStepDetailsToolCallsCodeObjectCodeInterpreterOutputsInner>;
}

/**
 * @type RunStepDeltaStepDetailsToolCallsCodeObjectCodeInterpreterOutputsInner
 * @export
 */
export type RunStepDeltaStepDetailsToolCallsCodeObjectCodeInterpreterOutputsInner = RunStepDeltaStepDetailsToolCallsCodeOutputImageObject | RunStepDeltaStepDetailsToolCallsCodeOutputLogsObject;


export interface RunStepDeltaStepDetailsToolCallsCodeOutputImageObject {
  /**
   * The index of the output in the outputs array.
   */
  index: number;
  /**
   * Always `image`.
   */
  type: RunStepDeltaStepDetailsToolCallsCodeOutputImageObjectTypeEnum;
  image?: RunStepDeltaStepDetailsToolCallsCodeOutputImageObjectImage;
}

/**
 * Enum for the type property.
 */
export type RunStepDeltaStepDetailsToolCallsCodeOutputImageObjectTypeEnum = 'image';


export interface RunStepDeltaStepDetailsToolCallsCodeOutputImageObjectImage {
  /**
   * The [file](/docs/api-reference/files) ID of the image.
   */
  file_id?: string;
}


/**
 * Text output from the Code Interpreter tool call as part of a run step.
 */
export interface RunStepDeltaStepDetailsToolCallsCodeOutputLogsObject {
  /**
   * The index of the output in the outputs array.
   */
  index: number;
  /**
   * Always `logs`.
   */
  type: RunStepDeltaStepDetailsToolCallsCodeOutputLogsObjectTypeEnum;
  /**
   * The text output from the Code Interpreter tool call.
   */
  logs?: string;
}

/**
 * Enum for the type property.
 */
export type RunStepDeltaStepDetailsToolCallsCodeOutputLogsObjectTypeEnum = 'logs';


export interface RunStepDeltaStepDetailsToolCallsFileSearchObject {
  /**
   * The index of the tool call in the tool calls array.
   */
  index: number;
  /**
   * The ID of the tool call object.
   */
  id?: string;
  /**
   * The type of tool call. This is always going to be `file_search` for this type of tool call.
   */
  type: RunStepDeltaStepDetailsToolCallsFileSearchObjectTypeEnum;
  /**
   * For now, this is always going to be an empty object.
   */
  file_search: object;
}

/**
 * Enum for the type property.
 */
export type RunStepDeltaStepDetailsToolCallsFileSearchObjectTypeEnum = 'file_search';


export interface RunStepDeltaStepDetailsToolCallsFunctionObject {
  /**
   * The index of the tool call in the tool calls array.
   */
  index: number;
  /**
   * The ID of the tool call object.
   */
  id?: string;
  /**
   * The type of tool call. This is always going to be `function` for this type of tool call.
   */
  type: RunStepDeltaStepDetailsToolCallsFunctionObjectTypeEnum;
  _function?: RunStepDeltaStepDetailsToolCallsFunctionObjectFunction;
}

/**
 * Enum for the type property.
 */
export type RunStepDeltaStepDetailsToolCallsFunctionObjectTypeEnum = 'function';


/**
 * The definition of the function that was called.
 */
export interface RunStepDeltaStepDetailsToolCallsFunctionObjectFunction {
  /**
   * The name of the function.
   */
  name?: string;
  /**
   * The arguments passed to the function.
   */
  arguments?: string;
  /**
   * The output of the function. This will be `null` if the outputs have not been [submitted](/docs/api-reference/runs/submitToolOutputs) yet.
   */
  output?: string;
}


/**
 * Details of the tool call.
 */
export interface RunStepDeltaStepDetailsToolCallsObject {
  /**
   * Always `tool_calls`.
   */
  type: RunStepDeltaStepDetailsToolCallsObjectTypeEnum;
  /**
   * An array of tool calls the run step was involved in. These can be associated with one of three types of tools: `code_interpreter`, `file_search`, or `function`. 
   */
  tool_calls?: Array<RunStepDeltaStepDetailsToolCallsObjectToolCallsInner>;
}

/**
 * Enum for the type property.
 */
export type RunStepDeltaStepDetailsToolCallsObjectTypeEnum = 'tool_calls';

/**
 * @type RunStepDeltaStepDetailsToolCallsObjectToolCallsInner
 * @export
 */
export type RunStepDeltaStepDetailsToolCallsObjectToolCallsInner = RunStepDeltaStepDetailsToolCallsCodeObject | RunStepDeltaStepDetailsToolCallsFileSearchObject | RunStepDeltaStepDetailsToolCallsFunctionObject;


/**
 * Details of the message creation by the run step.
 */
export interface RunStepDetailsMessageCreationObject {
  /**
   * Always `message_creation`.
   */
  type: RunStepDetailsMessageCreationObjectTypeEnum;
  message_creation: RunStepDetailsMessageCreationObjectMessageCreation;
}

/**
 * Enum for the type property.
 */
export type RunStepDetailsMessageCreationObjectTypeEnum = 'message_creation';


export interface RunStepDetailsMessageCreationObjectMessageCreation {
  /**
   * The ID of the message that was created by this run step.
   */
  message_id: string;
}


/**
 * Details of the Code Interpreter tool call the run step was involved in.
 */
export interface RunStepDetailsToolCallsCodeObject {
  /**
   * The ID of the tool call.
   */
  id: string;
  /**
   * The type of tool call. This is always going to be `code_interpreter` for this type of tool call.
   */
  type: RunStepDetailsToolCallsCodeObjectTypeEnum;
  code_interpreter: RunStepDetailsToolCallsCodeObjectCodeInterpreter;
}

/**
 * Enum for the type property.
 */
export type RunStepDetailsToolCallsCodeObjectTypeEnum = 'code_interpreter';


/**
 * The Code Interpreter tool call definition.
 */
export interface RunStepDetailsToolCallsCodeObjectCodeInterpreter {
  /**
   * The input to the Code Interpreter tool call.
   */
  input: string;
  /**
   * The outputs from the Code Interpreter tool call. Code Interpreter can output one or more items, including text (`logs`) or images (`image`). Each of these are represented by a different object type.
   */
  outputs: Array<RunStepDetailsToolCallsCodeObjectCodeInterpreterOutputsInner>;
}

/**
 * @type RunStepDetailsToolCallsCodeObjectCodeInterpreterOutputsInner
 * @export
 */
export type RunStepDetailsToolCallsCodeObjectCodeInterpreterOutputsInner = RunStepDetailsToolCallsCodeOutputImageObject | RunStepDetailsToolCallsCodeOutputLogsObject;


export interface RunStepDetailsToolCallsCodeOutputImageObject {
  /**
   * Always `image`.
   */
  type: RunStepDetailsToolCallsCodeOutputImageObjectTypeEnum;
  image: RunStepDetailsToolCallsCodeOutputImageObjectImage;
}

/**
 * Enum for the type property.
 */
export type RunStepDetailsToolCallsCodeOutputImageObjectTypeEnum = 'image';


export interface RunStepDetailsToolCallsCodeOutputImageObjectImage {
  /**
   * The [file](/docs/api-reference/files) ID of the image.
   */
  file_id: string;
}


/**
 * Text output from the Code Interpreter tool call as part of a run step.
 */
export interface RunStepDetailsToolCallsCodeOutputLogsObject {
  /**
   * Always `logs`.
   */
  type: RunStepDetailsToolCallsCodeOutputLogsObjectTypeEnum;
  /**
   * The text output from the Code Interpreter tool call.
   */
  logs: string;
}

/**
 * Enum for the type property.
 */
export type RunStepDetailsToolCallsCodeOutputLogsObjectTypeEnum = 'logs';


export interface RunStepDetailsToolCallsFileSearchObject {
  /**
   * The ID of the tool call object.
   */
  id: string;
  /**
   * The type of tool call. This is always going to be `file_search` for this type of tool call.
   */
  type: RunStepDetailsToolCallsFileSearchObjectTypeEnum;
  file_search: RunStepDetailsToolCallsFileSearchObjectFileSearch;
}

/**
 * Enum for the type property.
 */
export type RunStepDetailsToolCallsFileSearchObjectTypeEnum = 'file_search';


/**
 * For now, this is always going to be an empty object.
 */
export interface RunStepDetailsToolCallsFileSearchObjectFileSearch {
  ranking_options?: RunStepDetailsToolCallsFileSearchRankingOptionsObject;
  /**
   * The results of the file search.
   */
  results?: Array<RunStepDetailsToolCallsFileSearchResultObject>;
}


/**
 * The ranking options for the file search.
 */
export interface RunStepDetailsToolCallsFileSearchRankingOptionsObject {
  /**
   * The ranker used for the file search.
   */
  ranker: RunStepDetailsToolCallsFileSearchRankingOptionsObjectRankerEnum;
  /**
   * The score threshold for the file search. All values must be a floating point number between 0 and 1.
   */
  score_threshold: number;
}

/**
 * Enum for the ranker property.
 */
export type RunStepDetailsToolCallsFileSearchRankingOptionsObjectRankerEnum = 'default_2024_08_21';


/**
 * A result instance of the file search.
 */
export interface RunStepDetailsToolCallsFileSearchResultObject {
  /**
   * The ID of the file that result was found in.
   */
  file_id: string;
  /**
   * The name of the file that result was found in.
   */
  file_name: string;
  /**
   * The score of the result. All values must be a floating point number between 0 and 1.
   */
  score: number;
  /**
   * The content of the result that was found. The content is only included if requested via the include query parameter.
   */
  content?: Array<RunStepDetailsToolCallsFileSearchResultObjectContentInner>;
}


export interface RunStepDetailsToolCallsFileSearchResultObjectContentInner {
  /**
   * The type of the content.
   */
  type?: RunStepDetailsToolCallsFileSearchResultObjectContentInnerTypeEnum;
  /**
   * The text content of the file.
   */
  text?: string;
}

/**
 * Enum for the type property.
 */
export type RunStepDetailsToolCallsFileSearchResultObjectContentInnerTypeEnum = 'text';


export interface RunStepDetailsToolCallsFunctionObject {
  /**
   * The ID of the tool call object.
   */
  id: string;
  /**
   * The type of tool call. This is always going to be `function` for this type of tool call.
   */
  type: RunStepDetailsToolCallsFunctionObjectTypeEnum;
  _function: RunStepDetailsToolCallsFunctionObjectFunction;
}

/**
 * Enum for the type property.
 */
export type RunStepDetailsToolCallsFunctionObjectTypeEnum = 'function';


/**
 * The definition of the function that was called.
 */
export interface RunStepDetailsToolCallsFunctionObjectFunction {
  /**
   * The name of the function.
   */
  name: string;
  /**
   * The arguments passed to the function.
   */
  arguments: string;
  /**
   * The output of the function. This will be `null` if the outputs have not been [submitted](/docs/api-reference/runs/submitToolOutputs) yet.
   */
  output: string;
}


/**
 * Details of the tool call.
 */
export interface RunStepDetailsToolCallsObject {
  /**
   * Always `tool_calls`.
   */
  type: RunStepDetailsToolCallsObjectTypeEnum;
  /**
   * An array of tool calls the run step was involved in. These can be associated with one of three types of tools: `code_interpreter`, `file_search`, or `function`. 
   */
  tool_calls: Array<RunStepDetailsToolCallsObjectToolCallsInner>;
}

/**
 * Enum for the type property.
 */
export type RunStepDetailsToolCallsObjectTypeEnum = 'tool_calls';

/**
 * @type RunStepDetailsToolCallsObjectToolCallsInner
 * @export
 */
export type RunStepDetailsToolCallsObjectToolCallsInner = RunStepDetailsToolCallsCodeObject | RunStepDetailsToolCallsFileSearchObject | RunStepDetailsToolCallsFunctionObject;


/**
 * Represents a step in execution of a run. 
 */
export interface RunStepObject {
  /**
   * The identifier of the run step, which can be referenced in API endpoints.
   */
  id: string;
  /**
   * The object type, which is always `thread.run.step`.
   */
  object: RunStepObjectObjectEnum;
  /**
   * The Unix timestamp (in seconds) for when the run step was created.
   */
  created_at: number;
  /**
   * The ID of the [assistant](/docs/api-reference/assistants) associated with the run step.
   */
  assistant_id: string;
  /**
   * The ID of the [thread](/docs/api-reference/threads) that was run.
   */
  thread_id: string;
  /**
   * The ID of the [run](/docs/api-reference/runs) that this run step is a part of.
   */
  run_id: string;
  /**
   * The type of run step, which can be either `message_creation` or `tool_calls`.
   */
  type: RunStepObjectTypeEnum;
  /**
   * The status of the run step, which can be either `in_progress`, `cancelled`, `failed`, `completed`, or `expired`.
   */
  status: RunStepObjectStatusEnum;
  step_details: RunStepObjectStepDetails;
  last_error: RunStepObjectLastError;
  /**
   * The Unix timestamp (in seconds) for when the run step expired. A step is considered expired if the parent run is expired.
   */
  expired_at: number;
  /**
   * The Unix timestamp (in seconds) for when the run step was cancelled.
   */
  cancelled_at: number;
  /**
   * The Unix timestamp (in seconds) for when the run step failed.
   */
  failed_at: number;
  /**
   * The Unix timestamp (in seconds) for when the run step completed.
   */
  completed_at: number;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata: object;
  usage: RunStepCompletionUsage;
}

/**
 * Enum for the object property.
 */
export type RunStepObjectObjectEnum = 'thread.run.step';

/**
 * Enum for the type property.
 */
export type RunStepObjectTypeEnum = 'message_creation' | 'tool_calls';

/**
 * Enum for the status property.
 */
export type RunStepObjectStatusEnum = 'in_progress' | 'cancelled' | 'failed' | 'completed' | 'expired';


/**
 * The last error associated with this run step. Will be `null` if there are no errors.
 */
export interface RunStepObjectLastError {
  /**
   * One of `server_error` or `rate_limit_exceeded`.
   */
  code: RunStepObjectLastErrorCodeEnum;
  /**
   * A human-readable description of the error.
   */
  message: string;
}

/**
 * Enum for the code property.
 */
export type RunStepObjectLastErrorCodeEnum = 'server_error' | 'rate_limit_exceeded';

/**
 * @type RunStepObjectStepDetails
 * The details of the run step.
 * @export
 */
export type RunStepObjectStepDetails = RunStepDetailsMessageCreationObject | RunStepDetailsToolCallsObject;

/**
 * @type RunStepStreamEvent
 * @export
 */
export type RunStepStreamEvent = RunStepStreamEventOneOf | RunStepStreamEventOneOf1 | RunStepStreamEventOneOf2 | RunStepStreamEventOneOf3 | RunStepStreamEventOneOf4 | RunStepStreamEventOneOf5 | RunStepStreamEventOneOf6;


/**
 * Occurs when a [run step](/docs/api-reference/run-steps/step-object) is created.
 */
export interface RunStepStreamEventOneOf {
  event: RunStepStreamEventOneOfEventEnum;
  data: RunStepObject;
}

/**
 * Enum for the event property.
 */
export type RunStepStreamEventOneOfEventEnum = 'thread.run.step.created';


/**
 * Occurs when a [run step](/docs/api-reference/run-steps/step-object) moves to an `in_progress` state.
 */
export interface RunStepStreamEventOneOf1 {
  event: RunStepStreamEventOneOf1EventEnum;
  data: RunStepObject;
}

/**
 * Enum for the event property.
 */
export type RunStepStreamEventOneOf1EventEnum = 'thread.run.step.in_progress';


/**
 * Occurs when parts of a [run step](/docs/api-reference/run-steps/step-object) are being streamed.
 */
export interface RunStepStreamEventOneOf2 {
  event: RunStepStreamEventOneOf2EventEnum;
  data: RunStepDeltaObject;
}

/**
 * Enum for the event property.
 */
export type RunStepStreamEventOneOf2EventEnum = 'thread.run.step.delta';


/**
 * Occurs when a [run step](/docs/api-reference/run-steps/step-object) is completed.
 */
export interface RunStepStreamEventOneOf3 {
  event: RunStepStreamEventOneOf3EventEnum;
  data: RunStepObject;
}

/**
 * Enum for the event property.
 */
export type RunStepStreamEventOneOf3EventEnum = 'thread.run.step.completed';


/**
 * Occurs when a [run step](/docs/api-reference/run-steps/step-object) fails.
 */
export interface RunStepStreamEventOneOf4 {
  event: RunStepStreamEventOneOf4EventEnum;
  data: RunStepObject;
}

/**
 * Enum for the event property.
 */
export type RunStepStreamEventOneOf4EventEnum = 'thread.run.step.failed';


/**
 * Occurs when a [run step](/docs/api-reference/run-steps/step-object) is cancelled.
 */
export interface RunStepStreamEventOneOf5 {
  event: RunStepStreamEventOneOf5EventEnum;
  data: RunStepObject;
}

/**
 * Enum for the event property.
 */
export type RunStepStreamEventOneOf5EventEnum = 'thread.run.step.cancelled';


/**
 * Occurs when a [run step](/docs/api-reference/run-steps/step-object) expires.
 */
export interface RunStepStreamEventOneOf6 {
  event: RunStepStreamEventOneOf6EventEnum;
  data: RunStepObject;
}

/**
 * Enum for the event property.
 */
export type RunStepStreamEventOneOf6EventEnum = 'thread.run.step.expired';

/**
 * @type RunStreamEvent
 * @export
 */
export type RunStreamEvent = RunStreamEventOneOf | RunStreamEventOneOf1 | RunStreamEventOneOf2 | RunStreamEventOneOf3 | RunStreamEventOneOf4 | RunStreamEventOneOf5 | RunStreamEventOneOf6 | RunStreamEventOneOf7 | RunStreamEventOneOf8 | RunStreamEventOneOf9;


/**
 * Occurs when a new [run](/docs/api-reference/runs/object) is created.
 */
export interface RunStreamEventOneOf {
  event: RunStreamEventOneOfEventEnum;
  data: RunObject;
}

/**
 * Enum for the event property.
 */
export type RunStreamEventOneOfEventEnum = 'thread.run.created';


/**
 * Occurs when a [run](/docs/api-reference/runs/object) moves to a `queued` status.
 */
export interface RunStreamEventOneOf1 {
  event: RunStreamEventOneOf1EventEnum;
  data: RunObject;
}

/**
 * Enum for the event property.
 */
export type RunStreamEventOneOf1EventEnum = 'thread.run.queued';


/**
 * Occurs when a [run](/docs/api-reference/runs/object) moves to an `in_progress` status.
 */
export interface RunStreamEventOneOf2 {
  event: RunStreamEventOneOf2EventEnum;
  data: RunObject;
}

/**
 * Enum for the event property.
 */
export type RunStreamEventOneOf2EventEnum = 'thread.run.in_progress';


/**
 * Occurs when a [run](/docs/api-reference/runs/object) moves to a `requires_action` status.
 */
export interface RunStreamEventOneOf3 {
  event: RunStreamEventOneOf3EventEnum;
  data: RunObject;
}

/**
 * Enum for the event property.
 */
export type RunStreamEventOneOf3EventEnum = 'thread.run.requires_action';


/**
 * Occurs when a [run](/docs/api-reference/runs/object) is completed.
 */
export interface RunStreamEventOneOf4 {
  event: RunStreamEventOneOf4EventEnum;
  data: RunObject;
}

/**
 * Enum for the event property.
 */
export type RunStreamEventOneOf4EventEnum = 'thread.run.completed';


/**
 * Occurs when a [run](/docs/api-reference/runs/object) ends with status `incomplete`.
 */
export interface RunStreamEventOneOf5 {
  event: RunStreamEventOneOf5EventEnum;
  data: RunObject;
}

/**
 * Enum for the event property.
 */
export type RunStreamEventOneOf5EventEnum = 'thread.run.incomplete';


/**
 * Occurs when a [run](/docs/api-reference/runs/object) fails.
 */
export interface RunStreamEventOneOf6 {
  event: RunStreamEventOneOf6EventEnum;
  data: RunObject;
}

/**
 * Enum for the event property.
 */
export type RunStreamEventOneOf6EventEnum = 'thread.run.failed';


/**
 * Occurs when a [run](/docs/api-reference/runs/object) moves to a `cancelling` status.
 */
export interface RunStreamEventOneOf7 {
  event: RunStreamEventOneOf7EventEnum;
  data: RunObject;
}

/**
 * Enum for the event property.
 */
export type RunStreamEventOneOf7EventEnum = 'thread.run.cancelling';


/**
 * Occurs when a [run](/docs/api-reference/runs/object) is cancelled.
 */
export interface RunStreamEventOneOf8 {
  event: RunStreamEventOneOf8EventEnum;
  data: RunObject;
}

/**
 * Enum for the event property.
 */
export type RunStreamEventOneOf8EventEnum = 'thread.run.cancelled';


/**
 * Occurs when a [run](/docs/api-reference/runs/object) expires.
 */
export interface RunStreamEventOneOf9 {
  event: RunStreamEventOneOf9EventEnum;
  data: RunObject;
}

/**
 * Enum for the event property.
 */
export type RunStreamEventOneOf9EventEnum = 'thread.run.expired';


/**
 * Tool call objects
 */
export interface RunToolCallObject {
  /**
   * The ID of the tool call. This ID must be referenced when you submit the tool outputs in using the [Submit tool outputs to run](/docs/api-reference/runs/submitToolOutputs) endpoint.
   */
  id: string;
  /**
   * The type of tool call the output is required for. For now, this is always `function`.
   */
  type: RunToolCallObjectTypeEnum;
  _function: RunToolCallObjectFunction;
}

/**
 * Enum for the type property.
 */
export type RunToolCallObjectTypeEnum = 'function';


/**
 * The function definition.
 */
export interface RunToolCallObjectFunction {
  /**
   * The name of the function.
   */
  name: string;
  /**
   * The arguments that the model expects you to pass to the function.
   */
  arguments: string;
}


export interface StaticChunkingStrategy {
  /**
   * Always `static`.
   */
  type: StaticChunkingStrategyTypeEnum;
  _static: StaticChunkingStrategyStatic;
}

/**
 * Enum for the type property.
 */
export type StaticChunkingStrategyTypeEnum = 'static';


export interface StaticChunkingStrategyRequestParam {
  /**
   * Always `static`.
   */
  type: StaticChunkingStrategyRequestParamTypeEnum;
  _static: StaticChunkingStrategy;
}

/**
 * Enum for the type property.
 */
export type StaticChunkingStrategyRequestParamTypeEnum = 'static';


export interface StaticChunkingStrategyResponseParam {
  /**
   * Always `static`.
   */
  type: StaticChunkingStrategyResponseParamTypeEnum;
  _static: StaticChunkingStrategy;
}

/**
 * Enum for the type property.
 */
export type StaticChunkingStrategyResponseParamTypeEnum = 'static';


export interface StaticChunkingStrategyStatic {
  /**
   * The maximum number of tokens in each chunk. The default value is `800`. The minimum value is `100` and the maximum value is `4096`.
   */
  max_chunk_size_tokens: number;
  /**
   * The number of tokens that overlap between chunks. The default value is `400`.  Note that the overlap must not exceed half of `max_chunk_size_tokens`. 
   */
  chunk_overlap_tokens: number;
}


export interface SubmitToolOutputsRunRequest {
  /**
   * A list of tools for which the outputs are being submitted.
   */
  tool_outputs: Array<SubmitToolOutputsRunRequestToolOutputsInner>;
  /**
   * If `true`, returns a stream of events that happen during the Run as server-sent events, terminating when the Run enters a terminal state with a `data: [DONE]` message. 
   */
  stream?: boolean;
}


export interface SubmitToolOutputsRunRequestToolOutputsInner {
  /**
   * The ID of the tool call in the `required_action` object within the run object the output is being submitted for.
   */
  tool_call_id?: string;
  /**
   * The output of the tool call to be submitted to continue the run.
   */
  output?: string;
}


/**
 * Represents a thread that contains [messages](/docs/api-reference/messages).
 */
export interface ThreadObject {
  /**
   * The identifier, which can be referenced in API endpoints.
   */
  id: string;
  /**
   * The object type, which is always `thread`.
   */
  object: ThreadObjectObjectEnum;
  /**
   * The Unix timestamp (in seconds) for when the thread was created.
   */
  created_at: number;
  tool_resources: ModifyThreadRequestToolResources;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata: object;
}

/**
 * Enum for the object property.
 */
export type ThreadObjectObjectEnum = 'thread';


/**
 * Occurs when a new [thread](/docs/api-reference/threads/object) is created.
 */
export interface ThreadStreamEvent {
  /**
   * Whether to enable input audio transcription.
   */
  enabled?: boolean;
  event: ThreadStreamEventEventEnum;
  data: ThreadObject;
}

/**
 * Enum for the event property.
 */
export type ThreadStreamEventEventEnum = 'thread.created';


export interface TranscriptionSegment {
  /**
   * Unique identifier of the segment.
   */
  id: number;
  /**
   * Seek offset of the segment.
   */
  seek: number;
  /**
   * Start time of the segment in seconds.
   */
  start: number;
  /**
   * End time of the segment in seconds.
   */
  end: number;
  /**
   * Text content of the segment.
   */
  text: string;
  /**
   * Array of token IDs for the text content.
   */
  tokens: Array<number>;
  /**
   * Temperature parameter used for generating the segment.
   */
  temperature: number;
  /**
   * Average logprob of the segment. If the value is lower than -1, consider the logprobs failed.
   */
  avg_logprob: number;
  /**
   * Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed.
   */
  compression_ratio: number;
  /**
   * Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent.
   */
  no_speech_prob: number;
}


export interface TranscriptionWord {
  /**
   * The text content of the word.
   */
  word: string;
  /**
   * Start time of the word in seconds.
   */
  start: number;
  /**
   * End time of the word in seconds.
   */
  end: number;
}


/**
 * Controls for how a thread will be truncated prior to the run. Use this to control the intial context window of the run.
 */
export interface TruncationObject {
  /**
   * The truncation strategy to use for the thread. The default is `auto`. If set to `last_messages`, the thread will be truncated to the n most recent messages in the thread. When set to `auto`, messages in the middle of the thread will be dropped to fit the context length of the model, `max_prompt_tokens`.
   */
  type: TruncationObjectTypeEnum;
  /**
   * The number of most recent messages from the thread when constructing the context for the run.
   */
  last_messages?: number;
}

/**
 * Enum for the type property.
 */
export type TruncationObjectTypeEnum = 'auto' | 'last_messages';


export interface UpdateVectorStoreRequest {
  /**
   * The name of the vector store.
   */
  name?: string;
  expires_after?: VectorStoreExpirationAfter;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata?: object;
}


/**
 * The Upload object can accept byte chunks in the form of Parts. 
 */
export interface Upload {
  /**
   * The Upload unique identifier, which can be referenced in API endpoints.
   */
  id: string;
  /**
   * The Unix timestamp (in seconds) for when the Upload was created.
   */
  created_at: number;
  /**
   * The name of the file to be uploaded.
   */
  filename: string;
  /**
   * The intended number of bytes to be uploaded.
   */
  bytes: number;
  /**
   * The intended purpose of the file. [Please refer here](/docs/api-reference/files/object#files/object-purpose) for acceptable values.
   */
  purpose: string;
  /**
   * The status of the Upload.
   */
  status: UploadStatusEnum;
  /**
   * The Unix timestamp (in seconds) for when the Upload was created.
   */
  expires_at: number;
  /**
   * The object type, which is always \"upload\".
   */
  object?: UploadObjectEnum;
  file?: OpenAIFile;
}

/**
 * Enum for the status property.
 */
export type UploadStatusEnum = 'pending' | 'completed' | 'cancelled' | 'expired';

/**
 * Enum for the object property.
 */
export type UploadObjectEnum = 'upload';


/**
 * The upload Part represents a chunk of bytes we can add to an Upload object. 
 */
export interface UploadPart {
  /**
   * The upload Part unique identifier, which can be referenced in API endpoints.
   */
  id: string;
  /**
   * The Unix timestamp (in seconds) for when the Part was created.
   */
  created_at: number;
  /**
   * The ID of the Upload object that this Part was added to.
   */
  upload_id: string;
  /**
   * The object type, which is always `upload.part`.
   */
  object: UploadPartObjectEnum;
}

/**
 * Enum for the object property.
 */
export type UploadPartObjectEnum = 'upload.part';


/**
 * The aggregated audio speeches usage details of the specific time bucket.
 */
export interface UsageAudioSpeechesResult {
  object: UsageAudioSpeechesResultObjectEnum;
  /**
   * The number of characters processed.
   */
  characters: number;
  /**
   * The count of requests made to the model.
   */
  num_model_requests: number;
  /**
   * When `group_by=project_id`, this field provides the project ID of the grouped usage result.
   */
  project_id?: string;
  /**
   * When `group_by=user_id`, this field provides the user ID of the grouped usage result.
   */
  user_id?: string;
  /**
   * When `group_by=api_key_id`, this field provides the API key ID of the grouped usage result.
   */
  api_key_id?: string;
  /**
   * When `group_by=model`, this field provides the model name of the grouped usage result.
   */
  model?: string;
}

/**
 * Enum for the object property.
 */
export type UsageAudioSpeechesResultObjectEnum = 'organization.usage.audio_speeches.result';


/**
 * The aggregated audio transcriptions usage details of the specific time bucket.
 */
export interface UsageAudioTranscriptionsResult {
  object: UsageAudioTranscriptionsResultObjectEnum;
  /**
   * The number of seconds processed.
   */
  seconds: number;
  /**
   * The count of requests made to the model.
   */
  num_model_requests: number;
  /**
   * When `group_by=project_id`, this field provides the project ID of the grouped usage result.
   */
  project_id?: string;
  /**
   * When `group_by=user_id`, this field provides the user ID of the grouped usage result.
   */
  user_id?: string;
  /**
   * When `group_by=api_key_id`, this field provides the API key ID of the grouped usage result.
   */
  api_key_id?: string;
  /**
   * When `group_by=model`, this field provides the model name of the grouped usage result.
   */
  model?: string;
}

/**
 * Enum for the object property.
 */
export type UsageAudioTranscriptionsResultObjectEnum = 'organization.usage.audio_transcriptions.result';


/**
 * The aggregated code interpreter sessions usage details of the specific time bucket.
 */
export interface UsageCodeInterpreterSessionsResult {
  object: UsageCodeInterpreterSessionsResultObjectEnum;
  /**
   * The number of code interpreter sessions.
   */
  sessions: number;
  /**
   * When `group_by=project_id`, this field provides the project ID of the grouped usage result.
   */
  project_id?: string;
}

/**
 * Enum for the object property.
 */
export type UsageCodeInterpreterSessionsResultObjectEnum = 'organization.usage.code_interpreter_sessions.result';


/**
 * The aggregated completions usage details of the specific time bucket.
 */
export interface UsageCompletionsResult {
  object: UsageCompletionsResultObjectEnum;
  /**
   * The aggregated number of text input tokens used, including cached tokens. For customers subscribe to scale tier, this includes scale tier tokens.
   */
  input_tokens: number;
  /**
   * The aggregated number of text input tokens that has been cached from previous requests. For customers subscribe to scale tier, this includes scale tier tokens.
   */
  input_cached_tokens?: number;
  /**
   * The aggregated number of text output tokens used. For customers subscribe to scale tier, this includes scale tier tokens.
   */
  output_tokens: number;
  /**
   * The aggregated number of audio input tokens used, including cached tokens.
   */
  input_audio_tokens?: number;
  /**
   * The aggregated number of audio output tokens used.
   */
  output_audio_tokens?: number;
  /**
   * The count of requests made to the model.
   */
  num_model_requests: number;
  /**
   * When `group_by=project_id`, this field provides the project ID of the grouped usage result.
   */
  project_id?: string;
  /**
   * When `group_by=user_id`, this field provides the user ID of the grouped usage result.
   */
  user_id?: string;
  /**
   * When `group_by=api_key_id`, this field provides the API key ID of the grouped usage result.
   */
  api_key_id?: string;
  /**
   * When `group_by=model`, this field provides the model name of the grouped usage result.
   */
  model?: string;
  /**
   * When `group_by=batch`, this field tells whether the grouped usage result is batch or not.
   */
  batch?: boolean;
}

/**
 * Enum for the object property.
 */
export type UsageCompletionsResultObjectEnum = 'organization.usage.completions.result';


/**
 * The aggregated embeddings usage details of the specific time bucket.
 */
export interface UsageEmbeddingsResult {
  object: UsageEmbeddingsResultObjectEnum;
  /**
   * The aggregated number of input tokens used.
   */
  input_tokens: number;
  /**
   * The count of requests made to the model.
   */
  num_model_requests: number;
  /**
   * When `group_by=project_id`, this field provides the project ID of the grouped usage result.
   */
  project_id?: string;
  /**
   * When `group_by=user_id`, this field provides the user ID of the grouped usage result.
   */
  user_id?: string;
  /**
   * When `group_by=api_key_id`, this field provides the API key ID of the grouped usage result.
   */
  api_key_id?: string;
  /**
   * When `group_by=model`, this field provides the model name of the grouped usage result.
   */
  model?: string;
}

/**
 * Enum for the object property.
 */
export type UsageEmbeddingsResultObjectEnum = 'organization.usage.embeddings.result';


/**
 * The aggregated images usage details of the specific time bucket.
 */
export interface UsageImagesResult {
  object: UsageImagesResultObjectEnum;
  /**
   * The number of images processed.
   */
  images: number;
  /**
   * The count of requests made to the model.
   */
  num_model_requests: number;
  /**
   * When `group_by=source`, this field provides the source of the grouped usage result, possible values are `image.generation`, `image.edit`, `image.variation`.
   */
  source?: string;
  /**
   * When `group_by=size`, this field provides the image size of the grouped usage result.
   */
  size?: string;
  /**
   * When `group_by=project_id`, this field provides the project ID of the grouped usage result.
   */
  project_id?: string;
  /**
   * When `group_by=user_id`, this field provides the user ID of the grouped usage result.
   */
  user_id?: string;
  /**
   * When `group_by=api_key_id`, this field provides the API key ID of the grouped usage result.
   */
  api_key_id?: string;
  /**
   * When `group_by=model`, this field provides the model name of the grouped usage result.
   */
  model?: string;
}

/**
 * Enum for the object property.
 */
export type UsageImagesResultObjectEnum = 'organization.usage.images.result';


/**
 * The aggregated moderations usage details of the specific time bucket.
 */
export interface UsageModerationsResult {
  object: UsageModerationsResultObjectEnum;
  /**
   * The aggregated number of input tokens used.
   */
  input_tokens: number;
  /**
   * The count of requests made to the model.
   */
  num_model_requests: number;
  /**
   * When `group_by=project_id`, this field provides the project ID of the grouped usage result.
   */
  project_id?: string;
  /**
   * When `group_by=user_id`, this field provides the user ID of the grouped usage result.
   */
  user_id?: string;
  /**
   * When `group_by=api_key_id`, this field provides the API key ID of the grouped usage result.
   */
  api_key_id?: string;
  /**
   * When `group_by=model`, this field provides the model name of the grouped usage result.
   */
  model?: string;
}

/**
 * Enum for the object property.
 */
export type UsageModerationsResultObjectEnum = 'organization.usage.moderations.result';


export interface UsageResponse {
  object: UsageResponseObjectEnum;
  data: Array<UsageTimeBucket>;
  has_more: boolean;
  next_page: string;
}

/**
 * Enum for the object property.
 */
export type UsageResponseObjectEnum = 'page';


export interface UsageTimeBucket {
  object: UsageTimeBucketObjectEnum;
  start_time: number;
  end_time: number;
  result: Array<UsageTimeBucketResultInner>;
}

/**
 * Enum for the object property.
 */
export type UsageTimeBucketObjectEnum = 'bucket';

/**
 * @type UsageTimeBucketResultInner
 * @export
 */
export type UsageTimeBucketResultInner = CostsResult | UsageAudioSpeechesResult | UsageAudioTranscriptionsResult | UsageCodeInterpreterSessionsResult | UsageCompletionsResult | UsageEmbeddingsResult | UsageImagesResult | UsageModerationsResult | UsageVectorStoresResult;


/**
 * The aggregated vector stores usage details of the specific time bucket.
 */
export interface UsageVectorStoresResult {
  object: UsageVectorStoresResultObjectEnum;
  /**
   * The vector stores usage in bytes.
   */
  usage_bytes: number;
  /**
   * When `group_by=project_id`, this field provides the project ID of the grouped usage result.
   */
  project_id?: string;
}

/**
 * Enum for the object property.
 */
export type UsageVectorStoresResultObjectEnum = 'organization.usage.vector_stores.result';


/**
 * Represents an individual `user` within an organization.
 */
export interface User {
  /**
   * The object type, which is always `organization.user`
   */
  object: UserObjectEnum;
  /**
   * The identifier, which can be referenced in API endpoints
   */
  id: string;
  /**
   * The name of the user
   */
  name: string;
  /**
   * The email address of the user
   */
  email: string;
  /**
   * `owner` or `reader`
   */
  role: UserRoleEnum;
  /**
   * The Unix timestamp (in seconds) of when the user was added.
   */
  added_at: number;
}

/**
 * Enum for the object property.
 */
export type UserObjectEnum = 'organization.user';

/**
 * Enum for the role property.
 */
export type UserRoleEnum = 'owner' | 'reader';


export interface UserDeleteResponse {
  object: UserDeleteResponseObjectEnum;
  id: string;
  deleted: boolean;
}

/**
 * Enum for the object property.
 */
export type UserDeleteResponseObjectEnum = 'organization.user.deleted';


export interface UserListResponse {
  object: UserListResponseObjectEnum;
  data: Array<User>;
  first_id: string;
  last_id: string;
  has_more: boolean;
}

/**
 * Enum for the object property.
 */
export type UserListResponseObjectEnum = 'list';


export interface UserRoleUpdateRequest {
  /**
   * `owner` or `reader`
   */
  role: UserRoleUpdateRequestRoleEnum;
}

/**
 * Enum for the role property.
 */
export type UserRoleUpdateRequestRoleEnum = 'owner' | 'reader';


/**
 * The expiration policy for a vector store.
 */
export interface VectorStoreExpirationAfter {
  /**
   * Anchor timestamp after which the expiration policy applies. Supported anchors: `last_active_at`.
   */
  anchor: VectorStoreExpirationAfterAnchorEnum;
  /**
   * The number of days after the anchor time that the vector store will expire.
   */
  days: number;
}

/**
 * Enum for the anchor property.
 */
export type VectorStoreExpirationAfterAnchorEnum = 'last_active_at';


/**
 * A batch of files attached to a vector store.
 */
export interface VectorStoreFileBatchObject {
  /**
   * The identifier, which can be referenced in API endpoints.
   */
  id: string;
  /**
   * The object type, which is always `vector_store.file_batch`.
   */
  object: VectorStoreFileBatchObjectObjectEnum;
  /**
   * The Unix timestamp (in seconds) for when the vector store files batch was created.
   */
  created_at: number;
  /**
   * The ID of the [vector store](/docs/api-reference/vector-stores/object) that the [File](/docs/api-reference/files) is attached to.
   */
  vector_store_id: string;
  /**
   * The status of the vector store files batch, which can be either `in_progress`, `completed`, `cancelled` or `failed`.
   */
  status: VectorStoreFileBatchObjectStatusEnum;
  file_counts: VectorStoreFileBatchObjectFileCounts;
}

/**
 * Enum for the object property.
 */
export type VectorStoreFileBatchObjectObjectEnum = 'vector_store.files_batch';

/**
 * Enum for the status property.
 */
export type VectorStoreFileBatchObjectStatusEnum = 'in_progress' | 'completed' | 'cancelled' | 'failed';


export interface VectorStoreFileBatchObjectFileCounts {
  /**
   * The number of files that are currently being processed.
   */
  in_progress: number;
  /**
   * The number of files that have been processed.
   */
  completed: number;
  /**
   * The number of files that have failed to process.
   */
  failed: number;
  /**
   * The number of files that where cancelled.
   */
  cancelled: number;
  /**
   * The total number of files.
   */
  total: number;
}


/**
 * A list of files attached to a vector store.
 */
export interface VectorStoreFileObject {
  /**
   * The identifier, which can be referenced in API endpoints.
   */
  id: string;
  /**
   * The object type, which is always `vector_store.file`.
   */
  object: VectorStoreFileObjectObjectEnum;
  /**
   * The total vector store usage in bytes. Note that this may be different from the original file size.
   */
  usage_bytes: number;
  /**
   * The Unix timestamp (in seconds) for when the vector store file was created.
   */
  created_at: number;
  /**
   * The ID of the [vector store](/docs/api-reference/vector-stores/object) that the [File](/docs/api-reference/files) is attached to.
   */
  vector_store_id: string;
  /**
   * The status of the vector store file, which can be either `in_progress`, `completed`, `cancelled`, or `failed`. The status `completed` indicates that the vector store file is ready for use.
   */
  status: VectorStoreFileObjectStatusEnum;
  last_error: VectorStoreFileObjectLastError;
  chunking_strategy?: VectorStoreFileObjectChunkingStrategy;
}

/**
 * Enum for the object property.
 */
export type VectorStoreFileObjectObjectEnum = 'vector_store.file';

/**
 * Enum for the status property.
 */
export type VectorStoreFileObjectStatusEnum = 'in_progress' | 'completed' | 'cancelled' | 'failed';

/**
 * @type VectorStoreFileObjectChunkingStrategy
 * The strategy used to chunk the file.
 * @export
 */
export type VectorStoreFileObjectChunkingStrategy = OtherChunkingStrategyResponseParam | StaticChunkingStrategyResponseParam;


/**
 * The last error associated with this vector store file. Will be `null` if there are no errors.
 */
export interface VectorStoreFileObjectLastError {
  /**
   * One of `server_error` or `rate_limit_exceeded`.
   */
  code: VectorStoreFileObjectLastErrorCodeEnum;
  /**
   * A human-readable description of the error.
   */
  message: string;
}

/**
 * Enum for the code property.
 */
export type VectorStoreFileObjectLastErrorCodeEnum = 'server_error' | 'unsupported_file' | 'invalid_file';


/**
 * A vector store is a collection of processed files can be used by the `file_search` tool.
 */
export interface VectorStoreObject {
  /**
   * The identifier, which can be referenced in API endpoints.
   */
  id: string;
  /**
   * The object type, which is always `vector_store`.
   */
  object: VectorStoreObjectObjectEnum;
  /**
   * The Unix timestamp (in seconds) for when the vector store was created.
   */
  created_at: number;
  /**
   * The name of the vector store.
   */
  name: string;
  /**
   * The total number of bytes used by the files in the vector store.
   */
  usage_bytes: number;
  file_counts: VectorStoreObjectFileCounts;
  /**
   * The status of the vector store, which can be either `expired`, `in_progress`, or `completed`. A status of `completed` indicates that the vector store is ready for use.
   */
  status: VectorStoreObjectStatusEnum;
  expires_after?: VectorStoreExpirationAfter;
  /**
   * The Unix timestamp (in seconds) for when the vector store will expire.
   */
  expires_at?: number;
  /**
   * The Unix timestamp (in seconds) for when the vector store was last active.
   */
  last_active_at: number;
  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maximum of 512 characters long. 
   */
  metadata: object;
}

/**
 * Enum for the object property.
 */
export type VectorStoreObjectObjectEnum = 'vector_store';

/**
 * Enum for the status property.
 */
export type VectorStoreObjectStatusEnum = 'expired' | 'in_progress' | 'completed';


export interface VectorStoreObjectFileCounts {
  /**
   * The number of files that are currently being processed.
   */
  in_progress: number;
  /**
   * The number of files that have been successfully processed.
   */
  completed: number;
  /**
   * The number of files that have failed to process.
   */
  failed: number;
  /**
   * The number of files that were cancelled.
   */
  cancelled: number;
  /**
   * The total number of files.
   */
  total: number;
}

