/**
 * OpenAI API
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.3.0
 * Contact: blah+oapicf@cliffano.com
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */
import { CreateChatCompletionStreamResponseChoicesInner } from './createChatCompletionStreamResponseChoicesInner';
import { CreateChatCompletionStreamResponseUsage } from './createChatCompletionStreamResponseUsage';


/**
 * Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
 */
export interface CreateChatCompletionStreamResponse { 
    /**
     * A unique identifier for the chat completion. Each chunk has the same ID.
     */
    id: string;
    /**
     * A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the last chunk if you set `stream_options: {\"include_usage\": true}`. 
     */
    choices: Array<CreateChatCompletionStreamResponseChoicesInner>;
    /**
     * The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
     */
    created: number;
    /**
     * The model to generate the completion.
     */
    model: string;
    /**
     * The service tier used for processing the request. This field is only included if the `service_tier` parameter is specified in the request.
     */
    service_tier?: CreateChatCompletionStreamResponse.ServiceTierEnum | null;
    /**
     * This fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism. 
     */
    system_fingerprint?: string;
    /**
     * The object type, which is always `chat.completion.chunk`.
     */
    object: CreateChatCompletionStreamResponse.ObjectEnum;
    usage?: CreateChatCompletionStreamResponseUsage | null;
}
export namespace CreateChatCompletionStreamResponse {
    export type ServiceTierEnum = 'scale' | 'default';
    export const ServiceTierEnum = {
        Scale: 'scale' as ServiceTierEnum,
        Default: 'default' as ServiceTierEnum
    }
    export type ObjectEnum = 'chat.completion.chunk';
    export const ObjectEnum = {
        ChatCompletionChunk: 'chat.completion.chunk' as ObjectEnum
    }
}
