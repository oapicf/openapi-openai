/**
 * OpenAI API
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * OpenAPI spec version: 2.3.0
 * Contact: blah+oapicf@cliffano.com
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */

import { CreateChatCompletionStreamResponseChoicesInner } from '../models/CreateChatCompletionStreamResponseChoicesInner';
import { CreateChatCompletionStreamResponseUsage } from '../models/CreateChatCompletionStreamResponseUsage';
import { HttpFile } from '../http/http';

/**
* Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
*/
export class CreateChatCompletionStreamResponse {
    /**
    * A unique identifier for the chat completion. Each chunk has the same ID.
    */
    'id': string;
    /**
    * A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the last chunk if you set `stream_options: {\"include_usage\": true}`. 
    */
    'choices': Array<CreateChatCompletionStreamResponseChoicesInner>;
    /**
    * The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
    */
    'created': number;
    /**
    * The model to generate the completion.
    */
    'model': string;
    /**
    * The service tier used for processing the request. This field is only included if the `service_tier` parameter is specified in the request.
    */
    'serviceTier'?: CreateChatCompletionStreamResponseServiceTierEnum | null;
    /**
    * This fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism. 
    */
    'systemFingerprint'?: string;
    /**
    * The object type, which is always `chat.completion.chunk`.
    */
    'object': CreateChatCompletionStreamResponseObjectEnum;
    'usage'?: CreateChatCompletionStreamResponseUsage | null;

    static readonly discriminator: string | undefined = undefined;

    static readonly mapping: {[index: string]: string} | undefined = undefined;

    static readonly attributeTypeMap: Array<{name: string, baseName: string, type: string, format: string}> = [
        {
            "name": "id",
            "baseName": "id",
            "type": "string",
            "format": ""
        },
        {
            "name": "choices",
            "baseName": "choices",
            "type": "Array<CreateChatCompletionStreamResponseChoicesInner>",
            "format": ""
        },
        {
            "name": "created",
            "baseName": "created",
            "type": "number",
            "format": ""
        },
        {
            "name": "model",
            "baseName": "model",
            "type": "string",
            "format": ""
        },
        {
            "name": "serviceTier",
            "baseName": "service_tier",
            "type": "CreateChatCompletionStreamResponseServiceTierEnum",
            "format": ""
        },
        {
            "name": "systemFingerprint",
            "baseName": "system_fingerprint",
            "type": "string",
            "format": ""
        },
        {
            "name": "object",
            "baseName": "object",
            "type": "CreateChatCompletionStreamResponseObjectEnum",
            "format": ""
        },
        {
            "name": "usage",
            "baseName": "usage",
            "type": "CreateChatCompletionStreamResponseUsage",
            "format": ""
        }    ];

    static getAttributeTypeMap() {
        return CreateChatCompletionStreamResponse.attributeTypeMap;
    }

    public constructor() {
    }
}

export enum CreateChatCompletionStreamResponseServiceTierEnum {
    Scale = 'scale',
    Default = 'default'
}
export enum CreateChatCompletionStreamResponseObjectEnum {
    ChatCompletionChunk = 'chat.completion.chunk'
}

